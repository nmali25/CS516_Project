{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install aif360['all']"
      ],
      "metadata": {
        "id": "_msJ5E_gklwd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "1c9f129f-6b5d-44c0-887e-35d9e7833473"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting aif360[all]\n",
            "  Downloading aif360-0.6.1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.11/dist-packages (from aif360[all]) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from aif360[all]) (1.14.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from aif360[all]) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.11/dist-packages (from aif360[all]) (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from aif360[all]) (3.10.0)\n",
            "Collecting skorch (from aif360[all])\n",
            "  Downloading skorch-1.1.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting jupyter (from aif360[all])\n",
            "  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting sphinx-rtd-theme (from aif360[all])\n",
            "  Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting igraph[plotting] (from aif360[all])\n",
            "  Downloading igraph-0.11.8-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting lime (from aif360[all])\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from aif360[all]) (4.67.1)\n",
            "Collecting fairlearn~=0.7 (from aif360[all])\n",
            "  Downloading fairlearn-0.12.0-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting colorama (from aif360[all])\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: tensorflow>=1.13.1 in /usr/local/lib/python3.11/dist-packages (from aif360[all]) (2.18.0)\n",
            "Requirement already satisfied: sphinx in /usr/local/lib/python3.11/dist-packages (from aif360[all]) (8.2.3)\n",
            "Requirement already satisfied: mlxtend in /usr/local/lib/python3.11/dist-packages (from aif360[all]) (0.23.4)\n",
            "Collecting inFairness>=0.2.2 (from aif360[all])\n",
            "  Downloading inFairness-0.2.3-py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting pytest-cov>=2.8.1 (from aif360[all])\n",
            "  Downloading pytest_cov-6.1.1-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting ipympl (from aif360[all])\n",
            "  Downloading ipympl-0.9.7-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from aif360[all]) (2.6.0+cu124)\n",
            "Requirement already satisfied: jinja2>3.1.0 in /usr/local/lib/python3.11/dist-packages (from aif360[all]) (3.1.6)\n",
            "Collecting adversarial-robustness-toolbox>=1.0.0 (from aif360[all])\n",
            "  Downloading adversarial_robustness_toolbox-1.19.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting BlackBoxAuditing (from aif360[all])\n",
            "  Downloading BlackBoxAuditing-0.1.54.tar.gz (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from aif360[all]) (0.13.2)\n",
            "Requirement already satisfied: rpy2 in /usr/local/lib/python3.11/dist-packages (from aif360[all]) (3.5.17)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (from aif360[all]) (4.5.0)\n",
            "Requirement already satisfied: pytest>=3.5 in /usr/local/lib/python3.11/dist-packages (from aif360[all]) (8.3.5)\n",
            "Collecting pot (from aif360[all])\n",
            "  Downloading POT-0.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n",
            "Requirement already satisfied: cvxpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from aif360[all]) (1.6.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from adversarial-robustness-toolbox>=1.0.0->aif360[all]) (1.17.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from adversarial-robustness-toolbox>=1.0.0->aif360[all]) (75.2.0)\n",
            "Requirement already satisfied: osqp>=0.6.2 in /usr/local/lib/python3.11/dist-packages (from cvxpy>=1.0->aif360[all]) (1.0.3)\n",
            "Requirement already satisfied: clarabel>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from cvxpy>=1.0->aif360[all]) (0.10.0)\n",
            "Requirement already satisfied: scs>=3.2.4.post1 in /usr/local/lib/python3.11/dist-packages (from cvxpy>=1.0->aif360[all]) (3.2.7.post2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>3.1.0->aif360[all]) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.0->aif360[all]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.0->aif360[all]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.0->aif360[all]) (2025.2)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest>=3.5->aif360[all]) (2.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pytest>=3.5->aif360[all]) (24.2)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest>=3.5->aif360[all]) (1.5.0)\n",
            "Collecting coverage>=7.5 (from coverage[toml]>=7.5->pytest-cov>=2.8.1->aif360[all])\n",
            "  Downloading coverage-7.8.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0->aif360[all]) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0->aif360[all]) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.13.1->aif360[all]) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.13.1->aif360[all]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.13.1->aif360[all]) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.13.1->aif360[all]) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.13.1->aif360[all]) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.13.1->aif360[all]) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.13.1->aif360[all]) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.13.1->aif360[all]) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.13.1->aif360[all]) (2.32.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.13.1->aif360[all]) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.13.1->aif360[all]) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.13.1->aif360[all]) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.13.1->aif360[all]) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.13.1->aif360[all]) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.13.1->aif360[all]) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.13.1->aif360[all]) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.13.1->aif360[all]) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.13.1->aif360[all]) (0.37.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->aif360[all]) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->aif360[all]) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->aif360[all]) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->aif360[all])\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->aif360[all])\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->aif360[all])\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->aif360[all])\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->aif360[all])\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->aif360[all])\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->aif360[all])\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->aif360[all])\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->aif360[all])\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->aif360[all]) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->aif360[all]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->aif360[all]) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->aif360[all])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->aif360[all]) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->aif360[all]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->aif360[all]) (1.3.0)\n",
            "Collecting texttable>=1.6.2 (from igraph[plotting]; extra == \"all\"->aif360[all])\n",
            "  Downloading texttable-1.7.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting cairocffi>=1.2.0 (from igraph[plotting]; extra == \"all\"->aif360[all])\n",
            "  Downloading cairocffi-1.7.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: ipython<10 in /usr/local/lib/python3.11/dist-packages (from ipympl->aif360[all]) (7.34.0)\n",
            "Requirement already satisfied: ipywidgets<9,>=7.6.0 in /usr/local/lib/python3.11/dist-packages (from ipympl->aif360[all]) (7.7.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from ipympl->aif360[all]) (11.1.0)\n",
            "Requirement already satisfied: traitlets<6 in /usr/local/lib/python3.11/dist-packages (from ipympl->aif360[all]) (5.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->aif360[all]) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->aif360[all]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->aif360[all]) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->aif360[all]) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->aif360[all]) (3.2.3)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.11/dist-packages (from jupyter->aif360[all]) (6.5.7)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.11/dist-packages (from jupyter->aif360[all]) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.11/dist-packages (from jupyter->aif360[all]) (7.16.6)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.11/dist-packages (from jupyter->aif360[all]) (6.17.1)\n",
            "Collecting jupyterlab (from jupyter->aif360[all])\n",
            "  Downloading jupyterlab-4.4.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.11/dist-packages (from lime->aif360[all]) (0.25.2)\n",
            "Requirement already satisfied: cffi>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from rpy2->aif360[all]) (1.17.1)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.11/dist-packages (from rpy2->aif360[all]) (5.3.1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.11/dist-packages (from skorch->aif360[all]) (0.9.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from sphinx->aif360[all]) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx->aif360[all]) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx->aif360[all]) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from sphinx->aif360[all]) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx->aif360[all]) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.11/dist-packages (from sphinx->aif360[all]) (2.0.0)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.11/dist-packages (from sphinx->aif360[all]) (2.18.0)\n",
            "Requirement already satisfied: docutils<0.22,>=0.20 in /usr/local/lib/python3.11/dist-packages (from sphinx->aif360[all]) (0.21.2)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.11/dist-packages (from sphinx->aif360[all]) (2.2.0)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.11/dist-packages (from sphinx->aif360[all]) (2.17.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.11/dist-packages (from sphinx->aif360[all]) (1.0.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.11/dist-packages (from sphinx->aif360[all]) (1.4.1)\n",
            "Requirement already satisfied: roman-numerals-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from sphinx->aif360[all]) (3.1.0)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->aif360[all])\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow>=1.13.1->aif360[all]) (0.45.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.15.1->rpy2->aif360[all]) (2.22)\n",
            "Collecting jedi>=0.16 (from ipython<10->ipympl->aif360[all])\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython<10->ipympl->aif360[all]) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython<10->ipympl->aif360[all]) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython<10->ipympl->aif360[all]) (3.0.51)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython<10->ipympl->aif360[all]) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython<10->ipympl->aif360[all]) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython<10->ipympl->aif360[all]) (4.9.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets<9,>=7.6.0->ipympl->aif360[all]) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets<9,>=7.6.0->ipympl->aif360[all]) (3.6.10)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets<9,>=7.6.0->ipympl->aif360[all]) (3.0.14)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->aif360[all]) (1.8.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->aif360[all]) (6.1.12)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->aif360[all]) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->aif360[all]) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->aif360[all]) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->aif360[all]) (6.4.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow>=1.13.1->aif360[all]) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow>=1.13.1->aif360[all]) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow>=1.13.1->aif360[all]) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow>=1.13.1->aif360[all]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow>=1.13.1->aif360[all]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow>=1.13.1->aif360[all]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow>=1.13.1->aif360[all]) (2025.1.31)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime->aif360[all]) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime->aif360[all]) (2025.3.30)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime->aif360[all]) (0.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=1.13.1->aif360[all]) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=1.13.1->aif360[all]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=1.13.1->aif360[all]) (3.1.3)\n",
            "Collecting async-lru>=1.0.0 (from jupyterlab->jupyter->aif360[all])\n",
            "  Downloading async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: httpx>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->aif360[all]) (0.28.1)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->aif360[all]) (5.7.2)\n",
            "Collecting jupyter-lsp>=2.0.0 (from jupyterlab->jupyter->aif360[all])\n",
            "  Downloading jupyter_lsp-2.2.5-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting jupyter-server<3,>=2.4.0 (from jupyterlab->jupyter->aif360[all])\n",
            "  Downloading jupyter_server-2.15.0-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting jupyterlab-server<3,>=2.27.1 (from jupyterlab->jupyter->aif360[all])\n",
            "  Downloading jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: notebook-shim>=0.2 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->aif360[all]) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->aif360[all]) (4.13.4)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->aif360[all]) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->aif360[all]) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->aif360[all]) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->aif360[all]) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->aif360[all]) (0.10.2)\n",
            "Requirement already satisfied: nbformat>=5.7 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->aif360[all]) (5.10.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->aif360[all]) (1.5.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->aif360[all]) (23.1.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->aif360[all]) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->aif360[all]) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->aif360[all]) (0.21.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->aif360[all]) (1.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter->aif360[all]) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->aif360[all]) (1.4.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter->aif360[all]) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter->aif360[all]) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab->jupyter->aif360[all]) (0.14.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython<10->ipympl->aif360[all]) (0.8.4)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core->jupyterlab->jupyter->aif360[all]) (4.3.7)\n",
            "Collecting jupyter-client>=6.1.12 (from ipykernel->jupyter->aif360[all])\n",
            "  Downloading jupyter_client-7.4.9-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting jupyter-events>=0.11.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->aif360[all])\n",
            "  Downloading jupyter_events-0.12.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->aif360[all])\n",
            "  Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->aif360[all])\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->aif360[all]) (1.8.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook->jupyter->aif360[all]) (21.2.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel->jupyter->aif360[all]) (0.4)\n",
            "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->aif360[all])\n",
            "  Downloading json5-0.12.0-py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->aif360[all]) (4.23.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7->nbconvert->jupyter->aif360[all]) (2.21.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython<10->ipympl->aif360[all]) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<10->ipympl->aif360[all]) (0.2.13)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert->jupyter->aif360[all]) (2.7)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow>=1.13.1->aif360[all]) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.0->jupyterlab->jupyter->aif360[all]) (1.3.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->aif360[all]) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->aif360[all]) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->aif360[all]) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->aif360[all]) (0.24.0)\n",
            "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->aif360[all])\n",
            "  Downloading python_json_logger-3.3.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.11/dist-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->aif360[all]) (6.0.2)\n",
            "Collecting rfc3339-validator (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->aif360[all])\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->aif360[all])\n",
            "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow>=1.13.1->aif360[all]) (0.1.2)\n",
            "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->aif360[all])\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->aif360[all])\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->aif360[all]) (3.0.0)\n",
            "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->aif360[all])\n",
            "  Downloading uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->aif360[all]) (24.11.1)\n",
            "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->aif360[all])\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->aif360[all])\n",
            "  Downloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl.metadata (2.1 kB)\n",
            "Downloading adversarial_robustness_toolbox-1.19.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fairlearn-0.12.0-py3-none-any.whl (240 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.0/240.0 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading inFairness-0.2.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading POT-0.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_cov-6.1.1-py3-none-any.whl (23 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aif360-0.6.1-py3-none-any.whl (259 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.7/259.7 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading ipympl-0.9.7-py3-none-any.whl (515 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.7/515.7 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
            "Downloading skorch-1.1.0-py3-none-any.whl (228 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.9/228.9 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m113.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cairocffi-1.7.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coverage-7.8.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.0/244.0 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading igraph-0.11.8-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m114.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab-4.4.1-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m108.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_server-2.15.0-py3-none-any.whl (385 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.8/385.8 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_client-7.4.9-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.5/133.5 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json5-0.12.0-py3-none-any.whl (36 kB)\n",
            "Downloading jupyter_events-0.12.0-py3-none-any.whl (19 kB)\n",
            "Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading python_json_logger-3.3.0-py3-none-any.whl (15 kB)\n",
            "Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
            "Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl (14 kB)\n",
            "Building wheels for collected packages: BlackBoxAuditing, lime\n",
            "  Building wheel for BlackBoxAuditing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for BlackBoxAuditing: filename=BlackBoxAuditing-0.1.54-py2.py3-none-any.whl size=1394756 sha256=a37a4cf97ddea90dbf15b0ad8ec037c1cbae0a4d3f014cd813edbef8dddf9fc2\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/8c/03/073e80e604151fb4cdc68b2e56a97f338d7723e4a4ab5e3823\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283834 sha256=21fffe3e23462733f4ab7f54314d40954dca6e7853ef79095c90dc9eedd8766a\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/fa/a3/9c2d44c9f3cd77cf4e533b58900b2bf4487f2a17e8ec212a3d\n",
            "Successfully built BlackBoxAuditing lime\n",
            "Installing collected packages: texttable, uri-template, types-python-dateutil, rfc3986-validator, rfc3339-validator, python-json-logger, overrides, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, json5, jedi, igraph, fqdn, coverage, colorama, async-lru, pot, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jupyter-server-terminals, jupyter-client, cairocffi, arrow, sphinxcontrib-jquery, skorch, pytest-cov, nvidia-cusolver-cu12, lime, isoduration, fairlearn, BlackBoxAuditing, aif360, adversarial-robustness-toolbox, sphinx-rtd-theme, jupyter-events, inFairness, jupyter-server, jupyterlab-server, jupyter-lsp, jupyterlab, jupyter, ipympl\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: jupyter-client\n",
            "    Found existing installation: jupyter-client 6.1.12\n",
            "    Uninstalling jupyter-client-6.1.12:\n",
            "      Successfully uninstalled jupyter-client-6.1.12\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: jupyter-server\n",
            "    Found existing installation: jupyter-server 1.16.0\n",
            "    Uninstalling jupyter-server-1.16.0:\n",
            "      Successfully uninstalled jupyter-server-1.16.0\n",
            "Successfully installed BlackBoxAuditing-0.1.54 adversarial-robustness-toolbox-1.19.1 aif360-0.6.1 arrow-1.3.0 async-lru-2.0.5 cairocffi-1.7.1 colorama-0.4.6 coverage-7.8.0 fairlearn-0.12.0 fqdn-1.5.1 igraph-0.11.8 inFairness-0.2.3 ipympl-0.9.7 isoduration-20.11.0 jedi-0.19.2 json5-0.12.0 jupyter-1.1.1 jupyter-client-7.4.9 jupyter-events-0.12.0 jupyter-lsp-2.2.5 jupyter-server-2.15.0 jupyter-server-terminals-0.5.3 jupyterlab-4.4.1 jupyterlab-server-2.27.3 lime-0.2.0.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 overrides-7.7.0 pot-0.9.5 pytest-cov-6.1.1 python-json-logger-3.3.0 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 skorch-1.1.0 sphinx-rtd-theme-3.0.2 sphinxcontrib-jquery-4.1 texttable-1.7.0 types-python-dateutil-2.9.0.20241206 uri-template-1.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fairlearn"
      ],
      "metadata": {
        "id": "gjj8CEZFHwnT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68851199-9af5-42af-8109-d18ae9a6b6e7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fairlearn in /usr/local/lib/python3.11/dist-packages (0.12.0)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from fairlearn) (2.0.2)\n",
            "Requirement already satisfied: pandas>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from fairlearn) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.2.1 in /usr/local/lib/python3.11/dist-packages (from fairlearn) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.9.3 in /usr/local/lib/python3.11/dist-packages (from fairlearn) (1.14.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.3->fairlearn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.3->fairlearn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.3->fairlearn) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2.1->fairlearn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2.1->fairlearn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.3->fairlearn) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Core Python Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Scikit-Learn: Preprocessing, Modeling, Evaluation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# XGBoost\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# AIF360: Datasets and Metrics\n",
        "from aif360.datasets import BinaryLabelDataset\n",
        "from aif360.metrics import ClassificationMetric\n",
        "\n",
        "# AIF360: Preprocessing Algorithms\n",
        "from aif360.algorithms.preprocessing import (\n",
        "    Reweighing,\n",
        "    DisparateImpactRemover,\n",
        "    LFR\n",
        ")\n",
        "\n",
        "# AIF360: In-processing Algorithms\n",
        "from aif360.algorithms.inprocessing import (\n",
        "    MetaFairClassifier,\n",
        "    GerryFairClassifier,\n",
        "    PrejudiceRemover,\n",
        "    ExponentiatedGradientReduction,\n",
        "    GridSearchReduction,\n",
        "    ARTClassifier,\n",
        "    AdversarialDebiasing\n",
        ")\n",
        "\n",
        "#AIF360: Post-processing Algorithms\n",
        "from aif360.algorithms.postprocessing import (\n",
        "    RejectOptionClassification,\n",
        "    CalibratedEqOddsPostprocessing,\n",
        "    EqOddsPostprocessing\n",
        ")\n",
        "\n",
        "# TensorFlow for AdversarialDebiasing\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_eager_execution()\n"
      ],
      "metadata": {
        "id": "T-7mVQc2UKuZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17d14762-0de3-44f2-83f2-f9773b384661"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/inFairness/utils/ndcg.py:37: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
            "  vect_normalized_discounted_cumulative_gain = vmap(\n",
            "/usr/local/lib/python3.11/dist-packages/inFairness/utils/ndcg.py:48: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
            "  monte_carlo_vect_ndcg = vmap(vect_normalized_discounted_cumulative_gain, in_dims=(0,))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data loading"
      ],
      "metadata": {
        "id": "MRMGxO9EIJva"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_taiwan_data():\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
        "\n",
        "    # Load dataset with header at row 1\n",
        "    df = pd.read_excel(url, header=1)\n",
        "\n",
        "    # Rename target column and remap: 0 = No Default → 1 (favorable), 1 = Default → 0 (unfavorable)\n",
        "    df.rename(columns={\"default payment next month\": \"target\"}, inplace=True)\n",
        "    df['target'] = df['target'].map({0: 1, 1: 0})  # flip targets for fairness alignment\n",
        "\n",
        "    # Protected attribute: age >= 25 is privileged (1), else unprivileged (0)\n",
        "    df['age_binary'] = df['AGE'].apply(lambda x: 1 if x >= 25 else 0)\n",
        "\n",
        "    # Drop ID and AGE (already used in age_binary)\n",
        "    df = df.drop(columns=[\"ID\", \"AGE\"])\n",
        "\n",
        "    # One-hot encode categorical variables\n",
        "    df_encoded = pd.get_dummies(df.drop(columns=['target', 'age_binary']), drop_first=True)\n",
        "\n",
        "    X = df_encoded\n",
        "    y = df['target']\n",
        "    protected = df['age_binary']\n",
        "\n",
        "    # Normalize features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test, prot_train, prot_test = train_test_split(\n",
        "        X_scaled, y, protected, test_size=0.3, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, prot_train, prot_test\n"
      ],
      "metadata": {
        "id": "3KjeXIoWUsqX"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_taiwan_credit_data():\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
        "\n",
        "    # Read Excel file with header row at index 1\n",
        "    df = pd.read_excel(url, header=1)\n",
        "\n",
        "    # Rename and remap target column: 0 = No default (favorable), 1 = Default (unfavorable)\n",
        "    df.rename(columns={\"default payment next month\": \"target\"}, inplace=True)\n",
        "    df['target'] = df['target'].map({0: 1, 1: 0})\n",
        "\n",
        "    # Protected attribute: age >= 25 -> 1 (privileged), < 25 -> 0 (unprivileged)\n",
        "    df['age_binary'] = df['AGE'].apply(lambda x: 1 if x >= 25 else 0)\n",
        "\n",
        "    # Features excluding ID, AGE, target, and protected attribute\n",
        "    X_raw = df.drop(columns=[\"ID\", \"AGE\", \"target\", \"age_binary\"])\n",
        "    y = df['target'].values\n",
        "    prot = df['age_binary'].values\n",
        "\n",
        "    return X_raw, y, prot\n",
        "\n",
        "def preprocess_data(X_raw, y, prot):\n",
        "    cat_cols = X_raw.select_dtypes(include='object').columns.tolist()\n",
        "    num_cols = X_raw.select_dtypes(exclude='object').columns.tolist()\n",
        "\n",
        "    X_raw_train, X_raw_test, y_train, y_test, prot_train, prot_test = train_test_split(\n",
        "        X_raw, y, prot, test_size=0.3, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    encoder = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
        "    X_train_cat = encoder.fit_transform(X_raw_train[cat_cols])\n",
        "    X_test_cat = encoder.transform(X_raw_test[cat_cols])\n",
        "\n",
        "    X_train_num = X_raw_train[num_cols].values\n",
        "    X_test_num = X_raw_test[num_cols].values\n",
        "\n",
        "    X_train = np.hstack((X_train_num, X_train_cat))\n",
        "    X_test = np.hstack((X_test_num, X_test_cat))\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, prot_train, prot_test\n"
      ],
      "metadata": {
        "id": "wYKZa7A-FVqd"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_taiwan_credit_data_inprocess():\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
        "\n",
        "    # Load dataset from Excel with header at row 1\n",
        "    df = pd.read_excel(url, header=1)\n",
        "\n",
        "    # Rename and remap label column\n",
        "    df.rename(columns={\"default payment next month\": \"target\"}, inplace=True)\n",
        "    df['target'] = df['target'].map({0: 1, 1: 0})  # No default = favorable (1), default = unfavorable (0)\n",
        "\n",
        "    # Binary protected attribute: AGE >= 25 → privileged (1), else unprivileged (0)\n",
        "    df['age_binary'] = df['AGE'].apply(lambda x: 1 if x >= 25 else 0)\n",
        "\n",
        "    # Drop ID, AGE, target and age_binary to isolate X_raw\n",
        "    X_raw = df.drop(columns=[\"ID\", \"AGE\", \"target\", \"age_binary\"])\n",
        "    y = df['target'].values\n",
        "    prot = df['age_binary'].values\n",
        "\n",
        "    # Train-test split\n",
        "    X_train_raw, X_test_raw, y_train, y_test, prot_train, prot_test = train_test_split(\n",
        "        X_raw, y, prot, test_size=0.3, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    # Identify categorical and numeric features\n",
        "    categorical_cols = X_raw.select_dtypes(include='object').columns.tolist()\n",
        "    numeric_cols = X_raw.select_dtypes(exclude='object').columns.tolist()\n",
        "\n",
        "    # One-hot encode categorical features\n",
        "    encoder = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
        "    X_train_cat = encoder.fit_transform(X_train_raw[categorical_cols])\n",
        "    X_test_cat = encoder.transform(X_test_raw[categorical_cols])\n",
        "\n",
        "    # Combine numerical and categorical features\n",
        "    X_train_num = X_train_raw[numeric_cols].values\n",
        "    X_test_num = X_test_raw[numeric_cols].values\n",
        "\n",
        "    X_train = np.hstack((X_train_num, X_train_cat))\n",
        "    X_test = np.hstack((X_test_num, X_test_cat))\n",
        "\n",
        "    # Normalize features\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, prot_train, prot_test\n"
      ],
      "metadata": {
        "id": "GI7gpzv1IEuR"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Utility Functions"
      ],
      "metadata": {
        "id": "_ZsVhApdIFx3"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_baseline_model(X_train, y_train, sample_weight=None):\n",
        "    model = XGBClassifier(eval_metric='logloss', use_label_encoder=False)\n",
        "    model.fit(X_train, y_train, sample_weight=sample_weight)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "KdUC91D_F3Ig"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_fairness(y_true, y_pred, prot, X=None):\n",
        "    df = pd.DataFrame(np.hstack((X, y_true[:, None], prot[:, None])),\n",
        "                      columns=[f\"x{i}\" for i in range(X.shape[1])] + ['label', 'protected'])\n",
        "\n",
        "    dataset_true = BinaryLabelDataset(df=df,\n",
        "                                      label_names=[\"label\"],\n",
        "                                      protected_attribute_names=[\"protected\"],\n",
        "                                      favorable_label=1, unfavorable_label=0)\n",
        "\n",
        "    pred_dataset = dataset_true.copy()\n",
        "    pred_dataset.labels = y_pred.reshape(-1, 1)\n",
        "\n",
        "    metric = ClassificationMetric(dataset_true, pred_dataset,\n",
        "                                  privileged_groups=[{'protected': 1}],\n",
        "                                  unprivileged_groups=[{'protected': 0}])\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy_score(y_true, y_pred),\n",
        "        'disparate_impact': metric.disparate_impact(),\n",
        "        'statistical_parity_difference': metric.statistical_parity_difference(),\n",
        "        'equal_opportunity_difference': metric.equal_opportunity_difference()\n",
        "    }\n"
      ],
      "metadata": {
        "id": "UmXUc4HcF-mp"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X_test, y_test, prot_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    return evaluate_fairness(y_test, y_pred, prot_test, X_test)\n"
      ],
      "metadata": {
        "id": "h7DJ4vxXGAF6"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EwrdQgx2ea0l"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#XGboost\n",
        "X_train, X_test, y_train, y_test, prot_train, prot_test = load_and_preprocess_taiwan_data()\n",
        "baseline_model = train_baseline_model(X_train, y_train)\n",
        "baseline_metrics = evaluate_model(baseline_model, X_test, y_test.to_numpy(), prot_test.to_numpy())\n",
        "\n",
        "baseline_metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHXnaTnhVTlk",
        "outputId": "4d7c503c-8604-4484-b335-71b994e4a11b"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:04:17] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.8147777777777778,\n",
              " 'disparate_impact': np.float64(0.9344756978122642),\n",
              " 'statistical_parity_difference': np.float64(-0.057739791073124436),\n",
              " 'equal_opportunity_difference': np.float64(-0.01436395582737049)}"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inprocess_results = {}"
      ],
      "metadata": {
        "id": "3LXrBhDvGhSa"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##disparate impact"
      ],
      "metadata": {
        "id": "0iFApi5gedu4"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_disparate_impact_remover(X_train, y_train, prot_train, repair_level=1.0):\n",
        "    df = pd.DataFrame(X_train)\n",
        "    df['target'] = y_train.values\n",
        "    df['protected'] = prot_train.values\n",
        "\n",
        "    dataset = BinaryLabelDataset(\n",
        "        favorable_label=1,\n",
        "        unfavorable_label=0,\n",
        "        df=df,\n",
        "        label_names=['target'],\n",
        "        protected_attribute_names=['protected']\n",
        "    )\n",
        "\n",
        "    dir_remover = DisparateImpactRemover(repair_level=repair_level)\n",
        "    repaired_dataset = dir_remover.fit_transform(dataset)\n",
        "\n",
        "    X_repaired = pd.DataFrame(repaired_dataset.features)\n",
        "    y_repaired = pd.Series(repaired_dataset.labels.ravel())\n",
        "    prot_repaired = pd.Series(repaired_dataset.protected_attributes.ravel())\n",
        "\n",
        "    return X_repaired, y_repaired, prot_repaired\n"
      ],
      "metadata": {
        "id": "SS_fT8YeVWIH"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Disparate Impact Remover to both train and test sets\n",
        "X_train_repaired, y_train_repaired, prot_train_repaired = apply_disparate_impact_remover(\n",
        "    pd.DataFrame(X_train), y_train, prot_train\n",
        ")\n",
        "\n",
        "X_test_repaired, y_test_repaired, prot_test_repaired = apply_disparate_impact_remover(\n",
        "    pd.DataFrame(X_test), y_test, prot_test\n",
        ")\n",
        "\n",
        "# Train on repaired training data\n",
        "fair_model = train_baseline_model(X_train_repaired, y_train_repaired)\n",
        "\n",
        "# Evaluate on repaired test data\n",
        "fair_metrics = evaluate_model(\n",
        "    fair_model,\n",
        "    X_test_repaired.to_numpy(),\n",
        "    y_test_repaired.to_numpy(),\n",
        "    prot_test_repaired.to_numpy()\n",
        ")\n",
        "inprocess_results['Disparate_Impact_Remover']=fair_metrics\n",
        "fair_metrics\n"
      ],
      "metadata": {
        "id": "DZTmaL0dXLss",
        "outputId": "05fa9dd0-2fb5-48c8-a61a-cfac7b8d4866",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:04:28] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.8094444444444444,\n",
              " 'disparate_impact': np.float64(0.9334450027917365),\n",
              " 'statistical_parity_difference': np.float64(-0.056600189933523226),\n",
              " 'equal_opportunity_difference': np.float64(-0.013857416296440661)}"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##LFR"
      ],
      "metadata": {
        "id": "IM4Gsh0teXy-"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_lfr(X_train, y_train, prot_train):\n",
        "    df = pd.DataFrame(X_train)\n",
        "    df['target'] = y_train.values\n",
        "    df['protected'] = prot_train.values\n",
        "\n",
        "    dataset = BinaryLabelDataset(\n",
        "        favorable_label=1,\n",
        "        unfavorable_label=0,\n",
        "        df=df,\n",
        "        label_names=['target'],\n",
        "        protected_attribute_names=['protected']\n",
        "    )\n",
        "\n",
        "    lfr = LFR(unprivileged_groups=[{'protected': 0}],\n",
        "          privileged_groups=[{'protected': 1}],\n",
        "          k=10, Ax=0.01, Ay=1.0, Az=0.1, verbose=0)\n",
        "\n",
        "\n",
        "    lfr.fit(dataset)\n",
        "    transformed_dataset = lfr.transform(dataset)\n",
        "\n",
        "    X_transformed = pd.DataFrame(transformed_dataset.features)\n",
        "    y_transformed = pd.Series(transformed_dataset.labels.ravel())\n",
        "    prot_transformed = pd.Series(transformed_dataset.protected_attributes.ravel())\n",
        "\n",
        "    return X_transformed, y_transformed, prot_transformed, lfr\n"
      ],
      "metadata": {
        "id": "liLwxciLejD-"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_lfr, y_train_lfr, prot_train_lfr, lfr_model = apply_lfr(pd.DataFrame(X_train), y_train, prot_train)\n"
      ],
      "metadata": {
        "id": "Dke6DdwRekky"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Unique labels in y_train_lfr:\", np.unique(y_train_lfr))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmmBlvSRfCjX",
        "outputId": "36beb6d3-3de8-4185-87af-3446403fbcca"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique labels in y_train_lfr: [0. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fair_lfr_model = train_baseline_model(X_train_lfr, y_train_lfr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wy6VtAcQemhQ",
        "outputId": "e5ee650c-7aa5-460d-e323-684dc5c1abfe"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:05:51] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply LFR transformation on test set\n",
        "df_test = pd.DataFrame(X_test)\n",
        "df_test['target'] = y_test.values\n",
        "df_test['protected'] = prot_test.values\n",
        "\n",
        "test_dataset = BinaryLabelDataset(\n",
        "    favorable_label=1,\n",
        "    unfavorable_label=0,\n",
        "    df=df_test,\n",
        "    label_names=['target'],\n",
        "    protected_attribute_names=['protected']\n",
        ")\n",
        "\n",
        "transformed_test_dataset = lfr_model.transform(test_dataset)\n",
        "\n",
        "X_test_lfr = pd.DataFrame(transformed_test_dataset.features)\n",
        "y_test_lfr = pd.Series(transformed_test_dataset.labels.ravel())\n",
        "prot_test_lfr = pd.Series(transformed_test_dataset.protected_attributes.ravel())\n",
        "\n",
        "# Evaluate\n",
        "fair_lfr_metrics = evaluate_model(\n",
        "    fair_lfr_model,\n",
        "    X_test_lfr.to_numpy(),\n",
        "    y_test_lfr.to_numpy(),\n",
        "    prot_test_lfr.to_numpy()\n",
        ")\n",
        "inprocess_results['LFR']=fair_lfr_metrics\n",
        "\n",
        "fair_lfr_metrics\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhEjgCm3evMi",
        "outputId": "fd8cab8c-308f-44dd-a7de-cbe1d09ed1c2"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.9992222222222222,\n",
              " 'disparate_impact': np.float64(0.9374173845662974),\n",
              " 'statistical_parity_difference': np.float64(-0.05588115588115594),\n",
              " 'equal_opportunity_difference': np.float64(-0.0009281793974816877)}"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#reweighing"
      ],
      "metadata": {
        "id": "Maxq0HlAHAEv"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_reweighing(X_train, y_train, prot_train):\n",
        "    df_train = pd.DataFrame(np.hstack((X_train, y_train.reshape(-1, 1), prot_train.reshape(-1, 1))),\n",
        "                            columns=[f\"x{i}\" for i in range(X_train.shape[1])] + [\"label\", \"protected\"])\n",
        "    bld_train = BinaryLabelDataset(\n",
        "        favorable_label=1,\n",
        "        unfavorable_label=0,\n",
        "        df=df_train,\n",
        "        label_names=[\"label\"],\n",
        "        protected_attribute_names=[\"protected\"]\n",
        "    )\n",
        "    RW = Reweighing(unprivileged_groups=[{'protected': 0}], privileged_groups=[{'protected': 1}])\n",
        "    bld_rw = RW.fit_transform(bld_train)\n",
        "\n",
        "    return bld_rw.features, bld_rw.labels.ravel(), bld_rw.instance_weights\n"
      ],
      "metadata": {
        "id": "7CKRSQlnCnT5"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test, prot_train, prot_test = preprocess_data(*load_taiwan_credit_data())\n",
        "\n",
        "X_rw, y_rw, sample_weights = apply_reweighing(X_train, y_train, prot_train)\n",
        "\n",
        "expected_num_features = X_rw.shape[1]\n",
        "\n",
        "if X_test.shape[1] < expected_num_features:\n",
        "    padding = expected_num_features - X_test.shape[1]\n",
        "    X_test_aligned = np.hstack((X_test, np.zeros((X_test.shape[0], padding))))\n",
        "elif X_test.shape[1] > expected_num_features:\n",
        "    X_test_aligned = X_test[:, :expected_num_features]\n",
        "else:\n",
        "    X_test_aligned = X_test\n",
        "\n",
        "model_rw = train_baseline_model(X_rw, y_rw, sample_weight=sample_weights)\n",
        "\n",
        "reweighing_metric = evaluate_model(model_rw, X_test_aligned, y_test, prot_test)\n",
        "\n",
        "inprocess_results['reweighing'] = reweighing_metric\n",
        "reweighing_metric\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GygXVrOmDCHX",
        "outputId": "0e5f5539-e109-4fc2-b14a-1b0cc1bf461b"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:05:58] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.8106666666666666,\n",
              " 'disparate_impact': np.float64(0.9421403668334091),\n",
              " 'statistical_parity_difference': np.float64(-0.05178401845068514),\n",
              " 'equal_opportunity_difference': np.float64(-0.012024560805048656)}"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for model, metrics in inprocess_results.items():\n",
        "    print(f\"🔹 {model}\")\n",
        "    for k, v in metrics.items():\n",
        "        print(f\"  {k}: {v:.4f}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMCZUNH3HsQG",
        "outputId": "ea56aa59-f576-44d9-f9d7-136ce6d731c4"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Disparate_Impact_Remover\n",
            "  accuracy: 0.8094\n",
            "  disparate_impact: 0.9334\n",
            "  statistical_parity_difference: -0.0566\n",
            "  equal_opportunity_difference: -0.0139\n",
            "\n",
            "🔹 LFR\n",
            "  accuracy: 0.9992\n",
            "  disparate_impact: 0.9374\n",
            "  statistical_parity_difference: -0.0559\n",
            "  equal_opportunity_difference: -0.0009\n",
            "\n",
            "🔹 reweighing\n",
            "  accuracy: 0.8107\n",
            "  disparate_impact: 0.9421\n",
            "  statistical_parity_difference: -0.0518\n",
            "  equal_opportunity_difference: -0.0120\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Inprocess"
      ],
      "metadata": {
        "id": "ckjyZeplrqHh"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test, prot_train, prot_test = load_taiwan_credit_data_inprocess()\n",
        "results = {}"
      ],
      "metadata": {
        "id": "OvO0R5sPI5Ui"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gerryfair(X_train, y_train, prot_train, X_test, y_test, prot_test):\n",
        "    df_train = pd.DataFrame(np.hstack((X_train, y_train[:, None], prot_train[:, None])),\n",
        "                            columns=[f\"x{i}\" for i in range(X_train.shape[1])] + [\"label\", \"protected\"])\n",
        "    bld_train = BinaryLabelDataset(df=df_train, label_names=[\"label\"], protected_attribute_names=[\"protected\"],\n",
        "                                   favorable_label=1, unfavorable_label=0)\n",
        "\n",
        "    df_test = pd.DataFrame(np.hstack((X_test, y_test[:, None], prot_test[:, None])),\n",
        "                           columns=[f\"x{i}\" for i in range(X_test.shape[1])] + [\"label\", \"protected\"])\n",
        "    bld_test = BinaryLabelDataset(df=df_test, label_names=[\"label\"], protected_attribute_names=[\"protected\"],\n",
        "                                  favorable_label=1, unfavorable_label=0)\n",
        "\n",
        "    clf = GerryFairClassifier(C=100, printflag=False, gamma=0.005, fairness_def='FP', max_iters=50)\n",
        "    clf.fit(bld_train)\n",
        "    pred = clf.predict(bld_test)\n",
        "\n",
        "    return evaluate_fairness(y_test, pred.labels.ravel(), prot_test, X_test)\n"
      ],
      "metadata": {
        "id": "TUllqmP6xbgT"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results['GerryFair'] = train_gerryfair(X_train, y_train, prot_train, X_test, y_test, prot_test)\n"
      ],
      "metadata": {
        "id": "zhCuFVZBI6j0"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_prejudice_remover(X_train, y_train, prot_train, X_test, y_test, prot_test):\n",
        "    df_train = pd.DataFrame(np.hstack((X_train, y_train.reshape(-1, 1), prot_train.reshape(-1, 1))),\n",
        "                            columns=[f\"x{i}\" for i in range(X_train.shape[1])] + [\"label\", \"protected\"])\n",
        "    bld_train = BinaryLabelDataset(df=df_train,\n",
        "                                   label_names=[\"label\"],\n",
        "                                   protected_attribute_names=[\"protected\"],\n",
        "                                   favorable_label=1,\n",
        "                                   unfavorable_label=0)\n",
        "\n",
        "    df_test = pd.DataFrame(np.hstack((X_test, y_test.reshape(-1, 1), prot_test.reshape(-1, 1))),\n",
        "                           columns=[f\"x{i}\" for i in range(X_test.shape[1])] + [\"label\", \"protected\"])\n",
        "    bld_test = BinaryLabelDataset(df=df_test,\n",
        "                                  label_names=[\"label\"],\n",
        "                                  protected_attribute_names=[\"protected\"],\n",
        "                                  favorable_label=1,\n",
        "                                  unfavorable_label=0)\n",
        "\n",
        "    clf = PrejudiceRemover(sensitive_attr=\"protected\", eta=25.0)\n",
        "    clf.fit(bld_train)\n",
        "    pred = clf.predict(bld_test)\n",
        "\n",
        "    return evaluate_fairness(y_test, pred.labels.ravel(), prot_test, X_test)\n"
      ],
      "metadata": {
        "id": "xSmVTFLlxdBc"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results['PrejudiceRemover'] = train_prejudice_remover(X_train, y_train, prot_train, X_test, y_test, prot_test)\n"
      ],
      "metadata": {
        "id": "emuzoR6AJAeI"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_expgrad(X_train, y_train, prot_train, X_test, y_test, prot_test):\n",
        "    df_train = pd.DataFrame(np.hstack((X_train, y_train[:, None], prot_train[:, None])),\n",
        "                            columns=[f\"x{i}\" for i in range(X_train.shape[1])] + [\"label\", \"protected\"])\n",
        "    bld_train = BinaryLabelDataset(df=df_train,\n",
        "                                   label_names=[\"label\"],\n",
        "                                   protected_attribute_names=[\"protected\"],\n",
        "                                   favorable_label=1,\n",
        "                                   unfavorable_label=0)\n",
        "\n",
        "    df_test = pd.DataFrame(np.hstack((X_test, y_test[:, None], prot_test[:, None])),\n",
        "                           columns=[f\"x{i}\" for i in range(X_test.shape[1])] + [\"label\", \"protected\"])\n",
        "    bld_test = BinaryLabelDataset(df=df_test,\n",
        "                                  label_names=[\"label\"],\n",
        "                                  protected_attribute_names=[\"protected\"],\n",
        "                                  favorable_label=1,\n",
        "                                  unfavorable_label=0)\n",
        "\n",
        "    expgrad = ExponentiatedGradientReduction(\n",
        "        estimator=LogisticRegression(solver='liblinear'),\n",
        "        constraints=\"DemographicParity\"\n",
        "    )\n",
        "    expgrad.fit(bld_train)\n",
        "\n",
        "    pred = expgrad.predict(bld_test)\n",
        "    return evaluate_fairness(y_test, pred.labels.ravel(), prot_test, X_test)\n"
      ],
      "metadata": {
        "id": "lJff7KFSxeik"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results['ExponentiatedGradient'] = train_expgrad(X_train, y_train, prot_train, X_test, y_test, prot_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVSugHebJCdn",
        "outputId": "bf820b26-1c0f-4fb8-a91c-9455c1990e9e"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:110: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gridsearch(X_train, y_train, prot_train, X_test, y_test, prot_test):\n",
        "    df_train = pd.DataFrame(np.hstack((X_train, y_train[:, None], prot_train[:, None])),\n",
        "                            columns=[f\"x{i}\" for i in range(X_train.shape[1])] + [\"label\", \"protected\"])\n",
        "    bld_train = BinaryLabelDataset(df=df_train,\n",
        "                                   label_names=[\"label\"],\n",
        "                                   protected_attribute_names=[\"protected\"],\n",
        "                                   favorable_label=1,\n",
        "                                   unfavorable_label=0)\n",
        "\n",
        "    df_test = pd.DataFrame(np.hstack((X_test, y_test[:, None], prot_test[:, None])),\n",
        "                           columns=[f\"x{i}\" for i in range(X_test.shape[1])] + [\"label\", \"protected\"])\n",
        "    bld_test = BinaryLabelDataset(df=df_test,\n",
        "                                  label_names=[\"label\"],\n",
        "                                  protected_attribute_names=[\"protected\"],\n",
        "                                  favorable_label=1,\n",
        "                                  unfavorable_label=0)\n",
        "\n",
        "    grid = GridSearchReduction(\n",
        "        estimator=LogisticRegression(solver='liblinear'),\n",
        "        constraints=\"DemographicParity\"\n",
        "    )\n",
        "    grid.fit(bld_train)\n",
        "\n",
        "    pred = grid.predict(bld_test)\n",
        "    return evaluate_fairness(y_test, pred.labels.ravel(), prot_test, X_test)\n"
      ],
      "metadata": {
        "id": "paNuPAJlySlJ"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results['GridSearch'] = train_gridsearch(X_train, y_train, prot_train, X_test, y_test, prot_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njMrq0s_xgSb",
        "outputId": "2edd9559-8331-4391-90cf-c5beff57751f"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:110: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.disable_eager_execution()  # Required for TF1 compatibility in AIF360\n",
        "\n",
        "def train_adversarial_debiasing(X_train, y_train, prot_train, X_test, y_test, prot_test):\n",
        "    # Train data\n",
        "    tf.reset_default_graph()\n",
        "    df_train = pd.DataFrame(np.hstack((X_train, y_train.reshape(-1, 1), prot_train.reshape(-1, 1))),\n",
        "                            columns=[f\"x{i}\" for i in range(X_train.shape[1])] + [\"label\", \"protected\"])\n",
        "    bld_train = BinaryLabelDataset(df=df_train,\n",
        "                                   label_names=[\"label\"],\n",
        "                                   protected_attribute_names=[\"protected\"],\n",
        "                                   favorable_label=1,\n",
        "                                   unfavorable_label=0)\n",
        "\n",
        "    # Test data\n",
        "    df_test = pd.DataFrame(np.hstack((X_test, y_test.reshape(-1, 1), prot_test.reshape(-1, 1))),\n",
        "                           columns=[f\"x{i}\" for i in range(X_test.shape[1])] + [\"label\", \"protected\"])\n",
        "    bld_test = BinaryLabelDataset(df=df_test,\n",
        "                                  label_names=[\"label\"],\n",
        "                                  protected_attribute_names=[\"protected\"],\n",
        "                                  favorable_label=1,\n",
        "                                  unfavorable_label=0)\n",
        "\n",
        "    # TensorFlow session\n",
        "    sess = tf.Session()\n",
        "\n",
        "    clf = AdversarialDebiasing(\n",
        "        privileged_groups=[{'protected': 1}],\n",
        "        unprivileged_groups=[{'protected': 0}],\n",
        "        scope_name='adv_debiasing',\n",
        "        sess=sess,\n",
        "        num_epochs=50,\n",
        "        batch_size=64,\n",
        "        debias=True\n",
        "    )\n",
        "\n",
        "    clf.fit(bld_train)\n",
        "    pred = clf.predict(bld_test)\n",
        "\n",
        "    return evaluate_fairness(y_test, pred.labels.ravel(), prot_test, X_test)\n"
      ],
      "metadata": {
        "id": "H8z8KFCAzLIT"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results['AdversarialDebiasing'] = train_adversarial_debiasing(X_train, y_train, prot_train, X_test, y_test, prot_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FA25q1kMzMcd",
        "outputId": "2f9387f9-b0c3-4859-ba25-aa99ed6bae0b"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0; iter: 0; batch classifier loss: 0.631093; batch adversarial loss: 0.516974\n",
            "epoch 0; iter: 200; batch classifier loss: 0.628598; batch adversarial loss: 0.386894\n",
            "epoch 1; iter: 0; batch classifier loss: 0.458456; batch adversarial loss: 0.453721\n",
            "epoch 1; iter: 200; batch classifier loss: 0.549874; batch adversarial loss: 0.331929\n",
            "epoch 2; iter: 0; batch classifier loss: 0.496895; batch adversarial loss: 0.401980\n",
            "epoch 2; iter: 200; batch classifier loss: 0.410916; batch adversarial loss: 0.492252\n",
            "epoch 3; iter: 0; batch classifier loss: 0.481046; batch adversarial loss: 0.377303\n",
            "epoch 3; iter: 200; batch classifier loss: 0.471168; batch adversarial loss: 0.246060\n",
            "epoch 4; iter: 0; batch classifier loss: 0.438584; batch adversarial loss: 0.224712\n",
            "epoch 4; iter: 200; batch classifier loss: 0.455554; batch adversarial loss: 0.221404\n",
            "epoch 5; iter: 0; batch classifier loss: 0.582365; batch adversarial loss: 0.260500\n",
            "epoch 5; iter: 200; batch classifier loss: 0.362993; batch adversarial loss: 0.431218\n",
            "epoch 6; iter: 0; batch classifier loss: 0.434051; batch adversarial loss: 0.285998\n",
            "epoch 6; iter: 200; batch classifier loss: 0.532744; batch adversarial loss: 0.319849\n",
            "epoch 7; iter: 0; batch classifier loss: 0.448715; batch adversarial loss: 0.250980\n",
            "epoch 7; iter: 200; batch classifier loss: 0.545083; batch adversarial loss: 0.333852\n",
            "epoch 8; iter: 0; batch classifier loss: 0.387782; batch adversarial loss: 0.298711\n",
            "epoch 8; iter: 200; batch classifier loss: 0.458336; batch adversarial loss: 0.192827\n",
            "epoch 9; iter: 0; batch classifier loss: 0.532495; batch adversarial loss: 0.323449\n",
            "epoch 9; iter: 200; batch classifier loss: 0.347569; batch adversarial loss: 0.319242\n",
            "epoch 10; iter: 0; batch classifier loss: 0.393397; batch adversarial loss: 0.440744\n",
            "epoch 10; iter: 200; batch classifier loss: 0.442707; batch adversarial loss: 0.489013\n",
            "epoch 11; iter: 0; batch classifier loss: 0.577039; batch adversarial loss: 0.398945\n",
            "epoch 11; iter: 200; batch classifier loss: 0.527061; batch adversarial loss: 0.402400\n",
            "epoch 12; iter: 0; batch classifier loss: 0.548802; batch adversarial loss: 0.137169\n",
            "epoch 12; iter: 200; batch classifier loss: 0.607253; batch adversarial loss: 0.214978\n",
            "epoch 13; iter: 0; batch classifier loss: 0.403278; batch adversarial loss: 0.376396\n",
            "epoch 13; iter: 200; batch classifier loss: 0.452940; batch adversarial loss: 0.399322\n",
            "epoch 14; iter: 0; batch classifier loss: 0.484810; batch adversarial loss: 0.448491\n",
            "epoch 14; iter: 200; batch classifier loss: 0.499528; batch adversarial loss: 0.223560\n",
            "epoch 15; iter: 0; batch classifier loss: 0.477087; batch adversarial loss: 0.389851\n",
            "epoch 15; iter: 200; batch classifier loss: 0.458645; batch adversarial loss: 0.254673\n",
            "epoch 16; iter: 0; batch classifier loss: 0.502059; batch adversarial loss: 0.327983\n",
            "epoch 16; iter: 200; batch classifier loss: 0.328513; batch adversarial loss: 0.252208\n",
            "epoch 17; iter: 0; batch classifier loss: 0.430967; batch adversarial loss: 0.258883\n",
            "epoch 17; iter: 200; batch classifier loss: 0.465229; batch adversarial loss: 0.210259\n",
            "epoch 18; iter: 0; batch classifier loss: 0.532701; batch adversarial loss: 0.284910\n",
            "epoch 18; iter: 200; batch classifier loss: 0.472527; batch adversarial loss: 0.439951\n",
            "epoch 19; iter: 0; batch classifier loss: 0.571950; batch adversarial loss: 0.365654\n",
            "epoch 19; iter: 200; batch classifier loss: 0.601793; batch adversarial loss: 0.361480\n",
            "epoch 20; iter: 0; batch classifier loss: 0.664617; batch adversarial loss: 0.488349\n",
            "epoch 20; iter: 200; batch classifier loss: 0.533917; batch adversarial loss: 0.368699\n",
            "epoch 21; iter: 0; batch classifier loss: 0.488058; batch adversarial loss: 0.315812\n",
            "epoch 21; iter: 200; batch classifier loss: 0.466040; batch adversarial loss: 0.141956\n",
            "epoch 22; iter: 0; batch classifier loss: 0.486987; batch adversarial loss: 0.345554\n",
            "epoch 22; iter: 200; batch classifier loss: 0.557000; batch adversarial loss: 0.269260\n",
            "epoch 23; iter: 0; batch classifier loss: 0.388682; batch adversarial loss: 0.280809\n",
            "epoch 23; iter: 200; batch classifier loss: 0.312744; batch adversarial loss: 0.424503\n",
            "epoch 24; iter: 0; batch classifier loss: 0.400428; batch adversarial loss: 0.133731\n",
            "epoch 24; iter: 200; batch classifier loss: 0.493933; batch adversarial loss: 0.168084\n",
            "epoch 25; iter: 0; batch classifier loss: 0.438096; batch adversarial loss: 0.345049\n",
            "epoch 25; iter: 200; batch classifier loss: 0.408302; batch adversarial loss: 0.350128\n",
            "epoch 26; iter: 0; batch classifier loss: 0.424216; batch adversarial loss: 0.270485\n",
            "epoch 26; iter: 200; batch classifier loss: 0.264642; batch adversarial loss: 0.241939\n",
            "epoch 27; iter: 0; batch classifier loss: 0.566084; batch adversarial loss: 0.354350\n",
            "epoch 27; iter: 200; batch classifier loss: 0.532989; batch adversarial loss: 0.311079\n",
            "epoch 28; iter: 0; batch classifier loss: 0.413749; batch adversarial loss: 0.346277\n",
            "epoch 28; iter: 200; batch classifier loss: 0.363945; batch adversarial loss: 0.204432\n",
            "epoch 29; iter: 0; batch classifier loss: 0.353106; batch adversarial loss: 0.423663\n",
            "epoch 29; iter: 200; batch classifier loss: 0.321034; batch adversarial loss: 0.277417\n",
            "epoch 30; iter: 0; batch classifier loss: 0.304873; batch adversarial loss: 0.306663\n",
            "epoch 30; iter: 200; batch classifier loss: 0.507233; batch adversarial loss: 0.271736\n",
            "epoch 31; iter: 0; batch classifier loss: 0.271333; batch adversarial loss: 0.451550\n",
            "epoch 31; iter: 200; batch classifier loss: 0.541823; batch adversarial loss: 0.239062\n",
            "epoch 32; iter: 0; batch classifier loss: 0.463797; batch adversarial loss: 0.313451\n",
            "epoch 32; iter: 200; batch classifier loss: 0.521514; batch adversarial loss: 0.480700\n",
            "epoch 33; iter: 0; batch classifier loss: 0.479723; batch adversarial loss: 0.346301\n",
            "epoch 33; iter: 200; batch classifier loss: 0.400318; batch adversarial loss: 0.351145\n",
            "epoch 34; iter: 0; batch classifier loss: 0.440565; batch adversarial loss: 0.456164\n",
            "epoch 34; iter: 200; batch classifier loss: 0.534639; batch adversarial loss: 0.237688\n",
            "epoch 35; iter: 0; batch classifier loss: 0.353016; batch adversarial loss: 0.265245\n",
            "epoch 35; iter: 200; batch classifier loss: 0.461145; batch adversarial loss: 0.389595\n",
            "epoch 36; iter: 0; batch classifier loss: 0.399163; batch adversarial loss: 0.278798\n",
            "epoch 36; iter: 200; batch classifier loss: 0.460841; batch adversarial loss: 0.237834\n",
            "epoch 37; iter: 0; batch classifier loss: 0.456542; batch adversarial loss: 0.273688\n",
            "epoch 37; iter: 200; batch classifier loss: 0.428603; batch adversarial loss: 0.278407\n",
            "epoch 38; iter: 0; batch classifier loss: 0.406493; batch adversarial loss: 0.234165\n",
            "epoch 38; iter: 200; batch classifier loss: 0.487090; batch adversarial loss: 0.312042\n",
            "epoch 39; iter: 0; batch classifier loss: 0.420063; batch adversarial loss: 0.274592\n",
            "epoch 39; iter: 200; batch classifier loss: 0.491351; batch adversarial loss: 0.202421\n",
            "epoch 40; iter: 0; batch classifier loss: 0.511588; batch adversarial loss: 0.343652\n",
            "epoch 40; iter: 200; batch classifier loss: 0.474725; batch adversarial loss: 0.276492\n",
            "epoch 41; iter: 0; batch classifier loss: 0.458848; batch adversarial loss: 0.271445\n",
            "epoch 41; iter: 200; batch classifier loss: 0.394885; batch adversarial loss: 0.311410\n",
            "epoch 42; iter: 0; batch classifier loss: 0.497296; batch adversarial loss: 0.349262\n",
            "epoch 42; iter: 200; batch classifier loss: 0.508764; batch adversarial loss: 0.490105\n",
            "epoch 43; iter: 0; batch classifier loss: 0.379059; batch adversarial loss: 0.311486\n",
            "epoch 43; iter: 200; batch classifier loss: 0.441214; batch adversarial loss: 0.241843\n",
            "epoch 44; iter: 0; batch classifier loss: 0.395455; batch adversarial loss: 0.417002\n",
            "epoch 44; iter: 200; batch classifier loss: 0.342003; batch adversarial loss: 0.309667\n",
            "epoch 45; iter: 0; batch classifier loss: 0.477877; batch adversarial loss: 0.168591\n",
            "epoch 45; iter: 200; batch classifier loss: 0.508761; batch adversarial loss: 0.349068\n",
            "epoch 46; iter: 0; batch classifier loss: 0.529946; batch adversarial loss: 0.349700\n",
            "epoch 46; iter: 200; batch classifier loss: 0.610562; batch adversarial loss: 0.274067\n",
            "epoch 47; iter: 0; batch classifier loss: 0.426828; batch adversarial loss: 0.351367\n",
            "epoch 47; iter: 200; batch classifier loss: 0.474302; batch adversarial loss: 0.276205\n",
            "epoch 48; iter: 0; batch classifier loss: 0.396157; batch adversarial loss: 0.165582\n",
            "epoch 48; iter: 200; batch classifier loss: 0.583469; batch adversarial loss: 0.270695\n",
            "epoch 49; iter: 0; batch classifier loss: 0.446242; batch adversarial loss: 0.311936\n",
            "epoch 49; iter: 200; batch classifier loss: 0.387796; batch adversarial loss: 0.313536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Fairness Evaluation Across Compatible AIF360 In-processing Algorithms\\n\")\n",
        "for model, metrics in results.items():\n",
        "    print(f\"🔹 {model}\")\n",
        "    for metric_name, value in metrics.items():\n",
        "        print(f\"  {metric_name}: {value:.4f}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCfZUuo63kQ6",
        "outputId": "dc4cd3ea-2e77-4ba7-e229-c37b2dcc18cb"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fairness Evaluation Across Compatible AIF360 In-processing Algorithms\n",
            "\n",
            "🔹 GerryFair\n",
            "  accuracy: 0.8012\n",
            "  disparate_impact: 0.9767\n",
            "  statistical_parity_difference: -0.0223\n",
            "  equal_opportunity_difference: -0.0069\n",
            "\n",
            "🔹 PrejudiceRemover\n",
            "  accuracy: 0.8099\n",
            "  disparate_impact: 0.9902\n",
            "  statistical_parity_difference: -0.0091\n",
            "  equal_opportunity_difference: 0.0013\n",
            "\n",
            "🔹 ExponentiatedGradient\n",
            "  accuracy: 0.8108\n",
            "  disparate_impact: 0.9641\n",
            "  statistical_parity_difference: -0.0336\n",
            "  equal_opportunity_difference: -0.0088\n",
            "\n",
            "🔹 GridSearch\n",
            "  accuracy: 0.8099\n",
            "  disparate_impact: 0.9801\n",
            "  statistical_parity_difference: -0.0186\n",
            "  equal_opportunity_difference: -0.0023\n",
            "\n",
            "🔹 AdversarialDebiasing\n",
            "  accuracy: 0.8214\n",
            "  disparate_impact: 0.9310\n",
            "  statistical_parity_difference: -0.0619\n",
            "  equal_opportunity_difference: -0.0218\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#postprocessing"
      ],
      "metadata": {
        "id": "z5hWBXLxJMAE"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test, prot_train, prot_test = load_taiwan_credit_data_inprocess()\n",
        "post_results = {}"
      ],
      "metadata": {
        "id": "jdC4dnm-JvEo"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_roc_postprocessing_with_xgb(X_train, y_train, prot_train, X_test, y_test, prot_test):\n",
        "    model = train_baseline_model(X_train, y_train)\n",
        "\n",
        "    y_prob = model.predict_proba(X_test)[:, 1]\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    bld_test = BinaryLabelDataset(\n",
        "        favorable_label=1,\n",
        "        unfavorable_label=0,\n",
        "        df=pd.DataFrame(np.hstack((X_test, y_test[:, None], prot_test[:, None])),\n",
        "                        columns=[f'x{i}' for i in range(X_test.shape[1])] + ['label', 'protected']),\n",
        "        label_names=['label'],\n",
        "        protected_attribute_names=['protected']\n",
        "    )\n",
        "\n",
        "    bld_pred = bld_test.copy()\n",
        "    bld_pred.scores = y_prob.reshape(-1, 1)\n",
        "    bld_pred.labels = y_pred.reshape(-1, 1)\n",
        "\n",
        "    roc = RejectOptionClassification(\n",
        "        unprivileged_groups=[{'protected': 0}],\n",
        "        privileged_groups=[{'protected': 1}],\n",
        "        low_class_thresh=0.01, high_class_thresh=0.99,\n",
        "        num_class_thresh=100, num_ROC_margin=50,\n",
        "        metric_name=\"Statistical parity difference\",\n",
        "        metric_ub=0.05, metric_lb=-0.05\n",
        "    )\n",
        "    roc = roc.fit(bld_test, bld_pred)\n",
        "    pred = roc.predict(bld_pred)\n",
        "\n",
        "    return evaluate_fairness(y_test, pred.labels.ravel(), prot_test, X_test)\n"
      ],
      "metadata": {
        "id": "KAaGOOMv3ssO"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "post_results['RejectOptionClassification'] = train_roc_postprocessing_with_xgb(X_train, y_train, prot_train, X_test, y_test, prot_test)\n"
      ],
      "metadata": {
        "id": "5khEknevMU9G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6480732e-77ed-4722-edd6-77a4c7495b33"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:08:39] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_calibrated_eq_odds_with_xgb(X_train, y_train, prot_train, X_test, y_test, prot_test):\n",
        "    model = train_baseline_model(X_train, y_train)\n",
        "\n",
        "    y_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    bld_test = BinaryLabelDataset(\n",
        "        favorable_label=1,\n",
        "        unfavorable_label=0,\n",
        "        df=pd.DataFrame(np.hstack((X_test, y_test[:, None], prot_test[:, None])),\n",
        "                        columns=[f'x{i}' for i in range(X_test.shape[1])] + ['label', 'protected']),\n",
        "        label_names=['label'],\n",
        "        protected_attribute_names=['protected']\n",
        "    )\n",
        "\n",
        "    bld_pred = bld_test.copy()\n",
        "    bld_pred.scores = y_prob.reshape(-1, 1)\n",
        "\n",
        "    ceo = CalibratedEqOddsPostprocessing(\n",
        "        privileged_groups=[{'protected': 1}],\n",
        "        unprivileged_groups=[{'protected': 0}],\n",
        "        cost_constraint=\"fnr\",\n",
        "        seed=42\n",
        "    )\n",
        "    ceo = ceo.fit(bld_test, bld_pred)\n",
        "    pred = ceo.predict(bld_pred)\n",
        "\n",
        "    return evaluate_fairness(y_test, pred.labels.ravel(), prot_test, X_test)\n"
      ],
      "metadata": {
        "id": "4-lEz-kD6JUr"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "post_results['CalibratedEqOdds'] = train_calibrated_eq_odds_with_xgb(X_train, y_train, prot_train, X_test, y_test, prot_test)\n"
      ],
      "metadata": {
        "id": "1AunQ9VNMWxS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "622c9c41-3bf8-4388-dd6e-a1bd6164f2a5"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:09:21] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_equalized_odds_with_xgb(X_train, y_train, prot_train, X_test, y_test, prot_test):\n",
        "    model = train_baseline_model(X_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    bld_test = BinaryLabelDataset(\n",
        "        favorable_label=1,\n",
        "        unfavorable_label=0,\n",
        "        df=pd.DataFrame(np.hstack((X_test, y_test[:, None], prot_test[:, None])),\n",
        "                        columns=[f'x{i}' for i in range(X_test.shape[1])] + ['label', 'protected']),\n",
        "        label_names=['label'],\n",
        "        protected_attribute_names=['protected']\n",
        "    )\n",
        "\n",
        "    bld_pred = bld_test.copy()\n",
        "    bld_pred.labels = y_pred.reshape(-1, 1)\n",
        "\n",
        "    eq = EqOddsPostprocessing(\n",
        "        privileged_groups=[{'protected': 1}],\n",
        "        unprivileged_groups=[{'protected': 0}]\n",
        "    )\n",
        "    eq = eq.fit(bld_test, bld_pred)\n",
        "    pred = eq.predict(bld_pred)\n",
        "\n",
        "    return evaluate_fairness(y_test, pred.labels.ravel(), prot_test, X_test)\n"
      ],
      "metadata": {
        "id": "SjdA2g9P6LWG"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "post_results['EqualizedOdds'] = train_equalized_odds_with_xgb(X_train, y_train, prot_train, X_test, y_test, prot_test)"
      ],
      "metadata": {
        "id": "jDYTrQqF6Opo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc5e3e82-b2af-4f14-f094-388c27633917"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:09:23] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for model, metrics in post_results.items():\n",
        "    print(f\"🔹 {model}\")\n",
        "    for k, v in metrics.items():\n",
        "        print(f\"  {k}: {v:.4f}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pxj5oi7v6Y5a",
        "outputId": "825f09cc-b45f-406f-99dd-f336286f1f07"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 RejectOptionClassification\n",
            "  accuracy: 0.7416\n",
            "  disparate_impact: 0.9301\n",
            "  statistical_parity_difference: -0.0481\n",
            "  equal_opportunity_difference: -0.0114\n",
            "\n",
            "🔹 CalibratedEqOdds\n",
            "  accuracy: 0.7981\n",
            "  disparate_impact: 0.8616\n",
            "  statistical_parity_difference: -0.1322\n",
            "  equal_opportunity_difference: -0.0501\n",
            "\n",
            "🔹 EqualizedOdds\n",
            "  accuracy: 0.8129\n",
            "  disparate_impact: 0.9731\n",
            "  statistical_parity_difference: -0.0237\n",
            "  equal_opportunity_difference: -0.0014\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AHL-3AJmLOdv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}