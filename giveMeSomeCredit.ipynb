{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "665f4228-61c8-45f0-beb2-7e9f4481aea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "10d46c05-0e00-4c81-84e4-850a9f512abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bafbfd6f-0210-4a9d-94fc-021ed0ee8649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a3e42370-c8a8-489d-bae3-d9db4ad2add1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics import ClassificationMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d1100037-6aee-4d18-badb-7d75b54dba27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.algorithms.preprocessing import (\n",
    "    Reweighing,\n",
    "    DisparateImpactRemover,\n",
    "    LFR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9befe73e-17ea-4afb-bfb2-a78322627677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.algorithms.inprocessing import (\n",
    "    MetaFairClassifier,\n",
    "    GerryFairClassifier,\n",
    "    PrejudiceRemover,\n",
    "    ExponentiatedGradientReduction,\n",
    "    GridSearchReduction,\n",
    "    ARTClassifier,\n",
    "    AdversarialDebiasing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "93adf662-a1c5-4844-8695-5a82d5c94534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.algorithms.postprocessing import (\n",
    "    RejectOptionClassification,\n",
    "    CalibratedEqOddsPostprocessing,\n",
    "    EqOddsPostprocessing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f17eadf7-3bd8-4371-879f-3234cf8e1f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "f6311641-4754-4ad3-850a-e14b67df2a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gmsc():\n",
    "    df = pd.read_csv(r\"C:\\Users\\niyat\\OneDrive\\Documents\\Course Content\\CS516\\Project\\CS516_Project\\gmsc-training.csv\")\n",
    "    df = df.drop(columns=[\"Unnamed: 0\"])  \n",
    "    df.rename(columns={\"SeriousDlqin2yrs\": \"label\"}, inplace=True)\n",
    "\n",
    "    # Treat age <= 25 as unprivileged\n",
    "    df['age_binary'] = df['age'].apply(lambda x: 1 if x > 25 else 0)\n",
    "\n",
    "    # Fill missing values if any (basic cleaning)\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "\n",
    "    print(\"Null counts after filling:\")\n",
    "    print(df.isnull().sum()[df.isnull().sum() > 0])\n",
    "\n",
    "    X = df.drop(columns=['label', 'age_binary', 'age'])\n",
    "    y = df['label']\n",
    "    protected = df['age_binary']\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test, prot_train, prot_test = train_test_split(\n",
    "        X_scaled, y, protected, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, prot_train, prot_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "31eda183-0e6b-41f0-b650-93da0a726287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null counts after filling:\n",
      "Series([], dtype: int64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-0.02421753, -0.10041896, -0.17316509, ..., -0.90128301,\n",
       "         -0.05785249, -0.66612604],\n",
       "        [-0.02408498, -0.10041896, -0.17301992, ..., -0.01614492,\n",
       "         -0.05785249,  0.23720186],\n",
       "        [-0.02418571, -0.10041896, -0.17289105, ..., -0.01614492,\n",
       "         -0.05785249, -0.66612604],\n",
       "        ...,\n",
       "        [-0.02421753, -0.10041896, -0.17306934, ..., -0.01614492,\n",
       "         -0.05785249,  0.23720186],\n",
       "        [-0.02051314, -0.10041896, -0.17303615, ..., -0.90128301,\n",
       "         -0.05785249, -0.66612604],\n",
       "        [-0.0202136 ,  0.138087  ,  0.25173813, ..., -0.90128301,\n",
       "          0.18281181, -0.66612604]]),\n",
       " array([[-0.02421753, -0.10041896, -0.17318459, ..., -0.90128301,\n",
       "          0.18281181, -0.66612604],\n",
       "        [-0.02419443, -0.10041896, -0.17319631, ..., -0.01614492,\n",
       "         -0.05785249, -0.66612604],\n",
       "        [-0.02098353, -0.10041896, -0.1729547 , ...,  0.86899317,\n",
       "         -0.05785249,  0.23720186],\n",
       "        ...,\n",
       "        [-0.0202136 , -0.10041896, -0.17319827, ..., -0.90128301,\n",
       "         -0.05785249,  1.14052977],\n",
       "        [-0.02413482, -0.10041896, -0.15212646, ..., -0.90128301,\n",
       "         -0.05785249, -0.66612604],\n",
       "        [-0.02397863, -0.10041896, -0.17322252, ..., -0.90128301,\n",
       "         -0.05785249,  2.04385767]]),\n",
       " 138985    0\n",
       " 63964     0\n",
       " 46077     0\n",
       " 83030     0\n",
       " 11184     0\n",
       "          ..\n",
       " 90431     0\n",
       " 18048     0\n",
       " 3895      0\n",
       " 74354     0\n",
       " 80530     0\n",
       " Name: label, Length: 105000, dtype: int64,\n",
       " 31294     1\n",
       " 96643     0\n",
       " 144649    0\n",
       " 96911     0\n",
       " 120437    1\n",
       "          ..\n",
       " 113350    1\n",
       " 126613    0\n",
       " 137307    0\n",
       " 87700     0\n",
       " 93695     0\n",
       " Name: label, Length: 45000, dtype: int64,\n",
       " 138985    1\n",
       " 63964     1\n",
       " 46077     1\n",
       " 83030     1\n",
       " 11184     1\n",
       "          ..\n",
       " 90431     1\n",
       " 18048     1\n",
       " 3895      1\n",
       " 74354     1\n",
       " 80530     1\n",
       " Name: age_binary, Length: 105000, dtype: int64,\n",
       " 31294     1\n",
       " 96643     1\n",
       " 144649    1\n",
       " 96911     1\n",
       " 120437    1\n",
       "          ..\n",
       " 113350    1\n",
       " 126613    1\n",
       " 137307    1\n",
       " 87700     1\n",
       " 93695     1\n",
       " Name: age_binary, Length: 45000, dtype: int64)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_gmsc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "461c1d97-01b0-4abe-9bcb-6eb8dcd1f905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_give_me_some_credit():\n",
    "    df = pd.read_csv(r\"C:\\Users\\niyat\\OneDrive\\Documents\\Course Content\\CS516\\Project\\CS516_Project\\gmsc-training.csv\")\n",
    "    df.rename(columns={\"SeriousDlqin2yrs\": \"label\"}, inplace=True)\n",
    "    df['age_binary'] = df['age'].apply(lambda x: 1 if x >= 25 else 0)\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "\n",
    "    X_raw = df.drop(columns=['label', 'age', 'age_binary'])\n",
    "    y = df['label'].values\n",
    "    prot = df['age_binary'].values\n",
    "\n",
    "    return X_raw, y, prot\n",
    "\n",
    "def preprocess_data(X_raw, y, prot):\n",
    "    cat_cols = X_raw.select_dtypes(include='object').columns.tolist()\n",
    "    num_cols = X_raw.select_dtypes(exclude='object').columns.tolist()\n",
    "\n",
    "    X_raw_train, X_raw_test, y_train, y_test, prot_train, prot_test = train_test_split(\n",
    "        X_raw, y, prot, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    encoder = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "    X_train_cat = encoder.fit_transform(X_raw_train[cat_cols])\n",
    "    X_test_cat = encoder.transform(X_raw_test[cat_cols])\n",
    "\n",
    "    X_train_num = X_raw_train[num_cols].values\n",
    "    X_test_num = X_raw_test[num_cols].values\n",
    "\n",
    "    X_train = np.hstack((X_train_num, X_train_cat))\n",
    "    X_test = np.hstack((X_test_num, X_test_cat))\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, prot_train, prot_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "16e9680e-c1f3-4356-87cc-a3b83106b714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gmsc_inprocess():\n",
    "    df = pd.read_csv(r\"C:\\Users\\niyat\\OneDrive\\Documents\\Course Content\\CS516\\Project\\CS516_Project\\gmsc-training.csv\")\n",
    "    df.rename(columns={\"SeriousDlqin2yrs\": \"label\"}, inplace=True)\n",
    "    df['age_binary'] = df['age'].apply(lambda x: 1 if x >= 25 else 0)\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "\n",
    "    X_raw = df.drop(columns=['label', 'age', 'age_binary'])\n",
    "    y = df['label'].values\n",
    "    prot = df['age_binary'].values\n",
    "\n",
    "    X_train_raw, X_test_raw, y_train, y_test, prot_train, prot_test = train_test_split(\n",
    "        X_raw, y, prot, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    categorical_cols = X_raw.select_dtypes(include='object').columns.tolist()\n",
    "    numeric_cols = X_raw.select_dtypes(exclude='object').columns.tolist()\n",
    "\n",
    "    encoder = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "    X_train_cat = encoder.fit_transform(X_train_raw[categorical_cols])\n",
    "    X_test_cat = encoder.transform(X_test_raw[categorical_cols])\n",
    "\n",
    "    X_train_num = X_train_raw[numeric_cols].values\n",
    "    X_test_num = X_test_raw[numeric_cols].values\n",
    "\n",
    "    X_train = np.hstack((X_train_num, X_train_cat))\n",
    "    X_test = np.hstack((X_test_num, X_test_cat))\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, prot_train, prot_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "070d3f48-baca-4830-9d0f-1fcb4ecb716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_baseline_model(X_train, y_train, sample_weight=None):\n",
    "    model = XGBClassifier(eval_metric='logloss', use_label_encoder=False)\n",
    "    model.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6f97857f-9f14-41e0-8b36-3d555a21f092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fairness(y_true, y_pred, prot, X=None):\n",
    "    df = pd.DataFrame(np.hstack((X, y_true[:, None], prot[:, None])),\n",
    "                      columns=[f\"x{i}\" for i in range(X.shape[1])] + ['label', 'protected'])\n",
    "\n",
    "    dataset_true = BinaryLabelDataset(df=df,\n",
    "                                      label_names=[\"label\"],\n",
    "                                      protected_attribute_names=[\"protected\"],\n",
    "                                      favorable_label=0, unfavorable_label=1)\n",
    "\n",
    "    pred_dataset = dataset_true.copy()\n",
    "    pred_dataset.labels = y_pred.reshape(-1, 1)\n",
    "\n",
    "    metric = ClassificationMetric(dataset_true, pred_dataset,\n",
    "                                  privileged_groups=[{'protected': 1}],\n",
    "                                  unprivileged_groups=[{'protected': 0}])\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'disparate_impact': metric.disparate_impact(),\n",
    "        'statistical_parity_difference': metric.statistical_parity_difference(),\n",
    "        'equal_opportunity_difference': metric.equal_opportunity_difference()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e3e34149-f3e6-4e8e-8a67-8553d90ce49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, prot_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    return evaluate_fairness(y_test, y_pred, prot_test, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dd1e5826-f18c-4e66-b805-86f97430ae2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null counts after filling:\n",
      "Series([], dtype: int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niyat\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [01:55:47] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9351111111111111,\n",
       " 'disparate_impact': 0.9862524572485823,\n",
       " 'statistical_parity_difference': -0.013419901443461502,\n",
       " 'equal_opportunity_difference': -0.008154767458723122}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#XGboost\n",
    "X_train, X_test, y_train, y_test, prot_train, prot_test = load_gmsc()\n",
    "baseline_model = train_baseline_model(X_train, y_train)\n",
    "baseline_metrics = evaluate_model(baseline_model, X_test, y_test.to_numpy(), prot_test.to_numpy())\n",
    "\n",
    "baseline_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2f84a9b3-fd1e-4ce5-ae04-4abb49b470fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "##disparate impact\n",
    "def apply_disparate_impact_remover(X_train, y_train, prot_train, repair_level=1.0):\n",
    "    df = pd.DataFrame(X_train)\n",
    "    df['target'] = y_train.values if hasattr(y_train, 'values') else y_train\n",
    "    df['protected'] = prot_train.values if hasattr(prot_train, 'values') else prot_train\n",
    "\n",
    "    dataset = BinaryLabelDataset(\n",
    "        favorable_label=0,  # <-- GOOD outcome = 0 for GMSC\n",
    "        unfavorable_label=1,\n",
    "        df=df,\n",
    "        label_names=['target'],\n",
    "        protected_attribute_names=['protected']\n",
    "    )\n",
    "\n",
    "    dir_remover = DisparateImpactRemover(repair_level=repair_level)\n",
    "    repaired_dataset = dir_remover.fit_transform(dataset)\n",
    "\n",
    "    X_repaired = pd.DataFrame(repaired_dataset.features)\n",
    "    y_repaired = pd.Series(repaired_dataset.labels.ravel())\n",
    "    prot_repaired = pd.Series(repaired_dataset.protected_attributes.ravel())\n",
    "\n",
    "    return X_repaired, y_repaired, prot_repaired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c8c5e310-07bf-4065-85ec-c10e0c0959e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inprocess_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7acbe9b5-1a0d-4688-b93f-66e432de9278",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niyat\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [01:59:46] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9344444444444444,\n",
       " 'disparate_impact': 0.9679337014750431,\n",
       " 'statistical_parity_difference': -0.03142308135048755,\n",
       " 'equal_opportunity_difference': -0.02367341050221161}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply DIR ONLY to training data\n",
    "X_train_repaired, y_train_repaired, prot_train_repaired = apply_disparate_impact_remover(\n",
    "    pd.DataFrame(X_train), y_train, prot_train\n",
    ")\n",
    "\n",
    "# Split repaired training data into sub-train and validation\n",
    "X_subtrain, X_val, y_subtrain, y_val, prot_subtrain, prot_val = train_test_split(\n",
    "    X_train_repaired, y_train_repaired, prot_train_repaired, test_size=0.3, random_state=42, stratify=y_train_repaired\n",
    ")\n",
    "\n",
    "# Train model on sub-train\n",
    "fair_model = train_baseline_model(X_subtrain, y_subtrain)\n",
    "\n",
    "# Evaluate model on repaired validation\n",
    "fair_metrics = evaluate_model(\n",
    "    fair_model,\n",
    "    X_val.to_numpy(),\n",
    "    y_val.to_numpy(),\n",
    "    prot_val.to_numpy()\n",
    ")\n",
    "inprocess_results['Disparate_Impact_Remover'] = fair_metrics\n",
    "fair_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "133f3630-166e-434d-ace8-9eb408ee8341",
   "metadata": {},
   "outputs": [],
   "source": [
    "##LFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c448071f-a53e-4c0b-bee6-92582c5b5873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lfr(X_train, y_train, prot_train):\n",
    "    df = pd.DataFrame(X_train)\n",
    "    df['target'] = y_train.values\n",
    "    df['protected'] = prot_train.values\n",
    "\n",
    "    dataset = BinaryLabelDataset(\n",
    "        favorable_label=0,\n",
    "        unfavorable_label=1,\n",
    "        df=df,\n",
    "        label_names=['target'],\n",
    "        protected_attribute_names=['protected']\n",
    "    )\n",
    "\n",
    "    lfr = LFR(unprivileged_groups=[{'protected': 0}],\n",
    "          privileged_groups=[{'protected': 1}],\n",
    "          k=10, Ax=0.01, Ay=1.0, Az=0.1, verbose=0)\n",
    "\n",
    "    lfr.fit(dataset)\n",
    "    transformed_dataset = lfr.transform(dataset)\n",
    "\n",
    "    X_transformed = pd.DataFrame(transformed_dataset.features)\n",
    "    y_transformed = pd.Series(transformed_dataset.labels.ravel())\n",
    "    prot_transformed = pd.Series(transformed_dataset.protected_attributes.ravel())\n",
    "\n",
    "    return X_transformed, y_transformed, prot_transformed, lfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "939b219c-422f-4925-9010-7f945614723b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lfr, y_train_lfr, prot_train_lfr, lfr_model = apply_lfr(pd.DataFrame(X_train), y_train, prot_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "11004253-b8a2-4763-9f7c-44edf518793d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in y_train_lfr: [0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique labels in y_train_lfr:\", np.unique(y_train_lfr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0ae50d92-d376-4a57-8159-7e3bd3e3b9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niyat\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [02:50:25] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "fair_lfr_model = train_baseline_model(X_train_lfr, y_train_lfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "84d6c5a0-ca61-4443-9719-96fcf29dde78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9979111111111111,\n",
       " 'disparate_impact': 0.998858484460229,\n",
       " 'statistical_parity_difference': -0.0011414896791077656,\n",
       " 'equal_opportunity_difference': -0.0011865043320410384}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply LFR transformation on test set\n",
    "df_test = pd.DataFrame(X_test)\n",
    "df_test['target'] = y_test.values\n",
    "df_test['protected'] = prot_test.values\n",
    "\n",
    "test_dataset = BinaryLabelDataset(\n",
    "    favorable_label=0,\n",
    "    unfavorable_label=1,\n",
    "    df=df_test,\n",
    "    label_names=['target'],\n",
    "    protected_attribute_names=['protected']\n",
    ")\n",
    "\n",
    "transformed_test_dataset = lfr_model.transform(test_dataset)\n",
    "\n",
    "X_test_lfr = pd.DataFrame(transformed_test_dataset.features)\n",
    "y_test_lfr = pd.Series(transformed_test_dataset.labels.ravel())\n",
    "prot_test_lfr = pd.Series(transformed_test_dataset.protected_attributes.ravel())\n",
    "\n",
    "# Evaluate\n",
    "fair_lfr_metrics = evaluate_model(\n",
    "    fair_lfr_model,\n",
    "    X_test_lfr.to_numpy(),\n",
    "    y_test_lfr.to_numpy(),\n",
    "    prot_test_lfr.to_numpy()\n",
    ")\n",
    "inprocess_results['LFR']=fair_lfr_metrics\n",
    "\n",
    "fair_lfr_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6dc25394-3463-4068-a77e-527e418e2380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 1.]), array([44908,    92], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y_test_lfr, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c54bdfef-1220-4141-9a75-12958512ce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reweighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "b258ded4-8982-4d0f-8fdc-9a61dee16486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_reweighing(X_train, y_train, prot_train):\n",
    "    df_train = pd.DataFrame(np.hstack((X_train, y_train.reshape(-1, 1), prot_train.reshape(-1, 1))),\n",
    "                            columns=[f\"x{i}\" for i in range(X_train.shape[1])] + [\"label\", \"protected\"])\n",
    "    bld_train = BinaryLabelDataset(\n",
    "        favorable_label=0,\n",
    "        unfavorable_label=1,\n",
    "        df=df_train,\n",
    "        label_names=[\"label\"],\n",
    "        protected_attribute_names=[\"protected\"]\n",
    "    )\n",
    "    RW = Reweighing(unprivileged_groups=[{'protected': 0}], privileged_groups=[{'protected': 1}])\n",
    "    bld_rw = RW.fit_transform(bld_train)\n",
    "\n",
    "    return bld_rw.features, bld_rw.labels.ravel(), bld_rw.instance_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "f3d911c3-21e4-4878-9329-bf49418e65bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niyat\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [03:40:01] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9312666666666667,\n",
       " 'disparate_impact': 0.9978325765982927,\n",
       " 'statistical_parity_difference': -0.0020953775727402535,\n",
       " 'equal_opportunity_difference': -0.0012583229239316873}"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, prot_train, prot_test = preprocess_data(*load_give_me_some_credit())\n",
    "\n",
    "X_rw, y_rw, sample_weights = apply_reweighing(X_train, y_train, prot_train)\n",
    "\n",
    "expected_num_features = X_rw.shape[1]\n",
    "\n",
    "if X_test.shape[1] < expected_num_features:\n",
    "    padding = expected_num_features - X_test.shape[1]\n",
    "    X_test_aligned = np.hstack((X_test, np.zeros((X_test.shape[0], padding))))\n",
    "elif X_test.shape[1] > expected_num_features:\n",
    "    X_test_aligned = X_test[:, :expected_num_features]\n",
    "else:\n",
    "    X_test_aligned = X_test\n",
    "\n",
    "model_rw = train_baseline_model(X_rw, y_rw, sample_weight=sample_weights)\n",
    "\n",
    "reweighing_metric = evaluate_model(model_rw, X_test_aligned, y_test, prot_test)\n",
    "\n",
    "inprocess_results['reweighing'] = reweighing_metric\n",
    "reweighing_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "3570eed6-54a6-4b37-afb7-544e092afe4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Disparate_Impact_Remover\n",
      "  accuracy: 0.9344\n",
      "  disparate_impact: 0.9679\n",
      "  statistical_parity_difference: -0.0314\n",
      "  equal_opportunity_difference: -0.0237\n",
      "\n",
      "🔹 LFR\n",
      "  accuracy: 0.9979\n",
      "  disparate_impact: 0.9989\n",
      "  statistical_parity_difference: -0.0011\n",
      "  equal_opportunity_difference: -0.0012\n",
      "\n",
      "🔹 reweighing\n",
      "  accuracy: 0.9313\n",
      "  disparate_impact: 0.9978\n",
      "  statistical_parity_difference: -0.0021\n",
      "  equal_opportunity_difference: -0.0013\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model, metrics in inprocess_results.items():\n",
    "    print(f\"🔹 {model}\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86aee4e-04ca-478a-b6e4-3d21d22ea555",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "69786608-b4de-408a-8e19-5d4c9502e0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, prot_train, prot_test = load_gmsc_inprocess()\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "583ba3c8-b431-498a-8be2-b1bdf7383a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gerryfair(X_train, y_train, prot_train, X_test, y_test, prot_test):\n",
    "    df_train = pd.DataFrame(np.hstack((X_train, y_train[:, None], prot_train[:, None])),\n",
    "                            columns=[f\"x{i}\" for i in range(X_train.shape[1])] + [\"label\", \"protected\"])\n",
    "    bld_train = BinaryLabelDataset(df=df_train, label_names=[\"label\"], protected_attribute_names=[\"protected\"],\n",
    "                                   favorable_label=1, unfavorable_label=0)\n",
    "\n",
    "    df_test = pd.DataFrame(np.hstack((X_test, y_test[:, None], prot_test[:, None])),\n",
    "                           columns=[f\"x{i}\" for i in range(X_test.shape[1])] + [\"label\", \"protected\"])\n",
    "    bld_test = BinaryLabelDataset(df=df_test, label_names=[\"label\"], protected_attribute_names=[\"protected\"],\n",
    "                                  favorable_label=0, unfavorable_label=1)\n",
    "\n",
    "    clf = GerryFairClassifier(C=100, printflag=False, gamma=0.005, fairness_def='FP', max_iters=50)\n",
    "    clf.fit(bld_train)\n",
    "    pred = clf.predict(bld_test)\n",
    "\n",
    "    return evaluate_fairness(y_test, pred.labels.ravel(), prot_test, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "101f880f-199d-48e4-b350-27d3662020c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['GerryFair'] = train_gerryfair(X_train, y_train, prot_train, X_test, y_test, prot_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "922c7ba7-dedb-4c5d-8337-3b0bef8a2d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_prejudice_remover(X_train, y_train, prot_train, X_test, y_test, prot_test):\n",
    "    df_train = pd.DataFrame(np.hstack((X_train, y_train.reshape(-1, 1), prot_train.reshape(-1, 1))),\n",
    "                            columns=[f\"x{i}\" for i in range(X_train.shape[1])] + [\"label\", \"protected\"])\n",
    "    bld_train = BinaryLabelDataset(df=df_train,\n",
    "                                   label_names=[\"label\"],\n",
    "                                   protected_attribute_names=[\"protected\"],\n",
    "                                   favorable_label=0,\n",
    "                                   unfavorable_label=1)\n",
    "\n",
    "    df_test = pd.DataFrame(np.hstack((X_test, y_test.reshape(-1, 1), prot_test.reshape(-1, 1))),\n",
    "                           columns=[f\"x{i}\" for i in range(X_test.shape[1])] + [\"label\", \"protected\"])\n",
    "    bld_test = BinaryLabelDataset(df=df_test,\n",
    "                                  label_names=[\"label\"],\n",
    "                                  protected_attribute_names=[\"protected\"],\n",
    "                                  favorable_label=0,\n",
    "                                  unfavorable_label=1)\n",
    "\n",
    "    clf = PrejudiceRemover(sensitive_attr=\"protected\", eta=25.0)\n",
    "    clf.fit(bld_train)\n",
    "    pred = clf.predict(bld_test)\n",
    "\n",
    "    return evaluate_fairness(y_test, pred.labels.ravel(), prot_test, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "854bd43e-b0c6-4c9f-984a-707befbb9289",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['PrejudiceRemover'] = train_prejudice_remover(X_train, y_train, prot_train, X_test, y_test, prot_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "061c2dd4-8631-48db-877f-2b792fc0b22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_expgrad(X_train, y_train, prot_train, X_test, y_test, prot_test):\n",
    "    df_train = pd.DataFrame(np.hstack((X_train, y_train[:, None], prot_train[:, None])),\n",
    "                            columns=[f\"x{i}\" for i in range(X_train.shape[1])] + [\"label\", \"protected\"])\n",
    "    bld_train = BinaryLabelDataset(df=df_train,\n",
    "                                   label_names=[\"label\"],\n",
    "                                   protected_attribute_names=[\"protected\"],\n",
    "                                   favorable_label=0,\n",
    "                                   unfavorable_label=1)\n",
    "\n",
    "    df_test = pd.DataFrame(np.hstack((X_test, y_test[:, None], prot_test[:, None])),\n",
    "                           columns=[f\"x{i}\" for i in range(X_test.shape[1])] + [\"label\", \"protected\"])\n",
    "    bld_test = BinaryLabelDataset(df=df_test,\n",
    "                                  label_names=[\"label\"],\n",
    "                                  protected_attribute_names=[\"protected\"],\n",
    "                                  favorable_label=0,\n",
    "                                  unfavorable_label=1)\n",
    "\n",
    "    expgrad = ExponentiatedGradientReduction(\n",
    "        estimator=LogisticRegression(solver='liblinear'),\n",
    "        constraints=\"DemographicParity\"\n",
    "    )\n",
    "    expgrad.fit(bld_train)\n",
    "\n",
    "    pred = expgrad.predict(bld_test)\n",
    "    return evaluate_fairness(y_test, pred.labels.ravel(), prot_test, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "a103effb-d435-4df2-a932-b9f1f3dbb5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niyat\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "results['ExponentiatedGradient'] = train_expgrad(X_train, y_train, prot_train, X_test, y_test, prot_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "f766e6d8-5d07-47a2-9a50-14e44a44d857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gridsearch(X_train, y_train, prot_train, X_test, y_test, prot_test):\n",
    "    df_train = pd.DataFrame(np.hstack((X_train, y_train[:, None], prot_train[:, None])),\n",
    "                            columns=[f\"x{i}\" for i in range(X_train.shape[1])] + [\"label\", \"protected\"])\n",
    "    bld_train = BinaryLabelDataset(df=df_train,\n",
    "                                   label_names=[\"label\"],\n",
    "                                   protected_attribute_names=[\"protected\"],\n",
    "                                   favorable_label=0,\n",
    "                                   unfavorable_label=1)\n",
    "\n",
    "    df_test = pd.DataFrame(np.hstack((X_test, y_test[:, None], prot_test[:, None])),\n",
    "                           columns=[f\"x{i}\" for i in range(X_test.shape[1])] + [\"label\", \"protected\"])\n",
    "    bld_test = BinaryLabelDataset(df=df_test,\n",
    "                                  label_names=[\"label\"],\n",
    "                                  protected_attribute_names=[\"protected\"],\n",
    "                                  favorable_label=0,\n",
    "                                  unfavorable_label=1)\n",
    "\n",
    "    grid = GridSearchReduction(\n",
    "        estimator=LogisticRegression(solver='liblinear'),\n",
    "        constraints=\"DemographicParity\"\n",
    "    )\n",
    "    grid.fit(bld_train)\n",
    "\n",
    "    pred = grid.predict(bld_test)\n",
    "    return evaluate_fairness(y_test, pred.labels.ravel(), prot_test, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "3ae95dac-6c4c-41ee-9bf2-6e0d8468ffcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niyat\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "results['GridSearch'] = train_gridsearch(X_train, y_train, prot_train, X_test, y_test, prot_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "7cebb81e-2672-4318-a1d7-d5c4caed734d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "3431a154-7a4f-4f9d-8d7a-26b4aced0d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adversarial_debiasing(X_train, y_train, prot_train, X_test, y_test, prot_test):\n",
    "    # Train data\n",
    "    tf.reset_default_graph()\n",
    "    df_train = pd.DataFrame(np.hstack((X_train, y_train.reshape(-1, 1), prot_train.reshape(-1, 1))),\n",
    "                            columns=[f\"x{i}\" for i in range(X_train.shape[1])] + [\"label\", \"protected\"])\n",
    "    bld_train = BinaryLabelDataset(df=df_train,\n",
    "                                   label_names=[\"label\"],\n",
    "                                   protected_attribute_names=[\"protected\"],\n",
    "                                   favorable_label=0,\n",
    "                                   unfavorable_label=1)\n",
    "\n",
    "    # Test data\n",
    "    df_test = pd.DataFrame(np.hstack((X_test, y_test.reshape(-1, 1), prot_test.reshape(-1, 1))),\n",
    "                           columns=[f\"x{i}\" for i in range(X_test.shape[1])] + [\"label\", \"protected\"])\n",
    "    bld_test = BinaryLabelDataset(df=df_test,\n",
    "                                  label_names=[\"label\"],\n",
    "                                  protected_attribute_names=[\"protected\"],\n",
    "                                  favorable_label=0,\n",
    "                                  unfavorable_label=1)\n",
    "\n",
    "    # TensorFlow session\n",
    "    sess = tf.Session()\n",
    "\n",
    "    clf = AdversarialDebiasing(\n",
    "        privileged_groups=[{'protected': 1}],\n",
    "        unprivileged_groups=[{'protected': 0}],\n",
    "        scope_name='adv_debiasing',\n",
    "        sess=sess,\n",
    "        num_epochs=50,\n",
    "        batch_size=64,\n",
    "        debias=True\n",
    "    )\n",
    "\n",
    "    clf.fit(bld_train)\n",
    "    pred = clf.predict(bld_test)\n",
    "\n",
    "    return evaluate_fairness(y_test, pred.labels.ravel(), prot_test, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "21d5a66b-2a11-49c4-b20e-8314760fe345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\niyat\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\niyat\\anaconda3\\Lib\\site-packages\\aif360\\algorithms\\inprocessing\\adversarial_debiasing.py:164: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n",
      "epoch 0; iter: 0; batch classifier loss: 0.608301; batch adversarial loss: 1.066470\n",
      "epoch 0; iter: 200; batch classifier loss: 0.327045; batch adversarial loss: 0.928578\n",
      "epoch 0; iter: 400; batch classifier loss: 0.300993; batch adversarial loss: 0.659402\n",
      "epoch 0; iter: 600; batch classifier loss: 0.213995; batch adversarial loss: 0.452488\n",
      "epoch 0; iter: 800; batch classifier loss: 0.268944; batch adversarial loss: 0.334165\n",
      "epoch 0; iter: 1000; batch classifier loss: 0.155042; batch adversarial loss: 0.283616\n",
      "epoch 0; iter: 1200; batch classifier loss: 0.225082; batch adversarial loss: 0.200638\n",
      "epoch 0; iter: 1400; batch classifier loss: 0.135688; batch adversarial loss: 0.158124\n",
      "epoch 0; iter: 1600; batch classifier loss: 0.317485; batch adversarial loss: 0.163876\n",
      "epoch 1; iter: 0; batch classifier loss: 0.230486; batch adversarial loss: 0.169476\n",
      "epoch 1; iter: 200; batch classifier loss: 0.237164; batch adversarial loss: 0.112627\n",
      "epoch 1; iter: 400; batch classifier loss: 0.164370; batch adversarial loss: 0.133873\n",
      "epoch 1; iter: 600; batch classifier loss: 0.294992; batch adversarial loss: 0.125257\n",
      "epoch 1; iter: 800; batch classifier loss: 0.245691; batch adversarial loss: 0.126967\n",
      "epoch 1; iter: 1000; batch classifier loss: 0.133927; batch adversarial loss: 0.099757\n",
      "epoch 1; iter: 1200; batch classifier loss: 0.124201; batch adversarial loss: 0.052916\n",
      "epoch 1; iter: 1400; batch classifier loss: 0.200983; batch adversarial loss: 0.100989\n",
      "epoch 1; iter: 1600; batch classifier loss: 0.210823; batch adversarial loss: 0.099340\n",
      "epoch 2; iter: 0; batch classifier loss: 0.178960; batch adversarial loss: 0.080826\n",
      "epoch 2; iter: 200; batch classifier loss: 0.496354; batch adversarial loss: 0.078653\n",
      "epoch 2; iter: 400; batch classifier loss: 0.355357; batch adversarial loss: 0.146772\n",
      "epoch 2; iter: 600; batch classifier loss: 0.412940; batch adversarial loss: 0.034661\n",
      "epoch 2; iter: 800; batch classifier loss: 0.251998; batch adversarial loss: 0.031633\n",
      "epoch 2; iter: 1000; batch classifier loss: 0.137895; batch adversarial loss: 0.205135\n",
      "epoch 2; iter: 1200; batch classifier loss: 0.306830; batch adversarial loss: 0.027646\n",
      "epoch 2; iter: 1400; batch classifier loss: 0.255471; batch adversarial loss: 0.028739\n",
      "epoch 2; iter: 1600; batch classifier loss: 0.279906; batch adversarial loss: 0.173232\n",
      "epoch 3; iter: 0; batch classifier loss: 0.091342; batch adversarial loss: 0.083807\n",
      "epoch 3; iter: 200; batch classifier loss: 0.159372; batch adversarial loss: 0.088790\n",
      "epoch 3; iter: 400; batch classifier loss: 0.222495; batch adversarial loss: 0.085868\n",
      "epoch 3; iter: 600; batch classifier loss: 0.355411; batch adversarial loss: 0.083539\n",
      "epoch 3; iter: 800; batch classifier loss: 0.221734; batch adversarial loss: 0.135874\n",
      "epoch 3; iter: 1000; batch classifier loss: 0.187259; batch adversarial loss: 0.020683\n",
      "epoch 3; iter: 1200; batch classifier loss: 0.100686; batch adversarial loss: 0.121185\n",
      "epoch 3; iter: 1400; batch classifier loss: 0.228733; batch adversarial loss: 0.016871\n",
      "epoch 3; iter: 1600; batch classifier loss: 0.189914; batch adversarial loss: 0.017415\n",
      "epoch 4; iter: 0; batch classifier loss: 0.140489; batch adversarial loss: 0.062506\n",
      "epoch 4; iter: 200; batch classifier loss: 0.204717; batch adversarial loss: 0.014645\n",
      "epoch 4; iter: 400; batch classifier loss: 0.283056; batch adversarial loss: 0.014194\n",
      "epoch 4; iter: 600; batch classifier loss: 0.382961; batch adversarial loss: 0.084654\n",
      "epoch 4; iter: 800; batch classifier loss: 0.137713; batch adversarial loss: 0.150535\n",
      "epoch 4; iter: 1000; batch classifier loss: 0.115389; batch adversarial loss: 0.083508\n",
      "epoch 4; iter: 1200; batch classifier loss: 0.344951; batch adversarial loss: 0.142286\n",
      "epoch 4; iter: 1400; batch classifier loss: 0.273045; batch adversarial loss: 0.015917\n",
      "epoch 4; iter: 1600; batch classifier loss: 0.164160; batch adversarial loss: 0.083112\n",
      "epoch 5; iter: 0; batch classifier loss: 0.173274; batch adversarial loss: 0.136051\n",
      "epoch 5; iter: 200; batch classifier loss: 0.148434; batch adversarial loss: 0.015119\n",
      "epoch 5; iter: 400; batch classifier loss: 0.203448; batch adversarial loss: 0.082784\n",
      "epoch 5; iter: 600; batch classifier loss: 0.189526; batch adversarial loss: 0.085004\n",
      "epoch 5; iter: 800; batch classifier loss: 0.179870; batch adversarial loss: 0.016100\n",
      "epoch 5; iter: 1000; batch classifier loss: 0.200114; batch adversarial loss: 0.083500\n",
      "epoch 5; iter: 1200; batch classifier loss: 0.234802; batch adversarial loss: 0.016086\n",
      "epoch 5; iter: 1400; batch classifier loss: 0.275376; batch adversarial loss: 0.014622\n",
      "epoch 5; iter: 1600; batch classifier loss: 0.254698; batch adversarial loss: 0.218987\n",
      "epoch 6; iter: 0; batch classifier loss: 0.223673; batch adversarial loss: 0.218858\n",
      "epoch 6; iter: 200; batch classifier loss: 0.190662; batch adversarial loss: 0.082749\n",
      "epoch 6; iter: 400; batch classifier loss: 0.107795; batch adversarial loss: 0.083553\n",
      "epoch 6; iter: 600; batch classifier loss: 0.282642; batch adversarial loss: 0.083446\n",
      "epoch 6; iter: 800; batch classifier loss: 0.163533; batch adversarial loss: 0.014559\n",
      "epoch 6; iter: 1000; batch classifier loss: 0.249118; batch adversarial loss: 0.085161\n",
      "epoch 6; iter: 1200; batch classifier loss: 0.134638; batch adversarial loss: 0.013343\n",
      "epoch 6; iter: 1400; batch classifier loss: 0.200442; batch adversarial loss: 0.268288\n",
      "epoch 6; iter: 1600; batch classifier loss: 0.145705; batch adversarial loss: 0.083137\n",
      "epoch 7; iter: 0; batch classifier loss: 0.182401; batch adversarial loss: 0.083375\n",
      "epoch 7; iter: 200; batch classifier loss: 0.139008; batch adversarial loss: 0.014654\n",
      "epoch 7; iter: 400; batch classifier loss: 0.191273; batch adversarial loss: 0.013372\n",
      "epoch 7; iter: 600; batch classifier loss: 0.154517; batch adversarial loss: 0.151260\n",
      "epoch 7; iter: 800; batch classifier loss: 0.250953; batch adversarial loss: 0.085370\n",
      "epoch 7; iter: 1000; batch classifier loss: 0.204913; batch adversarial loss: 0.083189\n",
      "epoch 7; iter: 1200; batch classifier loss: 0.155570; batch adversarial loss: 0.084014\n",
      "epoch 7; iter: 1400; batch classifier loss: 0.223819; batch adversarial loss: 0.085120\n",
      "epoch 7; iter: 1600; batch classifier loss: 0.236686; batch adversarial loss: 0.080214\n",
      "epoch 8; iter: 0; batch classifier loss: 0.277443; batch adversarial loss: 0.018231\n",
      "epoch 8; iter: 200; batch classifier loss: 0.246734; batch adversarial loss: 0.078305\n",
      "epoch 8; iter: 400; batch classifier loss: 0.199274; batch adversarial loss: 0.081989\n",
      "epoch 8; iter: 600; batch classifier loss: 0.128108; batch adversarial loss: 0.084053\n",
      "epoch 8; iter: 800; batch classifier loss: 0.179545; batch adversarial loss: 0.084697\n",
      "epoch 8; iter: 1000; batch classifier loss: 0.141638; batch adversarial loss: 0.207072\n",
      "epoch 8; iter: 1200; batch classifier loss: 0.135432; batch adversarial loss: 0.014407\n",
      "epoch 8; iter: 1400; batch classifier loss: 0.257680; batch adversarial loss: 0.083958\n",
      "epoch 8; iter: 1600; batch classifier loss: 0.206472; batch adversarial loss: 0.015940\n",
      "epoch 9; iter: 0; batch classifier loss: 0.394908; batch adversarial loss: 0.083547\n",
      "epoch 9; iter: 200; batch classifier loss: 0.138116; batch adversarial loss: 0.015913\n",
      "epoch 9; iter: 400; batch classifier loss: 0.112307; batch adversarial loss: 0.016166\n",
      "epoch 9; iter: 600; batch classifier loss: 0.211483; batch adversarial loss: 0.210213\n",
      "epoch 9; iter: 800; batch classifier loss: 0.335318; batch adversarial loss: 0.146592\n",
      "epoch 9; iter: 1000; batch classifier loss: 0.201666; batch adversarial loss: 0.085034\n",
      "epoch 9; iter: 1200; batch classifier loss: 0.102903; batch adversarial loss: 0.014637\n",
      "epoch 9; iter: 1400; batch classifier loss: 0.239700; batch adversarial loss: 0.014932\n",
      "epoch 9; iter: 1600; batch classifier loss: 0.172229; batch adversarial loss: 0.071720\n",
      "epoch 10; iter: 0; batch classifier loss: 0.159072; batch adversarial loss: 0.082631\n",
      "epoch 10; iter: 200; batch classifier loss: 0.304643; batch adversarial loss: 0.082295\n",
      "epoch 10; iter: 400; batch classifier loss: 0.166085; batch adversarial loss: 0.128246\n",
      "epoch 10; iter: 600; batch classifier loss: 0.213255; batch adversarial loss: 0.015369\n",
      "epoch 10; iter: 800; batch classifier loss: 0.241505; batch adversarial loss: 0.015325\n",
      "epoch 10; iter: 1000; batch classifier loss: 0.118679; batch adversarial loss: 0.082430\n",
      "epoch 10; iter: 1200; batch classifier loss: 0.075094; batch adversarial loss: 0.013701\n",
      "epoch 10; iter: 1400; batch classifier loss: 0.130335; batch adversarial loss: 0.016040\n",
      "epoch 10; iter: 1600; batch classifier loss: 0.304485; batch adversarial loss: 0.084209\n",
      "epoch 11; iter: 0; batch classifier loss: 0.210423; batch adversarial loss: 0.150523\n",
      "epoch 11; iter: 200; batch classifier loss: 0.141146; batch adversarial loss: 0.013450\n",
      "epoch 11; iter: 400; batch classifier loss: 0.143617; batch adversarial loss: 0.014379\n",
      "epoch 11; iter: 600; batch classifier loss: 0.311951; batch adversarial loss: 0.144444\n",
      "epoch 11; iter: 800; batch classifier loss: 0.294734; batch adversarial loss: 0.080731\n",
      "epoch 11; iter: 1000; batch classifier loss: 0.137287; batch adversarial loss: 0.013829\n",
      "epoch 11; iter: 1200; batch classifier loss: 0.195275; batch adversarial loss: 0.016754\n",
      "epoch 11; iter: 1400; batch classifier loss: 0.219407; batch adversarial loss: 0.149426\n",
      "epoch 11; iter: 1600; batch classifier loss: 0.066351; batch adversarial loss: 0.013520\n",
      "epoch 12; iter: 0; batch classifier loss: 0.102243; batch adversarial loss: 0.149364\n",
      "epoch 12; iter: 200; batch classifier loss: 0.211678; batch adversarial loss: 0.085019\n",
      "epoch 12; iter: 400; batch classifier loss: 0.129825; batch adversarial loss: 0.014581\n",
      "epoch 12; iter: 600; batch classifier loss: 0.201735; batch adversarial loss: 0.082052\n",
      "epoch 12; iter: 800; batch classifier loss: 0.175941; batch adversarial loss: 0.084318\n",
      "epoch 12; iter: 1000; batch classifier loss: 0.154902; batch adversarial loss: 0.013525\n",
      "epoch 12; iter: 1200; batch classifier loss: 0.137551; batch adversarial loss: 0.014408\n",
      "epoch 12; iter: 1400; batch classifier loss: 0.173755; batch adversarial loss: 0.014915\n",
      "epoch 12; iter: 1600; batch classifier loss: 0.084680; batch adversarial loss: 0.150066\n",
      "epoch 13; iter: 0; batch classifier loss: 0.045854; batch adversarial loss: 0.082087\n",
      "epoch 13; iter: 200; batch classifier loss: 0.191884; batch adversarial loss: 0.150262\n",
      "epoch 13; iter: 400; batch classifier loss: 0.166475; batch adversarial loss: 0.014577\n",
      "epoch 13; iter: 600; batch classifier loss: 0.233577; batch adversarial loss: 0.083751\n",
      "epoch 13; iter: 800; batch classifier loss: 0.236310; batch adversarial loss: 0.083641\n",
      "epoch 13; iter: 1000; batch classifier loss: 0.177709; batch adversarial loss: 0.013700\n",
      "epoch 13; iter: 1200; batch classifier loss: 0.271445; batch adversarial loss: 0.016038\n",
      "epoch 13; iter: 1400; batch classifier loss: 0.170994; batch adversarial loss: 0.082430\n",
      "epoch 13; iter: 1600; batch classifier loss: 0.170859; batch adversarial loss: 0.015896\n",
      "epoch 14; iter: 0; batch classifier loss: 0.341799; batch adversarial loss: 0.146625\n",
      "epoch 14; iter: 200; batch classifier loss: 0.318178; batch adversarial loss: 0.014564\n",
      "epoch 14; iter: 400; batch classifier loss: 0.143331; batch adversarial loss: 0.015117\n",
      "epoch 14; iter: 600; batch classifier loss: 0.211740; batch adversarial loss: 0.147179\n",
      "epoch 14; iter: 800; batch classifier loss: 0.094773; batch adversarial loss: 0.016067\n",
      "epoch 14; iter: 1000; batch classifier loss: 0.176027; batch adversarial loss: 0.082016\n",
      "epoch 14; iter: 1200; batch classifier loss: 0.269626; batch adversarial loss: 0.016456\n",
      "epoch 14; iter: 1400; batch classifier loss: 0.284624; batch adversarial loss: 0.123597\n",
      "epoch 14; iter: 1600; batch classifier loss: 0.103953; batch adversarial loss: 0.150287\n",
      "epoch 15; iter: 0; batch classifier loss: 0.101132; batch adversarial loss: 0.014200\n",
      "epoch 15; iter: 200; batch classifier loss: 0.245787; batch adversarial loss: 0.151001\n",
      "epoch 15; iter: 400; batch classifier loss: 0.224945; batch adversarial loss: 0.083847\n",
      "epoch 15; iter: 600; batch classifier loss: 0.231334; batch adversarial loss: 0.014611\n",
      "epoch 15; iter: 800; batch classifier loss: 0.069026; batch adversarial loss: 0.014272\n",
      "epoch 15; iter: 1000; batch classifier loss: 0.154646; batch adversarial loss: 0.081989\n",
      "epoch 15; iter: 1200; batch classifier loss: 0.194918; batch adversarial loss: 0.015594\n",
      "epoch 15; iter: 1400; batch classifier loss: 0.148087; batch adversarial loss: 0.214946\n",
      "epoch 15; iter: 1600; batch classifier loss: 0.241274; batch adversarial loss: 0.014869\n",
      "epoch 16; iter: 0; batch classifier loss: 0.135392; batch adversarial loss: 0.082766\n",
      "epoch 16; iter: 200; batch classifier loss: 0.126121; batch adversarial loss: 0.082183\n",
      "epoch 16; iter: 400; batch classifier loss: 0.216884; batch adversarial loss: 0.016199\n",
      "epoch 16; iter: 600; batch classifier loss: 0.223633; batch adversarial loss: 0.016354\n",
      "epoch 16; iter: 800; batch classifier loss: 0.185074; batch adversarial loss: 0.149759\n",
      "epoch 16; iter: 1000; batch classifier loss: 0.057410; batch adversarial loss: 0.013996\n",
      "epoch 16; iter: 1200; batch classifier loss: 0.064223; batch adversarial loss: 0.081390\n",
      "epoch 16; iter: 1400; batch classifier loss: 0.102567; batch adversarial loss: 0.219307\n",
      "epoch 16; iter: 1600; batch classifier loss: 0.226943; batch adversarial loss: 0.016268\n",
      "epoch 17; iter: 0; batch classifier loss: 0.104434; batch adversarial loss: 0.082561\n",
      "epoch 17; iter: 200; batch classifier loss: 0.082347; batch adversarial loss: 0.014589\n",
      "epoch 17; iter: 400; batch classifier loss: 0.131812; batch adversarial loss: 0.084089\n",
      "epoch 17; iter: 600; batch classifier loss: 0.262619; batch adversarial loss: 0.013948\n",
      "epoch 17; iter: 800; batch classifier loss: 0.220218; batch adversarial loss: 0.152534\n",
      "epoch 17; iter: 1000; batch classifier loss: 0.161850; batch adversarial loss: 0.152193\n",
      "epoch 17; iter: 1200; batch classifier loss: 0.124739; batch adversarial loss: 0.014723\n",
      "epoch 17; iter: 1400; batch classifier loss: 0.160488; batch adversarial loss: 0.150116\n",
      "epoch 17; iter: 1600; batch classifier loss: 0.169271; batch adversarial loss: 0.015081\n",
      "epoch 18; iter: 0; batch classifier loss: 0.231807; batch adversarial loss: 0.082762\n",
      "epoch 18; iter: 200; batch classifier loss: 0.136508; batch adversarial loss: 0.150797\n",
      "epoch 18; iter: 400; batch classifier loss: 0.071988; batch adversarial loss: 0.149021\n",
      "epoch 18; iter: 600; batch classifier loss: 0.152237; batch adversarial loss: 0.081671\n",
      "epoch 18; iter: 800; batch classifier loss: 0.224998; batch adversarial loss: 0.014479\n",
      "epoch 18; iter: 1000; batch classifier loss: 0.129267; batch adversarial loss: 0.063961\n",
      "epoch 18; iter: 1200; batch classifier loss: 0.211751; batch adversarial loss: 0.145572\n",
      "epoch 18; iter: 1400; batch classifier loss: 0.120046; batch adversarial loss: 0.148838\n",
      "epoch 18; iter: 1600; batch classifier loss: 0.064078; batch adversarial loss: 0.081549\n",
      "epoch 19; iter: 0; batch classifier loss: 0.182675; batch adversarial loss: 0.014476\n",
      "epoch 19; iter: 200; batch classifier loss: 0.203059; batch adversarial loss: 0.148447\n",
      "epoch 19; iter: 400; batch classifier loss: 0.161842; batch adversarial loss: 0.081752\n",
      "epoch 19; iter: 600; batch classifier loss: 0.209393; batch adversarial loss: 0.015414\n",
      "epoch 19; iter: 800; batch classifier loss: 0.196963; batch adversarial loss: 0.082129\n",
      "epoch 19; iter: 1000; batch classifier loss: 0.052148; batch adversarial loss: 0.013898\n",
      "epoch 19; iter: 1200; batch classifier loss: 0.169969; batch adversarial loss: 0.014338\n",
      "epoch 19; iter: 1400; batch classifier loss: 0.204249; batch adversarial loss: 0.082620\n",
      "epoch 19; iter: 1600; batch classifier loss: 0.185279; batch adversarial loss: 0.194315\n",
      "epoch 20; iter: 0; batch classifier loss: 0.215988; batch adversarial loss: 0.124395\n",
      "epoch 20; iter: 200; batch classifier loss: 0.254792; batch adversarial loss: 0.150258\n",
      "epoch 20; iter: 400; batch classifier loss: 0.132300; batch adversarial loss: 0.082420\n",
      "epoch 20; iter: 600; batch classifier loss: 0.190582; batch adversarial loss: 0.149736\n",
      "epoch 20; iter: 800; batch classifier loss: 0.184031; batch adversarial loss: 0.013369\n",
      "epoch 20; iter: 1000; batch classifier loss: 0.127725; batch adversarial loss: 0.014449\n",
      "epoch 20; iter: 1200; batch classifier loss: 0.173992; batch adversarial loss: 0.016120\n",
      "epoch 20; iter: 1400; batch classifier loss: 0.201204; batch adversarial loss: 0.083048\n",
      "epoch 20; iter: 1600; batch classifier loss: 0.155562; batch adversarial loss: 0.081289\n",
      "epoch 21; iter: 0; batch classifier loss: 0.111701; batch adversarial loss: 0.014317\n",
      "epoch 21; iter: 200; batch classifier loss: 0.272480; batch adversarial loss: 0.216489\n",
      "epoch 21; iter: 400; batch classifier loss: 0.331155; batch adversarial loss: 0.078274\n",
      "epoch 21; iter: 600; batch classifier loss: 0.122126; batch adversarial loss: 0.082643\n",
      "epoch 21; iter: 800; batch classifier loss: 0.289067; batch adversarial loss: 0.016064\n",
      "epoch 21; iter: 1000; batch classifier loss: 0.199603; batch adversarial loss: 0.150656\n",
      "epoch 21; iter: 1200; batch classifier loss: 0.120558; batch adversarial loss: 0.145383\n",
      "epoch 21; iter: 1400; batch classifier loss: 0.184866; batch adversarial loss: 0.016631\n",
      "epoch 21; iter: 1600; batch classifier loss: 0.259509; batch adversarial loss: 0.149701\n",
      "epoch 22; iter: 0; batch classifier loss: 0.122751; batch adversarial loss: 0.014619\n",
      "epoch 22; iter: 200; batch classifier loss: 0.105352; batch adversarial loss: 0.149433\n",
      "epoch 22; iter: 400; batch classifier loss: 0.153601; batch adversarial loss: 0.081899\n",
      "epoch 22; iter: 600; batch classifier loss: 0.178126; batch adversarial loss: 0.081777\n",
      "epoch 22; iter: 800; batch classifier loss: 0.214975; batch adversarial loss: 0.081856\n",
      "epoch 22; iter: 1000; batch classifier loss: 0.199110; batch adversarial loss: 0.082436\n",
      "epoch 22; iter: 1200; batch classifier loss: 0.149949; batch adversarial loss: 0.285529\n",
      "epoch 22; iter: 1400; batch classifier loss: 0.277894; batch adversarial loss: 0.018579\n",
      "epoch 22; iter: 1600; batch classifier loss: 0.189876; batch adversarial loss: 0.132138\n",
      "epoch 23; iter: 0; batch classifier loss: 0.271626; batch adversarial loss: 0.151388\n",
      "epoch 23; iter: 200; batch classifier loss: 0.251168; batch adversarial loss: 0.150186\n",
      "epoch 23; iter: 400; batch classifier loss: 0.384198; batch adversarial loss: 0.151038\n",
      "epoch 23; iter: 600; batch classifier loss: 0.156056; batch adversarial loss: 0.082316\n",
      "epoch 23; iter: 800; batch classifier loss: 0.171611; batch adversarial loss: 0.151158\n",
      "epoch 23; iter: 1000; batch classifier loss: 0.116129; batch adversarial loss: 0.081247\n",
      "epoch 23; iter: 1200; batch classifier loss: 0.190224; batch adversarial loss: 0.150825\n",
      "epoch 23; iter: 1400; batch classifier loss: 0.150001; batch adversarial loss: 0.013883\n",
      "epoch 23; iter: 1600; batch classifier loss: 0.278627; batch adversarial loss: 0.083777\n",
      "epoch 24; iter: 0; batch classifier loss: 0.105984; batch adversarial loss: 0.013771\n",
      "epoch 24; iter: 200; batch classifier loss: 0.129539; batch adversarial loss: 0.014906\n",
      "epoch 24; iter: 400; batch classifier loss: 0.088179; batch adversarial loss: 0.014445\n",
      "epoch 24; iter: 600; batch classifier loss: 0.147040; batch adversarial loss: 0.014334\n",
      "epoch 24; iter: 800; batch classifier loss: 0.398845; batch adversarial loss: 0.014343\n",
      "epoch 24; iter: 1000; batch classifier loss: 0.363787; batch adversarial loss: 0.218166\n",
      "epoch 24; iter: 1200; batch classifier loss: 0.091168; batch adversarial loss: 0.081172\n",
      "epoch 24; iter: 1400; batch classifier loss: 0.273447; batch adversarial loss: 0.015689\n",
      "epoch 24; iter: 1600; batch classifier loss: 0.306526; batch adversarial loss: 0.016893\n",
      "epoch 25; iter: 0; batch classifier loss: 0.343111; batch adversarial loss: 0.151965\n",
      "epoch 25; iter: 200; batch classifier loss: 0.084560; batch adversarial loss: 0.217725\n",
      "epoch 25; iter: 400; batch classifier loss: 0.255188; batch adversarial loss: 0.082657\n",
      "epoch 25; iter: 600; batch classifier loss: 0.145544; batch adversarial loss: 0.081833\n",
      "epoch 25; iter: 800; batch classifier loss: 0.213059; batch adversarial loss: 0.263610\n",
      "epoch 25; iter: 1000; batch classifier loss: 0.155293; batch adversarial loss: 0.079872\n",
      "epoch 25; iter: 1200; batch classifier loss: 0.247751; batch adversarial loss: 0.014481\n",
      "epoch 25; iter: 1400; batch classifier loss: 0.109351; batch adversarial loss: 0.014483\n",
      "epoch 25; iter: 1600; batch classifier loss: 0.279274; batch adversarial loss: 0.015992\n",
      "epoch 26; iter: 0; batch classifier loss: 0.167700; batch adversarial loss: 0.081917\n",
      "epoch 26; iter: 200; batch classifier loss: 0.174711; batch adversarial loss: 0.147179\n",
      "epoch 26; iter: 400; batch classifier loss: 0.098158; batch adversarial loss: 0.149147\n",
      "epoch 26; iter: 600; batch classifier loss: 0.324298; batch adversarial loss: 0.015411\n",
      "epoch 26; iter: 800; batch classifier loss: 0.255028; batch adversarial loss: 0.014661\n",
      "epoch 26; iter: 1000; batch classifier loss: 0.278903; batch adversarial loss: 0.014556\n",
      "epoch 26; iter: 1200; batch classifier loss: 0.178447; batch adversarial loss: 0.014916\n",
      "epoch 26; iter: 1400; batch classifier loss: 0.254073; batch adversarial loss: 0.017164\n",
      "epoch 26; iter: 1600; batch classifier loss: 0.179471; batch adversarial loss: 0.016714\n",
      "epoch 27; iter: 0; batch classifier loss: 0.228731; batch adversarial loss: 0.014725\n",
      "epoch 27; iter: 200; batch classifier loss: 0.269586; batch adversarial loss: 0.014322\n",
      "epoch 27; iter: 400; batch classifier loss: 0.139084; batch adversarial loss: 0.066563\n",
      "epoch 27; iter: 600; batch classifier loss: 0.207802; batch adversarial loss: 0.082949\n",
      "epoch 27; iter: 800; batch classifier loss: 0.151558; batch adversarial loss: 0.081716\n",
      "epoch 27; iter: 1000; batch classifier loss: 0.135220; batch adversarial loss: 0.014566\n",
      "epoch 27; iter: 1200; batch classifier loss: 0.125379; batch adversarial loss: 0.015943\n",
      "epoch 27; iter: 1400; batch classifier loss: 0.103854; batch adversarial loss: 0.015143\n",
      "epoch 27; iter: 1600; batch classifier loss: 0.273773; batch adversarial loss: 0.014923\n",
      "epoch 28; iter: 0; batch classifier loss: 0.190652; batch adversarial loss: 0.082856\n",
      "epoch 28; iter: 200; batch classifier loss: 0.118163; batch adversarial loss: 0.082844\n",
      "epoch 28; iter: 400; batch classifier loss: 0.139619; batch adversarial loss: 0.015141\n",
      "epoch 28; iter: 600; batch classifier loss: 0.240971; batch adversarial loss: 0.071297\n",
      "epoch 28; iter: 800; batch classifier loss: 0.076321; batch adversarial loss: 0.013837\n",
      "epoch 28; iter: 1000; batch classifier loss: 0.268207; batch adversarial loss: 0.015536\n",
      "epoch 28; iter: 1200; batch classifier loss: 0.235120; batch adversarial loss: 0.082109\n",
      "epoch 28; iter: 1400; batch classifier loss: 0.206430; batch adversarial loss: 0.150073\n",
      "epoch 28; iter: 1600; batch classifier loss: 0.171855; batch adversarial loss: 0.014679\n",
      "epoch 29; iter: 0; batch classifier loss: 0.210187; batch adversarial loss: 0.016372\n",
      "epoch 29; iter: 200; batch classifier loss: 0.084097; batch adversarial loss: 0.149930\n",
      "epoch 29; iter: 400; batch classifier loss: 0.119982; batch adversarial loss: 0.149598\n",
      "epoch 29; iter: 600; batch classifier loss: 0.109781; batch adversarial loss: 0.082722\n",
      "epoch 29; iter: 800; batch classifier loss: 0.146255; batch adversarial loss: 0.081482\n",
      "epoch 29; iter: 1000; batch classifier loss: 0.281508; batch adversarial loss: 0.013922\n",
      "epoch 29; iter: 1200; batch classifier loss: 0.223645; batch adversarial loss: 0.214547\n",
      "epoch 29; iter: 1400; batch classifier loss: 0.234354; batch adversarial loss: 0.081814\n",
      "epoch 29; iter: 1600; batch classifier loss: 0.329788; batch adversarial loss: 0.131033\n",
      "epoch 30; iter: 0; batch classifier loss: 0.118104; batch adversarial loss: 0.074053\n",
      "epoch 30; iter: 200; batch classifier loss: 0.153949; batch adversarial loss: 0.014263\n",
      "epoch 30; iter: 400; batch classifier loss: 0.118853; batch adversarial loss: 0.081267\n",
      "epoch 30; iter: 600; batch classifier loss: 0.100410; batch adversarial loss: 0.081943\n",
      "epoch 30; iter: 800; batch classifier loss: 0.100644; batch adversarial loss: 0.013601\n",
      "epoch 30; iter: 1000; batch classifier loss: 0.197499; batch adversarial loss: 0.145071\n",
      "epoch 30; iter: 1200; batch classifier loss: 0.088578; batch adversarial loss: 0.081821\n",
      "epoch 30; iter: 1400; batch classifier loss: 0.203733; batch adversarial loss: 0.013483\n",
      "epoch 30; iter: 1600; batch classifier loss: 0.218508; batch adversarial loss: 0.062853\n",
      "epoch 31; iter: 0; batch classifier loss: 0.132430; batch adversarial loss: 0.083166\n",
      "epoch 31; iter: 200; batch classifier loss: 0.126397; batch adversarial loss: 0.015987\n",
      "epoch 31; iter: 400; batch classifier loss: 0.124587; batch adversarial loss: 0.142808\n",
      "epoch 31; iter: 600; batch classifier loss: 0.076199; batch adversarial loss: 0.148728\n",
      "epoch 31; iter: 800; batch classifier loss: 0.342802; batch adversarial loss: 0.014612\n",
      "epoch 31; iter: 1000; batch classifier loss: 0.260173; batch adversarial loss: 0.016208\n",
      "epoch 31; iter: 1200; batch classifier loss: 0.225724; batch adversarial loss: 0.082694\n",
      "epoch 31; iter: 1400; batch classifier loss: 0.170923; batch adversarial loss: 0.015197\n",
      "epoch 31; iter: 1600; batch classifier loss: 0.184529; batch adversarial loss: 0.079283\n",
      "epoch 32; iter: 0; batch classifier loss: 0.167083; batch adversarial loss: 0.015905\n",
      "epoch 32; iter: 200; batch classifier loss: 0.191440; batch adversarial loss: 0.014947\n",
      "epoch 32; iter: 400; batch classifier loss: 0.150066; batch adversarial loss: 0.217140\n",
      "epoch 32; iter: 600; batch classifier loss: 0.217582; batch adversarial loss: 0.066320\n",
      "epoch 32; iter: 800; batch classifier loss: 0.184092; batch adversarial loss: 0.260014\n",
      "epoch 32; iter: 1000; batch classifier loss: 0.154122; batch adversarial loss: 0.151490\n",
      "epoch 32; iter: 1200; batch classifier loss: 0.247356; batch adversarial loss: 0.014939\n",
      "epoch 32; iter: 1400; batch classifier loss: 0.175744; batch adversarial loss: 0.205751\n",
      "epoch 32; iter: 1600; batch classifier loss: 0.151814; batch adversarial loss: 0.014333\n",
      "epoch 33; iter: 0; batch classifier loss: 0.106385; batch adversarial loss: 0.013553\n",
      "epoch 33; iter: 200; batch classifier loss: 0.223043; batch adversarial loss: 0.082002\n",
      "epoch 33; iter: 400; batch classifier loss: 0.063745; batch adversarial loss: 0.014803\n",
      "epoch 33; iter: 600; batch classifier loss: 0.213117; batch adversarial loss: 0.149293\n",
      "epoch 33; iter: 800; batch classifier loss: 0.075624; batch adversarial loss: 0.083044\n",
      "epoch 33; iter: 1000; batch classifier loss: 0.161619; batch adversarial loss: 0.149817\n",
      "epoch 33; iter: 1200; batch classifier loss: 0.121548; batch adversarial loss: 0.150564\n",
      "epoch 33; iter: 1400; batch classifier loss: 0.247340; batch adversarial loss: 0.082844\n",
      "epoch 33; iter: 1600; batch classifier loss: 0.094121; batch adversarial loss: 0.150767\n",
      "epoch 34; iter: 0; batch classifier loss: 0.305525; batch adversarial loss: 0.150889\n",
      "epoch 34; iter: 200; batch classifier loss: 0.288582; batch adversarial loss: 0.082462\n",
      "epoch 34; iter: 400; batch classifier loss: 0.160577; batch adversarial loss: 0.013803\n",
      "epoch 34; iter: 600; batch classifier loss: 0.262765; batch adversarial loss: 0.151692\n",
      "epoch 34; iter: 800; batch classifier loss: 0.171808; batch adversarial loss: 0.082376\n",
      "epoch 34; iter: 1000; batch classifier loss: 0.253727; batch adversarial loss: 0.149460\n",
      "epoch 34; iter: 1200; batch classifier loss: 0.265284; batch adversarial loss: 0.060167\n",
      "epoch 34; iter: 1400; batch classifier loss: 0.259624; batch adversarial loss: 0.015763\n",
      "epoch 34; iter: 1600; batch classifier loss: 0.178018; batch adversarial loss: 0.214435\n",
      "epoch 35; iter: 0; batch classifier loss: 0.150176; batch adversarial loss: 0.015168\n",
      "epoch 35; iter: 200; batch classifier loss: 0.168323; batch adversarial loss: 0.016043\n",
      "epoch 35; iter: 400; batch classifier loss: 0.194672; batch adversarial loss: 0.147464\n",
      "epoch 35; iter: 600; batch classifier loss: 0.253097; batch adversarial loss: 0.014313\n",
      "epoch 35; iter: 800; batch classifier loss: 0.196419; batch adversarial loss: 0.150365\n",
      "epoch 35; iter: 1000; batch classifier loss: 0.185837; batch adversarial loss: 0.082583\n",
      "epoch 35; iter: 1200; batch classifier loss: 0.154740; batch adversarial loss: 0.015214\n",
      "epoch 35; iter: 1400; batch classifier loss: 0.170194; batch adversarial loss: 0.083430\n",
      "epoch 35; iter: 1600; batch classifier loss: 0.113084; batch adversarial loss: 0.149471\n",
      "epoch 36; iter: 0; batch classifier loss: 0.172156; batch adversarial loss: 0.149122\n",
      "epoch 36; iter: 200; batch classifier loss: 0.279631; batch adversarial loss: 0.014750\n",
      "epoch 36; iter: 400; batch classifier loss: 0.186991; batch adversarial loss: 0.082690\n",
      "epoch 36; iter: 600; batch classifier loss: 0.207228; batch adversarial loss: 0.082967\n",
      "epoch 36; iter: 800; batch classifier loss: 0.258817; batch adversarial loss: 0.015805\n",
      "epoch 36; iter: 1000; batch classifier loss: 0.230830; batch adversarial loss: 0.148305\n",
      "epoch 36; iter: 1200; batch classifier loss: 0.390928; batch adversarial loss: 0.082234\n",
      "epoch 36; iter: 1400; batch classifier loss: 0.341490; batch adversarial loss: 0.016650\n",
      "epoch 36; iter: 1600; batch classifier loss: 0.246833; batch adversarial loss: 0.081667\n",
      "epoch 37; iter: 0; batch classifier loss: 0.258395; batch adversarial loss: 0.081979\n",
      "epoch 37; iter: 200; batch classifier loss: 0.218251; batch adversarial loss: 0.014347\n",
      "epoch 37; iter: 400; batch classifier loss: 0.182841; batch adversarial loss: 0.015569\n",
      "epoch 37; iter: 600; batch classifier loss: 0.099403; batch adversarial loss: 0.081067\n",
      "epoch 37; iter: 800; batch classifier loss: 0.313041; batch adversarial loss: 0.082229\n",
      "epoch 37; iter: 1000; batch classifier loss: 0.218656; batch adversarial loss: 0.015064\n",
      "epoch 37; iter: 1200; batch classifier loss: 0.144698; batch adversarial loss: 0.076687\n",
      "epoch 37; iter: 1400; batch classifier loss: 0.198051; batch adversarial loss: 0.082563\n",
      "epoch 37; iter: 1600; batch classifier loss: 0.216198; batch adversarial loss: 0.014205\n",
      "epoch 38; iter: 0; batch classifier loss: 0.315606; batch adversarial loss: 0.084412\n",
      "epoch 38; iter: 200; batch classifier loss: 0.105928; batch adversarial loss: 0.138784\n",
      "epoch 38; iter: 400; batch classifier loss: 0.114516; batch adversarial loss: 0.073153\n",
      "epoch 38; iter: 600; batch classifier loss: 0.230422; batch adversarial loss: 0.081799\n",
      "epoch 38; iter: 800; batch classifier loss: 0.202006; batch adversarial loss: 0.148838\n",
      "epoch 38; iter: 1000; batch classifier loss: 0.088191; batch adversarial loss: 0.015496\n",
      "epoch 38; iter: 1200; batch classifier loss: 0.198745; batch adversarial loss: 0.014008\n",
      "epoch 38; iter: 1400; batch classifier loss: 0.085274; batch adversarial loss: 0.150003\n",
      "epoch 38; iter: 1600; batch classifier loss: 0.199014; batch adversarial loss: 0.083156\n",
      "epoch 39; iter: 0; batch classifier loss: 0.178533; batch adversarial loss: 0.014132\n",
      "epoch 39; iter: 200; batch classifier loss: 0.174534; batch adversarial loss: 0.071517\n",
      "epoch 39; iter: 400; batch classifier loss: 0.176265; batch adversarial loss: 0.082566\n",
      "epoch 39; iter: 600; batch classifier loss: 0.275660; batch adversarial loss: 0.082712\n",
      "epoch 39; iter: 800; batch classifier loss: 0.231460; batch adversarial loss: 0.081614\n",
      "epoch 39; iter: 1000; batch classifier loss: 0.250639; batch adversarial loss: 0.082857\n",
      "epoch 39; iter: 1200; batch classifier loss: 0.281041; batch adversarial loss: 0.062519\n",
      "epoch 39; iter: 1400; batch classifier loss: 0.211523; batch adversarial loss: 0.083322\n",
      "epoch 39; iter: 1600; batch classifier loss: 0.228436; batch adversarial loss: 0.014857\n",
      "epoch 40; iter: 0; batch classifier loss: 0.226332; batch adversarial loss: 0.150362\n",
      "epoch 40; iter: 200; batch classifier loss: 0.195846; batch adversarial loss: 0.014045\n",
      "epoch 40; iter: 400; batch classifier loss: 0.098138; batch adversarial loss: 0.013682\n",
      "epoch 40; iter: 600; batch classifier loss: 0.250594; batch adversarial loss: 0.082886\n",
      "epoch 40; iter: 800; batch classifier loss: 0.229557; batch adversarial loss: 0.146545\n",
      "epoch 40; iter: 1000; batch classifier loss: 0.216955; batch adversarial loss: 0.149575\n",
      "epoch 40; iter: 1200; batch classifier loss: 0.233951; batch adversarial loss: 0.082808\n",
      "epoch 40; iter: 1400; batch classifier loss: 0.196617; batch adversarial loss: 0.082841\n",
      "epoch 40; iter: 1600; batch classifier loss: 0.198070; batch adversarial loss: 0.081883\n",
      "epoch 41; iter: 0; batch classifier loss: 0.187002; batch adversarial loss: 0.014413\n",
      "epoch 41; iter: 200; batch classifier loss: 0.212596; batch adversarial loss: 0.145002\n",
      "epoch 41; iter: 400; batch classifier loss: 0.094763; batch adversarial loss: 0.081390\n",
      "epoch 41; iter: 600; batch classifier loss: 0.105772; batch adversarial loss: 0.015548\n",
      "epoch 41; iter: 800; batch classifier loss: 0.163894; batch adversarial loss: 0.014372\n",
      "epoch 41; iter: 1000; batch classifier loss: 0.214695; batch adversarial loss: 0.150684\n",
      "epoch 41; iter: 1200; batch classifier loss: 0.306626; batch adversarial loss: 0.149086\n",
      "epoch 41; iter: 1400; batch classifier loss: 0.203513; batch adversarial loss: 0.081802\n",
      "epoch 41; iter: 1600; batch classifier loss: 0.130403; batch adversarial loss: 0.081424\n",
      "epoch 42; iter: 0; batch classifier loss: 0.206488; batch adversarial loss: 0.083099\n",
      "epoch 42; iter: 200; batch classifier loss: 0.182815; batch adversarial loss: 0.149083\n",
      "epoch 42; iter: 400; batch classifier loss: 0.091942; batch adversarial loss: 0.081902\n",
      "epoch 42; iter: 600; batch classifier loss: 0.139149; batch adversarial loss: 0.152168\n",
      "epoch 42; iter: 800; batch classifier loss: 0.200983; batch adversarial loss: 0.145739\n",
      "epoch 42; iter: 1000; batch classifier loss: 0.250400; batch adversarial loss: 0.081951\n",
      "epoch 42; iter: 1200; batch classifier loss: 0.359476; batch adversarial loss: 0.081976\n",
      "epoch 42; iter: 1400; batch classifier loss: 0.209688; batch adversarial loss: 0.150728\n",
      "epoch 42; iter: 1600; batch classifier loss: 0.218730; batch adversarial loss: 0.082372\n",
      "epoch 43; iter: 0; batch classifier loss: 0.153941; batch adversarial loss: 0.013691\n",
      "epoch 43; iter: 200; batch classifier loss: 0.174716; batch adversarial loss: 0.083114\n",
      "epoch 43; iter: 400; batch classifier loss: 0.246909; batch adversarial loss: 0.204077\n",
      "epoch 43; iter: 600; batch classifier loss: 0.374491; batch adversarial loss: 0.014160\n",
      "epoch 43; iter: 800; batch classifier loss: 0.115811; batch adversarial loss: 0.217406\n",
      "epoch 43; iter: 1000; batch classifier loss: 0.064529; batch adversarial loss: 0.014820\n",
      "epoch 43; iter: 1200; batch classifier loss: 0.185560; batch adversarial loss: 0.218628\n",
      "epoch 43; iter: 1400; batch classifier loss: 0.113031; batch adversarial loss: 0.081934\n",
      "epoch 43; iter: 1600; batch classifier loss: 0.226498; batch adversarial loss: 0.146864\n",
      "epoch 44; iter: 0; batch classifier loss: 0.180637; batch adversarial loss: 0.081339\n",
      "epoch 44; iter: 200; batch classifier loss: 0.211311; batch adversarial loss: 0.218241\n",
      "epoch 44; iter: 400; batch classifier loss: 0.082534; batch adversarial loss: 0.014639\n",
      "epoch 44; iter: 600; batch classifier loss: 0.249959; batch adversarial loss: 0.013989\n",
      "epoch 44; iter: 800; batch classifier loss: 0.117605; batch adversarial loss: 0.081663\n",
      "epoch 44; iter: 1000; batch classifier loss: 0.191645; batch adversarial loss: 0.150689\n",
      "epoch 44; iter: 1200; batch classifier loss: 0.151372; batch adversarial loss: 0.081691\n",
      "epoch 44; iter: 1400; batch classifier loss: 0.110651; batch adversarial loss: 0.081651\n",
      "epoch 44; iter: 1600; batch classifier loss: 0.191321; batch adversarial loss: 0.016074\n",
      "epoch 45; iter: 0; batch classifier loss: 0.164327; batch adversarial loss: 0.078564\n",
      "epoch 45; iter: 200; batch classifier loss: 0.146169; batch adversarial loss: 0.082735\n",
      "epoch 45; iter: 400; batch classifier loss: 0.169335; batch adversarial loss: 0.015797\n",
      "epoch 45; iter: 600; batch classifier loss: 0.282968; batch adversarial loss: 0.216608\n",
      "epoch 45; iter: 800; batch classifier loss: 0.263063; batch adversarial loss: 0.082581\n",
      "epoch 45; iter: 1000; batch classifier loss: 0.328995; batch adversarial loss: 0.083377\n",
      "epoch 45; iter: 1200; batch classifier loss: 0.391322; batch adversarial loss: 0.215441\n",
      "epoch 45; iter: 1400; batch classifier loss: 0.334728; batch adversarial loss: 0.082349\n",
      "epoch 45; iter: 1600; batch classifier loss: 0.229614; batch adversarial loss: 0.082464\n",
      "epoch 46; iter: 0; batch classifier loss: 0.168949; batch adversarial loss: 0.083246\n",
      "epoch 46; iter: 200; batch classifier loss: 0.374099; batch adversarial loss: 0.082578\n",
      "epoch 46; iter: 400; batch classifier loss: 0.118367; batch adversarial loss: 0.149829\n",
      "epoch 46; iter: 600; batch classifier loss: 0.109116; batch adversarial loss: 0.082935\n",
      "epoch 46; iter: 800; batch classifier loss: 0.080318; batch adversarial loss: 0.014287\n",
      "epoch 46; iter: 1000; batch classifier loss: 0.255978; batch adversarial loss: 0.016259\n",
      "epoch 46; iter: 1200; batch classifier loss: 0.075548; batch adversarial loss: 0.081770\n",
      "epoch 46; iter: 1400; batch classifier loss: 0.134381; batch adversarial loss: 0.014076\n",
      "epoch 46; iter: 1600; batch classifier loss: 0.331596; batch adversarial loss: 0.082545\n",
      "epoch 47; iter: 0; batch classifier loss: 0.281728; batch adversarial loss: 0.013971\n",
      "epoch 47; iter: 200; batch classifier loss: 0.226797; batch adversarial loss: 0.014713\n",
      "epoch 47; iter: 400; batch classifier loss: 0.268203; batch adversarial loss: 0.014493\n",
      "epoch 47; iter: 600; batch classifier loss: 0.227479; batch adversarial loss: 0.014090\n",
      "epoch 47; iter: 800; batch classifier loss: 0.157326; batch adversarial loss: 0.081359\n",
      "epoch 47; iter: 1000; batch classifier loss: 0.192365; batch adversarial loss: 0.014837\n",
      "epoch 47; iter: 1200; batch classifier loss: 0.211720; batch adversarial loss: 0.014286\n",
      "epoch 47; iter: 1400; batch classifier loss: 0.081129; batch adversarial loss: 0.081078\n",
      "epoch 47; iter: 1600; batch classifier loss: 0.070446; batch adversarial loss: 0.149305\n",
      "epoch 48; iter: 0; batch classifier loss: 0.137282; batch adversarial loss: 0.014089\n",
      "epoch 48; iter: 200; batch classifier loss: 0.132665; batch adversarial loss: 0.066111\n",
      "epoch 48; iter: 400; batch classifier loss: 0.435821; batch adversarial loss: 0.015564\n",
      "epoch 48; iter: 600; batch classifier loss: 0.167857; batch adversarial loss: 0.082958\n",
      "epoch 48; iter: 800; batch classifier loss: 0.122600; batch adversarial loss: 0.081748\n",
      "epoch 48; iter: 1000; batch classifier loss: 0.218240; batch adversarial loss: 0.060991\n",
      "epoch 48; iter: 1200; batch classifier loss: 0.159253; batch adversarial loss: 0.081907\n",
      "epoch 48; iter: 1400; batch classifier loss: 0.069498; batch adversarial loss: 0.060181\n",
      "epoch 48; iter: 1600; batch classifier loss: 0.255898; batch adversarial loss: 0.081895\n",
      "epoch 49; iter: 0; batch classifier loss: 0.399946; batch adversarial loss: 0.079963\n",
      "epoch 49; iter: 200; batch classifier loss: 0.222652; batch adversarial loss: 0.015572\n",
      "epoch 49; iter: 400; batch classifier loss: 0.212690; batch adversarial loss: 0.214677\n",
      "epoch 49; iter: 600; batch classifier loss: 0.251249; batch adversarial loss: 0.066710\n",
      "epoch 49; iter: 800; batch classifier loss: 0.201080; batch adversarial loss: 0.081029\n",
      "epoch 49; iter: 1000; batch classifier loss: 0.208196; batch adversarial loss: 0.084179\n",
      "epoch 49; iter: 1200; batch classifier loss: 0.110285; batch adversarial loss: 0.081719\n",
      "epoch 49; iter: 1400; batch classifier loss: 0.210888; batch adversarial loss: 0.142459\n",
      "epoch 49; iter: 1600; batch classifier loss: 0.372074; batch adversarial loss: 0.085077\n"
     ]
    }
   ],
   "source": [
    "results['AdversarialDebiasing'] = train_adversarial_debiasing(X_train, y_train, prot_train, X_test, y_test, prot_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "dfe95a93-6a75-4c08-827e-abb7f40d70d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fairness Evaluation Across Compatible AIF360 In-processing Algorithms\n",
      "\n",
      "🔹 GerryFair\n",
      "  accuracy: 0.9332\n",
      "  disparate_impact: 0.9575\n",
      "  statistical_parity_difference: -0.0424\n",
      "  equal_opportunity_difference: -0.0313\n",
      "\n",
      "🔹 PrejudiceRemover\n",
      "  accuracy: 0.0659\n",
      "  disparate_impact: 0.3925\n",
      "  statistical_parity_difference: -0.0027\n",
      "  equal_opportunity_difference: -0.0019\n",
      "\n",
      "🔹 ExponentiatedGradient\n",
      "  accuracy: 0.9340\n",
      "  disparate_impact: 0.9861\n",
      "  statistical_parity_difference: -0.0139\n",
      "  equal_opportunity_difference: -0.0105\n",
      "\n",
      "🔹 GridSearch\n",
      "  accuracy: 0.9338\n",
      "  disparate_impact: 1.0035\n",
      "  statistical_parity_difference: 0.0034\n",
      "  equal_opportunity_difference: 0.0015\n",
      "\n",
      "🔹 AdversarialDebiasing\n",
      "  accuracy: 0.9358\n",
      "  disparate_impact: 0.9561\n",
      "  statistical_parity_difference: -0.0428\n",
      "  equal_opportunity_difference: -0.0296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Fairness Evaluation Across Compatible AIF360 In-processing Algorithms\\n\")\n",
    "for model, metrics in results.items():\n",
    "    print(f\"🔹 {model}\")\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"  {metric_name}: {value:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "744e0fad-36c1-4f97-ab4d-f236f426a79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "2dc8c3c7-4a8b-4a24-97f1-24395817ee58",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, prot_train, prot_test = load_gmsc_inprocess()\n",
    "post_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "c6803dfb-2800-413d-a8c8-cdba6ec9f0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_roc_postprocessing_with_xgb(X_train, y_train, prot_train, X_test, y_test, prot_test):\n",
    "    model = train_baseline_model(X_train, y_train)\n",
    "\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    bld_test = BinaryLabelDataset(\n",
    "        favorable_label=0,\n",
    "        unfavorable_label=1,\n",
    "        df=pd.DataFrame(np.hstack((X_test, y_test[:, None], prot_test[:, None])),\n",
    "                        columns=[f'x{i}' for i in range(X_test.shape[1])] + ['label', 'protected']),\n",
    "        label_names=['label'],\n",
    "        protected_attribute_names=['protected']\n",
    "    )\n",
    "\n",
    "    bld_pred = bld_test.copy()\n",
    "    bld_pred.scores = y_prob.reshape(-1, 1)\n",
    "    bld_pred.labels = y_pred.reshape(-1, 1)\n",
    "\n",
    "    roc = RejectOptionClassification(\n",
    "        unprivileged_groups=[{'protected': 0}],\n",
    "        privileged_groups=[{'protected': 1}],\n",
    "        low_class_thresh=0.3, high_class_thresh=0.7,\n",
    "        num_class_thresh=100, num_ROC_margin=50,\n",
    "        metric_name=\"Statistical parity difference\",\n",
    "        metric_ub=0.05, metric_lb=-0.05\n",
    "    )\n",
    "    roc = roc.fit(bld_test, bld_pred)\n",
    "    pred = roc.predict(bld_pred)\n",
    "\n",
    "    return evaluate_fairness(y_test, pred.labels.ravel(), prot_test, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "2168c0aa-76fc-4f6c-b372-08a5ef4fa88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niyat\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [05:04:28] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "post_results['RejectOptionClassification'] = train_roc_postprocessing_with_xgb(X_train, y_train, prot_train, X_test, y_test, prot_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "4eadcdcb-e48d-4193-9aa4-d24ce7188caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_calibrated_eq_odds_with_xgb(X_train, y_train, prot_train, X_test, y_test, prot_test):\n",
    "    model = train_baseline_model(X_train, y_train)\n",
    "\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    bld_test = BinaryLabelDataset(\n",
    "        favorable_label=0,\n",
    "        unfavorable_label=1,\n",
    "        df=pd.DataFrame(np.hstack((X_test, y_test[:, None], prot_test[:, None])),\n",
    "                        columns=[f'x{i}' for i in range(X_test.shape[1])] + ['label', 'protected']),\n",
    "        label_names=['label'],\n",
    "        protected_attribute_names=['protected']\n",
    "    )\n",
    "\n",
    "    bld_pred = bld_test.copy()\n",
    "    bld_pred.scores = y_prob.reshape(-1, 1)\n",
    "\n",
    "    ceo = CalibratedEqOddsPostprocessing(\n",
    "        privileged_groups=[{'protected': 1}],\n",
    "        unprivileged_groups=[{'protected': 0}],\n",
    "        cost_constraint=\"fnr\",\n",
    "        seed=42\n",
    "    )\n",
    "    ceo = ceo.fit(bld_test, bld_pred)\n",
    "    pred = ceo.predict(bld_pred)\n",
    "\n",
    "    return evaluate_fairness(y_test, pred.labels.ravel(), prot_test, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "81017dda-febe-495f-8915-89f7128033fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niyat\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [04:55:31] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "post_results['CalibratedEqOdds'] = train_calibrated_eq_odds_with_xgb(X_train, y_train, prot_train, X_test, y_test, prot_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "9a827471-94d8-4b11-a63a-2fc7c2f707b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_equalized_odds_with_xgb(X_train, y_train, prot_train, X_test, y_test, prot_test):\n",
    "    model = train_baseline_model(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    bld_test = BinaryLabelDataset(\n",
    "        favorable_label=0,\n",
    "        unfavorable_label=1,\n",
    "        df=pd.DataFrame(np.hstack((X_test, y_test[:, None], prot_test[:, None])),\n",
    "                        columns=[f'x{i}' for i in range(X_test.shape[1])] + ['label', 'protected']),\n",
    "        label_names=['label'],\n",
    "        protected_attribute_names=['protected']\n",
    "    )\n",
    "\n",
    "    bld_pred = bld_test.copy()\n",
    "    bld_pred.labels = y_pred.reshape(-1, 1)\n",
    "\n",
    "    eq = EqOddsPostprocessing(\n",
    "        privileged_groups=[{'protected': 1}],\n",
    "        unprivileged_groups=[{'protected': 0}]\n",
    "    )\n",
    "    eq = eq.fit(bld_test, bld_pred)\n",
    "    pred = eq.predict(bld_pred)\n",
    "\n",
    "    return evaluate_fairness(y_test, pred.labels.ravel(), prot_test, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "b2e0ecea-dbc8-41a9-a4e9-2b700de1f1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niyat\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [04:56:52] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "post_results['EqualizedOdds'] = train_equalized_odds_with_xgb(X_train, y_train, prot_train, X_test, y_test, prot_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "1e8776c5-6a02-4ab7-a425-34b2abafc1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 RejectOptionClassification\n",
      "  accuracy: 0.0666\n",
      "  disparate_impact: 219.8148\n",
      "  statistical_parity_difference: 0.0492\n",
      "  equal_opportunity_difference: 0.0261\n",
      "\n",
      "🔹 CalibratedEqOdds\n",
      "  accuracy: 0.0651\n",
      "  disparate_impact: 1.7005\n",
      "  statistical_parity_difference: 0.0175\n",
      "  equal_opportunity_difference: 0.0117\n",
      "\n",
      "🔹 EqualizedOdds\n",
      "  accuracy: 0.9224\n",
      "  disparate_impact: 0.9915\n",
      "  statistical_parity_difference: -0.0082\n",
      "  equal_opportunity_difference: 0.0001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model, metrics in post_results.items():\n",
    "    print(f\"🔹 {model}\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
