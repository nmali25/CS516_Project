{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install aif360['all']"
      ],
      "metadata": {
        "id": "_msJ5E_gklwd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "902800a5-6324-442a-97c2-50b1fdc490b9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting aif360[all]\n",
            "  Downloading aif360-0.6.1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.11/dist-packages (from aif360[all]) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from aif360[all]) (1.14.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from aif360[all]) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.11/dist-packages (from aif360[all]) (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from aif360[all]) (3.10.0)\n",
            "Collecting skorch (from aif360[all])\n",
            "  Downloading skorch-1.1.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting jupyter (from aif360[all])\n",
            "  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting sphinx-rtd-theme (from aif360[all])\n",
            "  Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting igraph[plotting] (from aif360[all])\n",
            "  Downloading igraph-0.11.8-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting lime (from aif360[all])\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from aif360[all]) (4.67.1)\n",
            "Collecting fairlearn~=0.7 (from aif360[all])\n",
            "  Downloading fairlearn-0.12.0-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting colorama (from aif360[all])\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: tensorflow>=1.13.1 in /usr/local/lib/python3.11/dist-packages (from aif360[all]) (2.18.0)\n",
            "Requirement already satisfied: sphinx in /usr/local/lib/python3.11/dist-packages (from aif360[all]) (8.2.3)\n",
            "Requirement already satisfied: mlxtend in /usr/local/lib/python3.11/dist-packages (from aif360[all]) (0.23.4)\n",
            "Collecting inFairness>=0.2.2 (from aif360[all])\n",
            "  Downloading inFairness-0.2.3-py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting pytest-cov>=2.8.1 (from aif360[all])\n",
            "  Downloading pytest_cov-6.1.1-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting ipympl (from aif360[all])\n",
            "  Downloading ipympl-0.9.7-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from aif360[all]) (2.6.0+cu124)\n",
            "Requirement already satisfied: jinja2>3.1.0 in /usr/local/lib/python3.11/dist-packages (from aif360[all]) (3.1.6)\n",
            "Collecting adversarial-robustness-toolbox>=1.0.0 (from aif360[all])\n",
            "  Downloading adversarial_robustness_toolbox-1.19.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting BlackBoxAuditing (from aif360[all])\n",
            "  Downloading BlackBoxAuditing-0.1.54.tar.gz (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from aif360[all]) (0.13.2)\n",
            "Requirement already satisfied: rpy2 in /usr/local/lib/python3.11/dist-packages (from aif360[all]) (3.5.17)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (from aif360[all]) (4.5.0)\n",
            "Requirement already satisfied: pytest>=3.5 in /usr/local/lib/python3.11/dist-packages (from aif360[all]) (8.3.5)\n",
            "Collecting pot (from aif360[all])\n",
            "  Downloading POT-0.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n",
            "Requirement already satisfied: cvxpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from aif360[all]) (1.6.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from adversarial-robustness-toolbox>=1.0.0->aif360[all]) (1.17.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from adversarial-robustness-toolbox>=1.0.0->aif360[all]) (75.2.0)\n",
            "Requirement already satisfied: osqp>=0.6.2 in /usr/local/lib/python3.11/dist-packages (from cvxpy>=1.0->aif360[all]) (1.0.3)\n",
            "Requirement already satisfied: clarabel>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from cvxpy>=1.0->aif360[all]) (0.10.0)\n",
            "Requirement already satisfied: scs>=3.2.4.post1 in /usr/local/lib/python3.11/dist-packages (from cvxpy>=1.0->aif360[all]) (3.2.7.post2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>3.1.0->aif360[all]) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.0->aif360[all]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.0->aif360[all]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.0->aif360[all]) (2025.2)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest>=3.5->aif360[all]) (2.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pytest>=3.5->aif360[all]) (24.2)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest>=3.5->aif360[all]) (1.5.0)\n",
            "Collecting coverage>=7.5 (from coverage[toml]>=7.5->pytest-cov>=2.8.1->aif360[all])\n",
            "  Downloading coverage-7.8.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0->aif360[all]) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0->aif360[all]) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.13.1->aif360[all]) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.13.1->aif360[all]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.13.1->aif360[all]) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.13.1->aif360[all]) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.13.1->aif360[all]) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.13.1->aif360[all]) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.13.1->aif360[all]) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.13.1->aif360[all]) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.13.1->aif360[all]) (2.32.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.13.1->aif360[all]) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.13.1->aif360[all]) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.13.1->aif360[all]) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.13.1->aif360[all]) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.13.1->aif360[all]) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.13.1->aif360[all]) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.13.1->aif360[all]) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.13.1->aif360[all]) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.13.1->aif360[all]) (0.37.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->aif360[all]) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->aif360[all]) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->aif360[all]) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->aif360[all])\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->aif360[all])\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->aif360[all])\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->aif360[all])\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->aif360[all])\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->aif360[all])\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->aif360[all])\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->aif360[all])\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->aif360[all])\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->aif360[all]) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->aif360[all]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->aif360[all]) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->aif360[all])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->aif360[all]) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->aif360[all]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->aif360[all]) (1.3.0)\n",
            "Collecting texttable>=1.6.2 (from igraph[plotting]; extra == \"all\"->aif360[all])\n",
            "  Downloading texttable-1.7.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting cairocffi>=1.2.0 (from igraph[plotting]; extra == \"all\"->aif360[all])\n",
            "  Downloading cairocffi-1.7.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: ipython<10 in /usr/local/lib/python3.11/dist-packages (from ipympl->aif360[all]) (7.34.0)\n",
            "Requirement already satisfied: ipywidgets<9,>=7.6.0 in /usr/local/lib/python3.11/dist-packages (from ipympl->aif360[all]) (7.7.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from ipympl->aif360[all]) (11.1.0)\n",
            "Requirement already satisfied: traitlets<6 in /usr/local/lib/python3.11/dist-packages (from ipympl->aif360[all]) (5.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->aif360[all]) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->aif360[all]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->aif360[all]) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->aif360[all]) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->aif360[all]) (3.2.3)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.11/dist-packages (from jupyter->aif360[all]) (6.5.7)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.11/dist-packages (from jupyter->aif360[all]) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.11/dist-packages (from jupyter->aif360[all]) (7.16.6)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.11/dist-packages (from jupyter->aif360[all]) (6.17.1)\n",
            "Collecting jupyterlab (from jupyter->aif360[all])\n",
            "  Downloading jupyterlab-4.4.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.11/dist-packages (from lime->aif360[all]) (0.25.2)\n",
            "Requirement already satisfied: cffi>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from rpy2->aif360[all]) (1.17.1)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.11/dist-packages (from rpy2->aif360[all]) (5.3.1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.11/dist-packages (from skorch->aif360[all]) (0.9.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from sphinx->aif360[all]) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx->aif360[all]) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx->aif360[all]) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from sphinx->aif360[all]) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx->aif360[all]) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.11/dist-packages (from sphinx->aif360[all]) (2.0.0)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.11/dist-packages (from sphinx->aif360[all]) (2.18.0)\n",
            "Requirement already satisfied: docutils<0.22,>=0.20 in /usr/local/lib/python3.11/dist-packages (from sphinx->aif360[all]) (0.21.2)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.11/dist-packages (from sphinx->aif360[all]) (2.2.0)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.11/dist-packages (from sphinx->aif360[all]) (2.17.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.11/dist-packages (from sphinx->aif360[all]) (1.0.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.11/dist-packages (from sphinx->aif360[all]) (1.4.1)\n",
            "Requirement already satisfied: roman-numerals-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from sphinx->aif360[all]) (3.1.0)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->aif360[all])\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow>=1.13.1->aif360[all]) (0.45.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.15.1->rpy2->aif360[all]) (2.22)\n",
            "Collecting jedi>=0.16 (from ipython<10->ipympl->aif360[all])\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython<10->ipympl->aif360[all]) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython<10->ipympl->aif360[all]) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython<10->ipympl->aif360[all]) (3.0.51)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython<10->ipympl->aif360[all]) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython<10->ipympl->aif360[all]) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython<10->ipympl->aif360[all]) (4.9.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets<9,>=7.6.0->ipympl->aif360[all]) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets<9,>=7.6.0->ipympl->aif360[all]) (3.6.10)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets<9,>=7.6.0->ipympl->aif360[all]) (3.0.14)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->aif360[all]) (1.8.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->aif360[all]) (6.1.12)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->aif360[all]) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->aif360[all]) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->aif360[all]) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->aif360[all]) (6.4.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow>=1.13.1->aif360[all]) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow>=1.13.1->aif360[all]) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow>=1.13.1->aif360[all]) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow>=1.13.1->aif360[all]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow>=1.13.1->aif360[all]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow>=1.13.1->aif360[all]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow>=1.13.1->aif360[all]) (2025.1.31)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime->aif360[all]) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime->aif360[all]) (2025.3.30)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime->aif360[all]) (0.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=1.13.1->aif360[all]) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=1.13.1->aif360[all]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=1.13.1->aif360[all]) (3.1.3)\n",
            "Collecting async-lru>=1.0.0 (from jupyterlab->jupyter->aif360[all])\n",
            "  Downloading async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: httpx>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->aif360[all]) (0.28.1)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->aif360[all]) (5.7.2)\n",
            "Collecting jupyter-lsp>=2.0.0 (from jupyterlab->jupyter->aif360[all])\n",
            "  Downloading jupyter_lsp-2.2.5-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting jupyter-server<3,>=2.4.0 (from jupyterlab->jupyter->aif360[all])\n",
            "  Downloading jupyter_server-2.15.0-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting jupyterlab-server<3,>=2.27.1 (from jupyterlab->jupyter->aif360[all])\n",
            "  Downloading jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: notebook-shim>=0.2 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->aif360[all]) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->aif360[all]) (4.13.4)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->aif360[all]) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->aif360[all]) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->aif360[all]) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->aif360[all]) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->aif360[all]) (0.10.2)\n",
            "Requirement already satisfied: nbformat>=5.7 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->aif360[all]) (5.10.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->aif360[all]) (1.5.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->aif360[all]) (23.1.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->aif360[all]) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->aif360[all]) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->aif360[all]) (0.21.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->aif360[all]) (1.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter->aif360[all]) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->aif360[all]) (1.4.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter->aif360[all]) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter->aif360[all]) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab->jupyter->aif360[all]) (0.14.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython<10->ipympl->aif360[all]) (0.8.4)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core->jupyterlab->jupyter->aif360[all]) (4.3.7)\n",
            "Collecting jupyter-client>=6.1.12 (from ipykernel->jupyter->aif360[all])\n",
            "  Downloading jupyter_client-7.4.9-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting jupyter-events>=0.11.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->aif360[all])\n",
            "  Downloading jupyter_events-0.12.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->aif360[all])\n",
            "  Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->aif360[all])\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->aif360[all]) (1.8.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook->jupyter->aif360[all]) (21.2.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel->jupyter->aif360[all]) (0.4)\n",
            "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->aif360[all])\n",
            "  Downloading json5-0.12.0-py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->aif360[all]) (4.23.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7->nbconvert->jupyter->aif360[all]) (2.21.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython<10->ipympl->aif360[all]) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<10->ipympl->aif360[all]) (0.2.13)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert->jupyter->aif360[all]) (2.7)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow>=1.13.1->aif360[all]) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.0->jupyterlab->jupyter->aif360[all]) (1.3.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->aif360[all]) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->aif360[all]) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->aif360[all]) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->aif360[all]) (0.24.0)\n",
            "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->aif360[all])\n",
            "  Downloading python_json_logger-3.3.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.11/dist-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->aif360[all]) (6.0.2)\n",
            "Collecting rfc3339-validator (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->aif360[all])\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->aif360[all])\n",
            "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow>=1.13.1->aif360[all]) (0.1.2)\n",
            "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->aif360[all])\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->aif360[all])\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->aif360[all]) (3.0.0)\n",
            "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->aif360[all])\n",
            "  Downloading uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->aif360[all]) (24.11.1)\n",
            "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->aif360[all])\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->aif360[all])\n",
            "  Downloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl.metadata (2.1 kB)\n",
            "Downloading adversarial_robustness_toolbox-1.19.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fairlearn-0.12.0-py3-none-any.whl (240 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.0/240.0 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading inFairness-0.2.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading POT-0.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_cov-6.1.1-py3-none-any.whl (23 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m123.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aif360-0.6.1-py3-none-any.whl (259 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.7/259.7 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading ipympl-0.9.7-py3-none-any.whl (515 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.7/515.7 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
            "Downloading skorch-1.1.0-py3-none-any.whl (228 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.9/228.9 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m123.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cairocffi-1.7.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coverage-7.8.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.0/244.0 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading igraph-0.11.8-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m111.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab-4.4.1-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m125.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_server-2.15.0-py3-none-any.whl (385 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.8/385.8 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_client-7.4.9-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.5/133.5 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json5-0.12.0-py3-none-any.whl (36 kB)\n",
            "Downloading jupyter_events-0.12.0-py3-none-any.whl (19 kB)\n",
            "Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading python_json_logger-3.3.0-py3-none-any.whl (15 kB)\n",
            "Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
            "Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl (14 kB)\n",
            "Building wheels for collected packages: BlackBoxAuditing, lime\n",
            "  Building wheel for BlackBoxAuditing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for BlackBoxAuditing: filename=BlackBoxAuditing-0.1.54-py2.py3-none-any.whl size=1394756 sha256=f32bccfdc0c3a4e21b64eed25fda2d1aaf9ebd8bd82cf977f2d9f3a71daba3bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/8c/03/073e80e604151fb4cdc68b2e56a97f338d7723e4a4ab5e3823\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283834 sha256=a3ec5fe096d90910e966b7b55916aba6501c2b485e78b7dad0dc60859e2a4e76\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/fa/a3/9c2d44c9f3cd77cf4e533b58900b2bf4487f2a17e8ec212a3d\n",
            "Successfully built BlackBoxAuditing lime\n",
            "Installing collected packages: texttable, uri-template, types-python-dateutil, rfc3986-validator, rfc3339-validator, python-json-logger, overrides, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, json5, jedi, igraph, fqdn, coverage, colorama, async-lru, pot, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jupyter-server-terminals, jupyter-client, cairocffi, arrow, sphinxcontrib-jquery, skorch, pytest-cov, nvidia-cusolver-cu12, lime, isoduration, fairlearn, BlackBoxAuditing, aif360, adversarial-robustness-toolbox, sphinx-rtd-theme, jupyter-events, inFairness, jupyter-server, jupyterlab-server, jupyter-lsp, jupyterlab, jupyter, ipympl\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: jupyter-client\n",
            "    Found existing installation: jupyter-client 6.1.12\n",
            "    Uninstalling jupyter-client-6.1.12:\n",
            "      Successfully uninstalled jupyter-client-6.1.12\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: jupyter-server\n",
            "    Found existing installation: jupyter-server 1.16.0\n",
            "    Uninstalling jupyter-server-1.16.0:\n",
            "      Successfully uninstalled jupyter-server-1.16.0\n",
            "Successfully installed BlackBoxAuditing-0.1.54 adversarial-robustness-toolbox-1.19.1 aif360-0.6.1 arrow-1.3.0 async-lru-2.0.5 cairocffi-1.7.1 colorama-0.4.6 coverage-7.8.0 fairlearn-0.12.0 fqdn-1.5.1 igraph-0.11.8 inFairness-0.2.3 ipympl-0.9.7 isoduration-20.11.0 jedi-0.19.2 json5-0.12.0 jupyter-1.1.1 jupyter-client-7.4.9 jupyter-events-0.12.0 jupyter-lsp-2.2.5 jupyter-server-2.15.0 jupyter-server-terminals-0.5.3 jupyterlab-4.4.1 jupyterlab-server-2.27.3 lime-0.2.0.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 overrides-7.7.0 pot-0.9.5 pytest-cov-6.1.1 python-json-logger-3.3.0 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 skorch-1.1.0 sphinx-rtd-theme-3.0.2 sphinxcontrib-jquery-4.1 texttable-1.7.0 types-python-dateutil-2.9.0.20241206 uri-template-1.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fairlearn"
      ],
      "metadata": {
        "id": "gjj8CEZFHwnT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "be29b000-b63e-4c8c-8575-99b97f8058f3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fairlearn in /usr/local/lib/python3.11/dist-packages (0.12.0)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from fairlearn) (2.0.2)\n",
            "Requirement already satisfied: pandas>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from fairlearn) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.2.1 in /usr/local/lib/python3.11/dist-packages (from fairlearn) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.9.3 in /usr/local/lib/python3.11/dist-packages (from fairlearn) (1.14.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.3->fairlearn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.3->fairlearn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.3->fairlearn) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2.1->fairlearn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2.1->fairlearn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.3->fairlearn) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Core Python Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Scikit-Learn: Preprocessing, Modeling, Evaluation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# XGBoost\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# AIF360: Datasets and Metrics\n",
        "from aif360.datasets import BinaryLabelDataset\n",
        "from aif360.metrics import ClassificationMetric\n",
        "\n",
        "# AIF360: Preprocessing Algorithms\n",
        "from aif360.algorithms.preprocessing import (\n",
        "    Reweighing,\n",
        "    DisparateImpactRemover,\n",
        "    LFR\n",
        ")\n",
        "\n",
        "# AIF360: In-processing Algorithms\n",
        "from aif360.algorithms.inprocessing import (\n",
        "    MetaFairClassifier,\n",
        "    GerryFairClassifier,\n",
        "    PrejudiceRemover,\n",
        "    ExponentiatedGradientReduction,\n",
        "    GridSearchReduction,\n",
        "    ARTClassifier,\n",
        "    AdversarialDebiasing\n",
        ")\n",
        "\n",
        "#AIF360: Post-processing Algorithms\n",
        "from aif360.algorithms.postprocessing import (\n",
        "    RejectOptionClassification,\n",
        "    CalibratedEqOddsPostprocessing,\n",
        "    EqOddsPostprocessing\n",
        ")\n",
        "\n",
        "# TensorFlow for AdversarialDebiasing\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_eager_execution()\n"
      ],
      "metadata": {
        "id": "T-7mVQc2UKuZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7c46c42-394b-46f1-8699-974d577b3370"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/inFairness/utils/ndcg.py:37: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
            "  vect_normalized_discounted_cumulative_gain = vmap(\n",
            "/usr/local/lib/python3.11/dist-packages/inFairness/utils/ndcg.py:48: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
            "  monte_carlo_vect_ndcg = vmap(vect_normalized_discounted_cumulative_gain, in_dims=(0,))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data loading"
      ],
      "metadata": {
        "id": "MRMGxO9EIJva"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_path='/content/AER_credit_card_data.csv'"
      ],
      "metadata": {
        "id": "Lir4CNX7yMJi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_creditcard_csv(csv_path):\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "    # Load the credit card dataset\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Target column: binary 1/0 (1 = accepted, 0 = rejected)\n",
        "    df['target'] = df['card'].map({'yes': 1, 'no': 0}) if df['card'].dtype == object else df['card']\n",
        "    df.drop(columns=['card'], inplace=True)\n",
        "\n",
        "    # Binary protected attribute: age >= 25 → 1 (privileged), else 0\n",
        "    df['age_binary'] = df['age'].apply(lambda x: 1 if x >= 25 else 0)\n",
        "\n",
        "    # One-hot encode categorical columns\n",
        "    df_encoded = pd.get_dummies(df.drop(columns=['target', 'age_binary', 'age']), drop_first=True)\n",
        "\n",
        "    # Feature matrix and labels\n",
        "    X = df_encoded\n",
        "    y = df['target']\n",
        "    protected = df['age_binary']\n",
        "\n",
        "    # Standardize numeric features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test, prot_train, prot_test = train_test_split(\n",
        "        X_scaled, y, protected, test_size=0.3, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, prot_train, prot_test\n"
      ],
      "metadata": {
        "id": "3KjeXIoWUsqX"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_credit_card_data(csv_path):\n",
        "    import pandas as pd\n",
        "\n",
        "    # Load dataset\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Binary target column: 1 = approved, 0 = not approved\n",
        "    df['target'] = df['card'].map({'yes': 1, 'no': 0}) if df['card'].dtype == object else df['card']\n",
        "    df.drop(columns=['card'], inplace=True)\n",
        "\n",
        "    # Protected attribute: age >= 25 is privileged\n",
        "    df['age_binary'] = df['age'].apply(lambda x: 1 if x >= 25 else 0)\n",
        "\n",
        "    # Raw features (exclude target and protected columns)\n",
        "    X_raw = df.drop(columns=['target', 'age', 'age_binary'])\n",
        "    y = df['target'].values\n",
        "    prot = df['age_binary'].values\n",
        "\n",
        "    return X_raw, y, prot\n",
        "def preprocess_data(X_raw, y, prot):\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "    import numpy as np\n",
        "\n",
        "    # Categorical and numeric columns\n",
        "    cat_cols = X_raw.select_dtypes(include='object').columns.tolist()\n",
        "    num_cols = X_raw.select_dtypes(exclude='object').columns.tolist()\n",
        "\n",
        "    # Train-test split\n",
        "    X_raw_train, X_raw_test, y_train, y_test, prot_train, prot_test = train_test_split(\n",
        "        X_raw, y, prot, test_size=0.3, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    # Encode categorical\n",
        "    encoder = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
        "    X_train_cat = encoder.fit_transform(X_raw_train[cat_cols])\n",
        "    X_test_cat = encoder.transform(X_raw_test[cat_cols])\n",
        "\n",
        "    # Extract numeric\n",
        "    X_train_num = X_raw_train[num_cols].values\n",
        "    X_test_num = X_raw_test[num_cols].values\n",
        "\n",
        "    # Combine encoded + numeric\n",
        "    X_train = np.hstack((X_train_num, X_train_cat))\n",
        "    X_test = np.hstack((X_test_num, X_test_cat))\n",
        "\n",
        "    # Normalize\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, prot_train, prot_test\n"
      ],
      "metadata": {
        "id": "wYKZa7A-FVqd"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_creditcard_data_inprocess(csv_path):\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "    # Load dataset\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Convert 'card' to binary target\n",
        "    df['target'] = df['card'].map({'yes': 1, 'no': 0}) if df['card'].dtype == object else df['card']\n",
        "    df.drop(columns=['card'], inplace=True)\n",
        "\n",
        "    # Protected attribute: age >= 25 is privileged\n",
        "    df['age_binary'] = df['age'].apply(lambda x: 1 if x >= 25 else 0)\n",
        "\n",
        "    # Features (drop target, age, and binary protected attribute)\n",
        "    X_raw = df.drop(columns=['target', 'age', 'age_binary'])\n",
        "    y = df['target'].values\n",
        "    prot = df['age_binary'].values\n",
        "\n",
        "    # Train-test split\n",
        "    X_train_raw, X_test_raw, y_train, y_test, prot_train, prot_test = train_test_split(\n",
        "        X_raw, y, prot, test_size=0.3, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    # Identify categorical vs numerical columns\n",
        "    categorical_cols = X_raw.select_dtypes(include='object').columns.tolist()\n",
        "    numeric_cols = X_raw.select_dtypes(exclude='object').columns.tolist()\n",
        "\n",
        "    # One-hot encode categorical features\n",
        "    encoder = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
        "    X_train_cat = encoder.fit_transform(X_train_raw[categorical_cols])\n",
        "    X_test_cat = encoder.transform(X_test_raw[categorical_cols])\n",
        "\n",
        "    # Numeric features\n",
        "    X_train_num = X_train_raw[numeric_cols].values\n",
        "    X_test_num = X_test_raw[numeric_cols].values\n",
        "\n",
        "    # Combine encoded + numeric\n",
        "    X_train = np.hstack((X_train_num, X_train_cat))\n",
        "    X_test = np.hstack((X_test_num, X_test_cat))\n",
        "\n",
        "    # Standardize all features\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, prot_train, prot_test\n"
      ],
      "metadata": {
        "id": "GI7gpzv1IEuR"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Utility Functions"
      ],
      "metadata": {
        "id": "_ZsVhApdIFx3"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_baseline_model(X_train, y_train, sample_weight=None):\n",
        "    model = XGBClassifier(eval_metric='logloss', use_label_encoder=False)\n",
        "    model.fit(X_train, y_train, sample_weight=sample_weight)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "KdUC91D_F3Ig"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_fairness(y_true, y_pred, prot, X=None):\n",
        "    df = pd.DataFrame(np.hstack((X, y_true[:, None], prot[:, None])),\n",
        "                      columns=[f\"x{i}\" for i in range(X.shape[1])] + ['label', 'protected'])\n",
        "\n",
        "    dataset_true = BinaryLabelDataset(df=df,\n",
        "                                      label_names=[\"label\"],\n",
        "                                      protected_attribute_names=[\"protected\"],\n",
        "                                      favorable_label=1, unfavorable_label=0)\n",
        "\n",
        "    pred_dataset = dataset_true.copy()\n",
        "    pred_dataset.labels = y_pred.reshape(-1, 1)\n",
        "\n",
        "    metric = ClassificationMetric(dataset_true, pred_dataset,\n",
        "                                  privileged_groups=[{'protected': 1}],\n",
        "                                  unprivileged_groups=[{'protected': 0}])\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy_score(y_true, y_pred),\n",
        "        'disparate_impact': metric.disparate_impact(),\n",
        "        'statistical_parity_difference': metric.statistical_parity_difference(),\n",
        "        'equal_opportunity_difference': metric.equal_opportunity_difference()\n",
        "    }\n"
      ],
      "metadata": {
        "id": "UmXUc4HcF-mp"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X_test, y_test, prot_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    return evaluate_fairness(y_test, y_pred, prot_test, X_test)\n"
      ],
      "metadata": {
        "id": "h7DJ4vxXGAF6"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EwrdQgx2ea0l"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#XGboost\n",
        "X_train, X_test, y_train, y_test, prot_train, prot_test = load_and_preprocess_creditcard_csv(csv_path)\n",
        "baseline_model = train_baseline_model(X_train, y_train)\n",
        "baseline_metrics = evaluate_model(baseline_model, X_test, y_test.to_numpy(), prot_test.to_numpy())\n",
        "\n",
        "baseline_metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHXnaTnhVTlk",
        "outputId": "fb6fe409-1120-4364-8531-9150b261f30c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:41:15] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.9671717171717171,\n",
              " 'disparate_impact': np.float64(0.9952838427947598),\n",
              " 'statistical_parity_difference': np.float64(-0.003648648648648667),\n",
              " 'equal_opportunity_difference': np.float64(-0.03678414096916305)}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inprocess_results = {}"
      ],
      "metadata": {
        "id": "3LXrBhDvGhSa"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##disparate impact"
      ],
      "metadata": {
        "id": "0iFApi5gedu4"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_disparate_impact_remover(X_train, y_train, prot_train, repair_level=1.0):\n",
        "    df = pd.DataFrame(X_train)\n",
        "    df['target'] = y_train.values\n",
        "    df['protected'] = prot_train.values\n",
        "\n",
        "    dataset = BinaryLabelDataset(\n",
        "        favorable_label=1,\n",
        "        unfavorable_label=0,\n",
        "        df=df,\n",
        "        label_names=['target'],\n",
        "        protected_attribute_names=['protected']\n",
        "    )\n",
        "\n",
        "    dir_remover = DisparateImpactRemover(repair_level=repair_level)\n",
        "    repaired_dataset = dir_remover.fit_transform(dataset)\n",
        "\n",
        "    X_repaired = pd.DataFrame(repaired_dataset.features)\n",
        "    y_repaired = pd.Series(repaired_dataset.labels.ravel())\n",
        "    prot_repaired = pd.Series(repaired_dataset.protected_attributes.ravel())\n",
        "\n",
        "    return X_repaired, y_repaired, prot_repaired\n"
      ],
      "metadata": {
        "id": "SS_fT8YeVWIH"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Disparate Impact Remover to both train and test sets\n",
        "X_train_repaired, y_train_repaired, prot_train_repaired = apply_disparate_impact_remover(\n",
        "    pd.DataFrame(X_train), y_train, prot_train\n",
        ")\n",
        "\n",
        "X_test_repaired, y_test_repaired, prot_test_repaired = apply_disparate_impact_remover(\n",
        "    pd.DataFrame(X_test), y_test, prot_test\n",
        ")\n",
        "\n",
        "# Train on repaired training data\n",
        "fair_model = train_baseline_model(X_train_repaired, y_train_repaired)\n",
        "\n",
        "# Evaluate on repaired test data\n",
        "fair_metrics = evaluate_model(\n",
        "    fair_model,\n",
        "    X_test_repaired.to_numpy(),\n",
        "    y_test_repaired.to_numpy(),\n",
        "    prot_test_repaired.to_numpy()\n",
        ")\n",
        "inprocess_results['Disparate_Impact_Remover']=fair_metrics\n",
        "fair_metrics\n"
      ],
      "metadata": {
        "id": "DZTmaL0dXLss",
        "outputId": "daecfb60-dff0-4f5d-d74d-6c7d658a0d8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:41:16] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.9696969696969697,\n",
              " 'disparate_impact': np.float64(1.0261333333333333),\n",
              " 'statistical_parity_difference': np.float64(0.01986486486486494),\n",
              " 'equal_opportunity_difference': np.float64(-0.015473568281938355)}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##LFR"
      ],
      "metadata": {
        "id": "IM4Gsh0teXy-"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_lfr(X_train, y_train, prot_train):\n",
        "    df = pd.DataFrame(X_train)\n",
        "    df['target'] = y_train.values\n",
        "    df['protected'] = prot_train.values\n",
        "\n",
        "    dataset = BinaryLabelDataset(\n",
        "        favorable_label=1,\n",
        "        unfavorable_label=0,\n",
        "        df=df,\n",
        "        label_names=['target'],\n",
        "        protected_attribute_names=['protected']\n",
        "    )\n",
        "\n",
        "    lfr = LFR(unprivileged_groups=[{'protected': 0}],\n",
        "          privileged_groups=[{'protected': 1}],\n",
        "          k=10, Ax=0.01, Ay=1.0, Az=0.1, verbose=0)\n",
        "\n",
        "\n",
        "    lfr.fit(dataset)\n",
        "    transformed_dataset = lfr.transform(dataset)\n",
        "\n",
        "    X_transformed = pd.DataFrame(transformed_dataset.features)\n",
        "    y_transformed = pd.Series(transformed_dataset.labels.ravel())\n",
        "    prot_transformed = pd.Series(transformed_dataset.protected_attributes.ravel())\n",
        "\n",
        "    return X_transformed, y_transformed, prot_transformed, lfr\n"
      ],
      "metadata": {
        "id": "liLwxciLejD-"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_lfr, y_train_lfr, prot_train_lfr, lfr_model = apply_lfr(pd.DataFrame(X_train), y_train, prot_train)\n"
      ],
      "metadata": {
        "id": "Dke6DdwRekky"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Unique labels in y_train_lfr:\", np.unique(y_train_lfr))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmmBlvSRfCjX",
        "outputId": "79df93c2-5c86-42a6-816a-e4340881d247"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique labels in y_train_lfr: [0. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fair_lfr_model = train_baseline_model(X_train_lfr, y_train_lfr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wy6VtAcQemhQ",
        "outputId": "cda75640-9b4d-498a-cda4-b518efcf9380"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:41:20] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply LFR transformation on test set\n",
        "df_test = pd.DataFrame(X_test)\n",
        "df_test['target'] = y_test.values\n",
        "df_test['protected'] = prot_test.values\n",
        "\n",
        "test_dataset = BinaryLabelDataset(\n",
        "    favorable_label=1,\n",
        "    unfavorable_label=0,\n",
        "    df=df_test,\n",
        "    label_names=['target'],\n",
        "    protected_attribute_names=['protected']\n",
        ")\n",
        "\n",
        "transformed_test_dataset = lfr_model.transform(test_dataset)\n",
        "\n",
        "X_test_lfr = pd.DataFrame(transformed_test_dataset.features)\n",
        "y_test_lfr = pd.Series(transformed_test_dataset.labels.ravel())\n",
        "prot_test_lfr = pd.Series(transformed_test_dataset.protected_attributes.ravel())\n",
        "\n",
        "# Evaluate\n",
        "fair_lfr_metrics = evaluate_model(\n",
        "    fair_lfr_model,\n",
        "    X_test_lfr.to_numpy(),\n",
        "    y_test_lfr.to_numpy(),\n",
        "    prot_test_lfr.to_numpy()\n",
        ")\n",
        "inprocess_results['LFR']=fair_lfr_metrics\n",
        "\n",
        "fair_lfr_metrics\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhEjgCm3evMi",
        "outputId": "e1639153-efec-42ec-f0d2-800a47d681ef"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.9974747474747475,\n",
              " 'disparate_impact': np.float64(0.9270943396226414),\n",
              " 'statistical_parity_difference': np.float64(-0.06527027027027033),\n",
              " 'equal_opportunity_difference': np.float64(0.003759398496240629)}"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#reweighing"
      ],
      "metadata": {
        "id": "Maxq0HlAHAEv"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_reweighing(X_train, y_train, prot_train):\n",
        "    df_train = pd.DataFrame(np.hstack((X_train, y_train.reshape(-1, 1), prot_train.reshape(-1, 1))),\n",
        "                            columns=[f\"x{i}\" for i in range(X_train.shape[1])] + [\"label\", \"protected\"])\n",
        "    bld_train = BinaryLabelDataset(\n",
        "        favorable_label=1,\n",
        "        unfavorable_label=0,\n",
        "        df=df_train,\n",
        "        label_names=[\"label\"],\n",
        "        protected_attribute_names=[\"protected\"]\n",
        "    )\n",
        "    RW = Reweighing(unprivileged_groups=[{'protected': 0}], privileged_groups=[{'protected': 1}])\n",
        "    bld_rw = RW.fit_transform(bld_train)\n",
        "\n",
        "    return bld_rw.features, bld_rw.labels.ravel(), bld_rw.instance_weights\n"
      ],
      "metadata": {
        "id": "7CKRSQlnCnT5"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test, prot_train, prot_test = preprocess_data(*load_credit_card_data(csv_path))\n",
        "\n",
        "X_rw, y_rw, sample_weights = apply_reweighing(X_train, y_train, prot_train)\n",
        "\n",
        "expected_num_features = X_rw.shape[1]\n",
        "\n",
        "if X_test.shape[1] < expected_num_features:\n",
        "    padding = expected_num_features - X_test.shape[1]\n",
        "    X_test_aligned = np.hstack((X_test, np.zeros((X_test.shape[0], padding))))\n",
        "elif X_test.shape[1] > expected_num_features:\n",
        "    X_test_aligned = X_test[:, :expected_num_features]\n",
        "else:\n",
        "    X_test_aligned = X_test\n",
        "\n",
        "model_rw = train_baseline_model(X_rw, y_rw, sample_weight=sample_weights)\n",
        "\n",
        "reweighing_metric = evaluate_model(model_rw, X_test_aligned, y_test, prot_test)\n",
        "\n",
        "inprocess_results['reweighing'] = reweighing_metric\n",
        "reweighing_metric\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GygXVrOmDCHX",
        "outputId": "63429200-26f3-4e53-d8c7-2b1c931b3ab9"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:43:26] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.9696969696969697,\n",
              " 'disparate_impact': np.float64(0.98235807860262),\n",
              " 'statistical_parity_difference': np.float64(-0.013648648648648676),\n",
              " 'equal_opportunity_difference': np.float64(-0.03678414096916305)}"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for model, metrics in inprocess_results.items():\n",
        "    print(f\"🔹 {model}\")\n",
        "    for k, v in metrics.items():\n",
        "        print(f\"  {k}: {v:.4f}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMCZUNH3HsQG",
        "outputId": "3ee5dabf-550d-4399-d318-27dd4b523ff3"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Disparate_Impact_Remover\n",
            "  accuracy: 0.9697\n",
            "  disparate_impact: 1.0261\n",
            "  statistical_parity_difference: 0.0199\n",
            "  equal_opportunity_difference: -0.0155\n",
            "\n",
            "🔹 LFR\n",
            "  accuracy: 0.9975\n",
            "  disparate_impact: 0.9271\n",
            "  statistical_parity_difference: -0.0653\n",
            "  equal_opportunity_difference: 0.0038\n",
            "\n",
            "🔹 reweighing\n",
            "  accuracy: 0.9697\n",
            "  disparate_impact: 0.9824\n",
            "  statistical_parity_difference: -0.0136\n",
            "  equal_opportunity_difference: -0.0368\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Inprocess"
      ],
      "metadata": {
        "id": "ckjyZeplrqHh"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test, prot_train, prot_test = load_creditcard_data_inprocess(csv_path)\n",
        "results = {}"
      ],
      "metadata": {
        "id": "OvO0R5sPI5Ui"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gerryfair(X_train, y_train, prot_train, X_test, y_test, prot_test):\n",
        "    df_train = pd.DataFrame(np.hstack((X_train, y_train[:, None], prot_train[:, None])),\n",
        "                            columns=[f\"x{i}\" for i in range(X_train.shape[1])] + [\"label\", \"protected\"])\n",
        "    bld_train = BinaryLabelDataset(df=df_train, label_names=[\"label\"], protected_attribute_names=[\"protected\"],\n",
        "                                   favorable_label=1, unfavorable_label=0)\n",
        "\n",
        "    df_test = pd.DataFrame(np.hstack((X_test, y_test[:, None], prot_test[:, None])),\n",
        "                           columns=[f\"x{i}\" for i in range(X_test.shape[1])] + [\"label\", \"protected\"])\n",
        "    bld_test = BinaryLabelDataset(df=df_test, label_names=[\"label\"], protected_attribute_names=[\"protected\"],\n",
        "                                  favorable_label=1, unfavorable_label=0)\n",
        "\n",
        "    clf = GerryFairClassifier(C=100, printflag=False, gamma=0.005, fairness_def='FP', max_iters=50)\n",
        "    clf.fit(bld_train)\n",
        "    pred = clf.predict(bld_test)\n",
        "\n",
        "    return evaluate_fairness(y_test, pred.labels.ravel(), prot_test, X_test)\n"
      ],
      "metadata": {
        "id": "TUllqmP6xbgT"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results['GerryFair'] = train_gerryfair(X_train, y_train, prot_train, X_test, y_test, prot_test)\n"
      ],
      "metadata": {
        "id": "zhCuFVZBI6j0"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_prejudice_remover(X_train, y_train, prot_train, X_test, y_test, prot_test):\n",
        "    df_train = pd.DataFrame(np.hstack((X_train, y_train.reshape(-1, 1), prot_train.reshape(-1, 1))),\n",
        "                            columns=[f\"x{i}\" for i in range(X_train.shape[1])] + [\"label\", \"protected\"])\n",
        "    bld_train = BinaryLabelDataset(df=df_train,\n",
        "                                   label_names=[\"label\"],\n",
        "                                   protected_attribute_names=[\"protected\"],\n",
        "                                   favorable_label=1,\n",
        "                                   unfavorable_label=0)\n",
        "\n",
        "    df_test = pd.DataFrame(np.hstack((X_test, y_test.reshape(-1, 1), prot_test.reshape(-1, 1))),\n",
        "                           columns=[f\"x{i}\" for i in range(X_test.shape[1])] + [\"label\", \"protected\"])\n",
        "    bld_test = BinaryLabelDataset(df=df_test,\n",
        "                                  label_names=[\"label\"],\n",
        "                                  protected_attribute_names=[\"protected\"],\n",
        "                                  favorable_label=1,\n",
        "                                  unfavorable_label=0)\n",
        "\n",
        "    clf = PrejudiceRemover(sensitive_attr=\"protected\", eta=25.0)\n",
        "    clf.fit(bld_train)\n",
        "    pred = clf.predict(bld_test)\n",
        "\n",
        "    return evaluate_fairness(y_test, pred.labels.ravel(), prot_test, X_test)\n"
      ],
      "metadata": {
        "id": "xSmVTFLlxdBc"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results['PrejudiceRemover'] = train_prejudice_remover(X_train, y_train, prot_train, X_test, y_test, prot_test)\n"
      ],
      "metadata": {
        "id": "emuzoR6AJAeI"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_expgrad(X_train, y_train, prot_train, X_test, y_test, prot_test):\n",
        "    df_train = pd.DataFrame(np.hstack((X_train, y_train[:, None], prot_train[:, None])),\n",
        "                            columns=[f\"x{i}\" for i in range(X_train.shape[1])] + [\"label\", \"protected\"])\n",
        "    bld_train = BinaryLabelDataset(df=df_train,\n",
        "                                   label_names=[\"label\"],\n",
        "                                   protected_attribute_names=[\"protected\"],\n",
        "                                   favorable_label=1,\n",
        "                                   unfavorable_label=0)\n",
        "\n",
        "    df_test = pd.DataFrame(np.hstack((X_test, y_test[:, None], prot_test[:, None])),\n",
        "                           columns=[f\"x{i}\" for i in range(X_test.shape[1])] + [\"label\", \"protected\"])\n",
        "    bld_test = BinaryLabelDataset(df=df_test,\n",
        "                                  label_names=[\"label\"],\n",
        "                                  protected_attribute_names=[\"protected\"],\n",
        "                                  favorable_label=1,\n",
        "                                  unfavorable_label=0)\n",
        "\n",
        "    expgrad = ExponentiatedGradientReduction(\n",
        "        estimator=LogisticRegression(solver='liblinear'),\n",
        "        constraints=\"DemographicParity\"\n",
        "    )\n",
        "    expgrad.fit(bld_train)\n",
        "\n",
        "    pred = expgrad.predict(bld_test)\n",
        "    return evaluate_fairness(y_test, pred.labels.ravel(), prot_test, X_test)\n"
      ],
      "metadata": {
        "id": "lJff7KFSxeik"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results['ExponentiatedGradient'] = train_expgrad(X_train, y_train, prot_train, X_test, y_test, prot_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVSugHebJCdn",
        "outputId": "d779ab86-56ed-4a98-9e22-a7f25a8e23eb"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:110: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gridsearch(X_train, y_train, prot_train, X_test, y_test, prot_test):\n",
        "    df_train = pd.DataFrame(np.hstack((X_train, y_train[:, None], prot_train[:, None])),\n",
        "                            columns=[f\"x{i}\" for i in range(X_train.shape[1])] + [\"label\", \"protected\"])\n",
        "    bld_train = BinaryLabelDataset(df=df_train,\n",
        "                                   label_names=[\"label\"],\n",
        "                                   protected_attribute_names=[\"protected\"],\n",
        "                                   favorable_label=1,\n",
        "                                   unfavorable_label=0)\n",
        "\n",
        "    df_test = pd.DataFrame(np.hstack((X_test, y_test[:, None], prot_test[:, None])),\n",
        "                           columns=[f\"x{i}\" for i in range(X_test.shape[1])] + [\"label\", \"protected\"])\n",
        "    bld_test = BinaryLabelDataset(df=df_test,\n",
        "                                  label_names=[\"label\"],\n",
        "                                  protected_attribute_names=[\"protected\"],\n",
        "                                  favorable_label=1,\n",
        "                                  unfavorable_label=0)\n",
        "\n",
        "    grid = GridSearchReduction(\n",
        "        estimator=LogisticRegression(solver='liblinear'),\n",
        "        constraints=\"DemographicParity\"\n",
        "    )\n",
        "    grid.fit(bld_train)\n",
        "\n",
        "    pred = grid.predict(bld_test)\n",
        "    return evaluate_fairness(y_test, pred.labels.ravel(), prot_test, X_test)\n"
      ],
      "metadata": {
        "id": "paNuPAJlySlJ"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results['GridSearch'] = train_gridsearch(X_train, y_train, prot_train, X_test, y_test, prot_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njMrq0s_xgSb",
        "outputId": "fb832ea6-014a-402d-8e5a-bd6a531fa92b"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:110: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.disable_eager_execution()  # Required for TF1 compatibility in AIF360\n",
        "\n",
        "def train_adversarial_debiasing(X_train, y_train, prot_train, X_test, y_test, prot_test):\n",
        "    # Train data\n",
        "    tf.reset_default_graph()\n",
        "    df_train = pd.DataFrame(np.hstack((X_train, y_train.reshape(-1, 1), prot_train.reshape(-1, 1))),\n",
        "                            columns=[f\"x{i}\" for i in range(X_train.shape[1])] + [\"label\", \"protected\"])\n",
        "    bld_train = BinaryLabelDataset(df=df_train,\n",
        "                                   label_names=[\"label\"],\n",
        "                                   protected_attribute_names=[\"protected\"],\n",
        "                                   favorable_label=1,\n",
        "                                   unfavorable_label=0)\n",
        "\n",
        "    # Test data\n",
        "    df_test = pd.DataFrame(np.hstack((X_test, y_test.reshape(-1, 1), prot_test.reshape(-1, 1))),\n",
        "                           columns=[f\"x{i}\" for i in range(X_test.shape[1])] + [\"label\", \"protected\"])\n",
        "    bld_test = BinaryLabelDataset(df=df_test,\n",
        "                                  label_names=[\"label\"],\n",
        "                                  protected_attribute_names=[\"protected\"],\n",
        "                                  favorable_label=1,\n",
        "                                  unfavorable_label=0)\n",
        "\n",
        "    # TensorFlow session\n",
        "    sess = tf.Session()\n",
        "\n",
        "    clf = AdversarialDebiasing(\n",
        "        privileged_groups=[{'protected': 1}],\n",
        "        unprivileged_groups=[{'protected': 0}],\n",
        "        scope_name='adv_debiasing',\n",
        "        sess=sess,\n",
        "        num_epochs=50,\n",
        "        batch_size=64,\n",
        "        debias=True\n",
        "    )\n",
        "\n",
        "    clf.fit(bld_train)\n",
        "    pred = clf.predict(bld_test)\n",
        "\n",
        "    return evaluate_fairness(y_test, pred.labels.ravel(), prot_test, X_test)\n"
      ],
      "metadata": {
        "id": "H8z8KFCAzLIT"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results['AdversarialDebiasing'] = train_adversarial_debiasing(X_train, y_train, prot_train, X_test, y_test, prot_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FA25q1kMzMcd",
        "outputId": "59fde5c3-b7da-4847-985a-eab1c6106f66"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.11/dist-packages/tensorflow/python/util/dispatch.py:1260: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0; iter: 0; batch classifier loss: 0.637053; batch adversarial loss: 0.777676\n",
            "epoch 1; iter: 0; batch classifier loss: 0.478905; batch adversarial loss: 0.796420\n",
            "epoch 2; iter: 0; batch classifier loss: 0.396052; batch adversarial loss: 0.749408\n",
            "epoch 3; iter: 0; batch classifier loss: 0.518667; batch adversarial loss: 0.804999\n",
            "epoch 4; iter: 0; batch classifier loss: 0.390882; batch adversarial loss: 0.769547\n",
            "epoch 5; iter: 0; batch classifier loss: 0.252213; batch adversarial loss: 0.711296\n",
            "epoch 6; iter: 0; batch classifier loss: 0.238057; batch adversarial loss: 0.709862\n",
            "epoch 7; iter: 0; batch classifier loss: 0.311911; batch adversarial loss: 0.739607\n",
            "epoch 8; iter: 0; batch classifier loss: 0.292733; batch adversarial loss: 0.690118\n",
            "epoch 9; iter: 0; batch classifier loss: 0.225404; batch adversarial loss: 0.669096\n",
            "epoch 10; iter: 0; batch classifier loss: 0.248713; batch adversarial loss: 0.682964\n",
            "epoch 11; iter: 0; batch classifier loss: 0.245129; batch adversarial loss: 0.661297\n",
            "epoch 12; iter: 0; batch classifier loss: 0.155126; batch adversarial loss: 0.644693\n",
            "epoch 13; iter: 0; batch classifier loss: 0.156274; batch adversarial loss: 0.615644\n",
            "epoch 14; iter: 0; batch classifier loss: 0.219445; batch adversarial loss: 0.628728\n",
            "epoch 15; iter: 0; batch classifier loss: 0.210940; batch adversarial loss: 0.627897\n",
            "epoch 16; iter: 0; batch classifier loss: 0.263934; batch adversarial loss: 0.637157\n",
            "epoch 17; iter: 0; batch classifier loss: 0.211801; batch adversarial loss: 0.622232\n",
            "epoch 18; iter: 0; batch classifier loss: 0.210960; batch adversarial loss: 0.616130\n",
            "epoch 19; iter: 0; batch classifier loss: 0.127942; batch adversarial loss: 0.582812\n",
            "epoch 20; iter: 0; batch classifier loss: 0.194620; batch adversarial loss: 0.591016\n",
            "epoch 21; iter: 0; batch classifier loss: 0.155539; batch adversarial loss: 0.601269\n",
            "epoch 22; iter: 0; batch classifier loss: 0.116613; batch adversarial loss: 0.554407\n",
            "epoch 23; iter: 0; batch classifier loss: 0.177015; batch adversarial loss: 0.572623\n",
            "epoch 24; iter: 0; batch classifier loss: 0.213120; batch adversarial loss: 0.650797\n",
            "epoch 25; iter: 0; batch classifier loss: 0.156764; batch adversarial loss: 0.564030\n",
            "epoch 26; iter: 0; batch classifier loss: 0.175291; batch adversarial loss: 0.576168\n",
            "epoch 27; iter: 0; batch classifier loss: 0.158255; batch adversarial loss: 0.578030\n",
            "epoch 28; iter: 0; batch classifier loss: 0.225733; batch adversarial loss: 0.584627\n",
            "epoch 29; iter: 0; batch classifier loss: 0.200752; batch adversarial loss: 0.527573\n",
            "epoch 30; iter: 0; batch classifier loss: 0.150660; batch adversarial loss: 0.550560\n",
            "epoch 31; iter: 0; batch classifier loss: 0.072473; batch adversarial loss: 0.522607\n",
            "epoch 32; iter: 0; batch classifier loss: 0.136293; batch adversarial loss: 0.505774\n",
            "epoch 33; iter: 0; batch classifier loss: 0.129685; batch adversarial loss: 0.553860\n",
            "epoch 34; iter: 0; batch classifier loss: 0.197383; batch adversarial loss: 0.553597\n",
            "epoch 35; iter: 0; batch classifier loss: 0.242362; batch adversarial loss: 0.536308\n",
            "epoch 36; iter: 0; batch classifier loss: 0.151751; batch adversarial loss: 0.585921\n",
            "epoch 37; iter: 0; batch classifier loss: 0.143814; batch adversarial loss: 0.579782\n",
            "epoch 38; iter: 0; batch classifier loss: 0.162119; batch adversarial loss: 0.497645\n",
            "epoch 39; iter: 0; batch classifier loss: 0.164886; batch adversarial loss: 0.540418\n",
            "epoch 40; iter: 0; batch classifier loss: 0.130042; batch adversarial loss: 0.506773\n",
            "epoch 41; iter: 0; batch classifier loss: 0.116292; batch adversarial loss: 0.485528\n",
            "epoch 42; iter: 0; batch classifier loss: 0.197287; batch adversarial loss: 0.538016\n",
            "epoch 43; iter: 0; batch classifier loss: 0.137804; batch adversarial loss: 0.512564\n",
            "epoch 44; iter: 0; batch classifier loss: 0.097255; batch adversarial loss: 0.600935\n",
            "epoch 45; iter: 0; batch classifier loss: 0.114749; batch adversarial loss: 0.535404\n",
            "epoch 46; iter: 0; batch classifier loss: 0.150500; batch adversarial loss: 0.467291\n",
            "epoch 47; iter: 0; batch classifier loss: 0.173102; batch adversarial loss: 0.501845\n",
            "epoch 48; iter: 0; batch classifier loss: 0.223469; batch adversarial loss: 0.468322\n",
            "epoch 49; iter: 0; batch classifier loss: 0.114192; batch adversarial loss: 0.428795\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Fairness Evaluation Across Compatible AIF360 In-processing Algorithms\\n\")\n",
        "for model, metrics in results.items():\n",
        "    print(f\"🔹 {model}\")\n",
        "    for metric_name, value in metrics.items():\n",
        "        print(f\"  {metric_name}: {value:.4f}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCfZUuo63kQ6",
        "outputId": "411faf19-addb-482d-9a14-50093ef40177"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fairness Evaluation Across Compatible AIF360 In-processing Algorithms\n",
            "\n",
            "🔹 GerryFair\n",
            "  accuracy: 0.8030\n",
            "  disparate_impact: 0.9833\n",
            "  statistical_parity_difference: -0.0164\n",
            "  equal_opportunity_difference: 0.0000\n",
            "\n",
            "🔹 PrejudiceRemover\n",
            "  accuracy: 0.9242\n",
            "  disparate_impact: 0.9740\n",
            "  statistical_parity_difference: -0.0205\n",
            "  equal_opportunity_difference: -0.0780\n",
            "\n",
            "🔹 ExponentiatedGradient\n",
            "  accuracy: 0.9293\n",
            "  disparate_impact: 0.9226\n",
            "  statistical_parity_difference: -0.0604\n",
            "  equal_opportunity_difference: -0.0817\n",
            "\n",
            "🔹 GridSearch\n",
            "  accuracy: 0.9293\n",
            "  disparate_impact: 0.9482\n",
            "  statistical_parity_difference: -0.0404\n",
            "  equal_opportunity_difference: -0.0692\n",
            "\n",
            "🔹 AdversarialDebiasing\n",
            "  accuracy: 0.9192\n",
            "  disparate_impact: 0.8905\n",
            "  statistical_parity_difference: -0.0873\n",
            "  equal_opportunity_difference: -0.0986\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#postprocessing"
      ],
      "metadata": {
        "id": "z5hWBXLxJMAE"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test, prot_train, prot_test = load_creditcard_data_inprocess(csv_path)\n",
        "post_results = {}"
      ],
      "metadata": {
        "id": "jdC4dnm-JvEo"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_roc_postprocessing_with_xgb(X_train, y_train, prot_train, X_test, y_test, prot_test):\n",
        "    model = train_baseline_model(X_train, y_train)\n",
        "\n",
        "    y_prob = model.predict_proba(X_test)[:, 1]\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    bld_test = BinaryLabelDataset(\n",
        "        favorable_label=1,\n",
        "        unfavorable_label=0,\n",
        "        df=pd.DataFrame(np.hstack((X_test, y_test[:, None], prot_test[:, None])),\n",
        "                        columns=[f'x{i}' for i in range(X_test.shape[1])] + ['label', 'protected']),\n",
        "        label_names=['label'],\n",
        "        protected_attribute_names=['protected']\n",
        "    )\n",
        "\n",
        "    bld_pred = bld_test.copy()\n",
        "    bld_pred.scores = y_prob.reshape(-1, 1)\n",
        "    bld_pred.labels = y_pred.reshape(-1, 1)\n",
        "\n",
        "    roc = RejectOptionClassification(\n",
        "        unprivileged_groups=[{'protected': 0}],\n",
        "        privileged_groups=[{'protected': 1}],\n",
        "        low_class_thresh=0.01, high_class_thresh=0.99,\n",
        "        num_class_thresh=100, num_ROC_margin=50,\n",
        "        metric_name=\"Statistical parity difference\",\n",
        "        metric_ub=0.05, metric_lb=-0.05\n",
        "    )\n",
        "    roc = roc.fit(bld_test, bld_pred)\n",
        "    pred = roc.predict(bld_pred)\n",
        "\n",
        "    return evaluate_fairness(y_test, pred.labels.ravel(), prot_test, X_test)\n"
      ],
      "metadata": {
        "id": "KAaGOOMv3ssO"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "post_results['RejectOptionClassification'] = train_roc_postprocessing_with_xgb(X_train, y_train, prot_train, X_test, y_test, prot_test)\n"
      ],
      "metadata": {
        "id": "5khEknevMU9G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14fef1e2-8789-4d1f-a87c-2eb87407a1ec"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:44:27] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_calibrated_eq_odds_with_xgb(X_train, y_train, prot_train, X_test, y_test, prot_test):\n",
        "    model = train_baseline_model(X_train, y_train)\n",
        "\n",
        "    y_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    bld_test = BinaryLabelDataset(\n",
        "        favorable_label=1,\n",
        "        unfavorable_label=0,\n",
        "        df=pd.DataFrame(np.hstack((X_test, y_test[:, None], prot_test[:, None])),\n",
        "                        columns=[f'x{i}' for i in range(X_test.shape[1])] + ['label', 'protected']),\n",
        "        label_names=['label'],\n",
        "        protected_attribute_names=['protected']\n",
        "    )\n",
        "\n",
        "    bld_pred = bld_test.copy()\n",
        "    bld_pred.scores = y_prob.reshape(-1, 1)\n",
        "\n",
        "    ceo = CalibratedEqOddsPostprocessing(\n",
        "        privileged_groups=[{'protected': 1}],\n",
        "        unprivileged_groups=[{'protected': 0}],\n",
        "        cost_constraint=\"fnr\",\n",
        "        seed=42\n",
        "    )\n",
        "    ceo = ceo.fit(bld_test, bld_pred)\n",
        "    pred = ceo.predict(bld_pred)\n",
        "\n",
        "    return evaluate_fairness(y_test, pred.labels.ravel(), prot_test, X_test)\n"
      ],
      "metadata": {
        "id": "4-lEz-kD6JUr"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "post_results['CalibratedEqOdds'] = train_calibrated_eq_odds_with_xgb(X_train, y_train, prot_train, X_test, y_test, prot_test)\n"
      ],
      "metadata": {
        "id": "1AunQ9VNMWxS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d75fb8f-aa0f-44e5-b617-ba7b00483e70"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:44:33] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_equalized_odds_with_xgb(X_train, y_train, prot_train, X_test, y_test, prot_test):\n",
        "    model = train_baseline_model(X_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    bld_test = BinaryLabelDataset(\n",
        "        favorable_label=1,\n",
        "        unfavorable_label=0,\n",
        "        df=pd.DataFrame(np.hstack((X_test, y_test[:, None], prot_test[:, None])),\n",
        "                        columns=[f'x{i}' for i in range(X_test.shape[1])] + ['label', 'protected']),\n",
        "        label_names=['label'],\n",
        "        protected_attribute_names=['protected']\n",
        "    )\n",
        "\n",
        "    bld_pred = bld_test.copy()\n",
        "    bld_pred.labels = y_pred.reshape(-1, 1)\n",
        "\n",
        "    eq = EqOddsPostprocessing(\n",
        "        privileged_groups=[{'protected': 1}],\n",
        "        unprivileged_groups=[{'protected': 0}]\n",
        "    )\n",
        "    eq = eq.fit(bld_test, bld_pred)\n",
        "    pred = eq.predict(bld_pred)\n",
        "\n",
        "    return evaluate_fairness(y_test, pred.labels.ravel(), prot_test, X_test)\n"
      ],
      "metadata": {
        "id": "SjdA2g9P6LWG"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "post_results['EqualizedOdds'] = train_equalized_odds_with_xgb(X_train, y_train, prot_train, X_test, y_test, prot_test)"
      ],
      "metadata": {
        "id": "jDYTrQqF6Opo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ee08db4-859f-429f-b58d-a8075c9f6fef"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:44:35] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for model, metrics in post_results.items():\n",
        "    print(f\"🔹 {model}\")\n",
        "    for k, v in metrics.items():\n",
        "        print(f\"  {k}: {v:.4f}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pxj5oi7v6Y5a",
        "outputId": "14d59601-21b7-4054-9694-1a56d42b7c5f"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 RejectOptionClassification\n",
            "  accuracy: 0.9773\n",
            "  disparate_impact: 1.0133\n",
            "  statistical_parity_difference: 0.0100\n",
            "  equal_opportunity_difference: -0.0280\n",
            "\n",
            "🔹 CalibratedEqOdds\n",
            "  accuracy: 0.9394\n",
            "  disparate_impact: 0.9497\n",
            "  statistical_parity_difference: -0.0408\n",
            "  equal_opportunity_difference: -0.0368\n",
            "\n",
            "🔹 EqualizedOdds\n",
            "  accuracy: 0.9470\n",
            "  disparate_impact: 1.0313\n",
            "  statistical_parity_difference: 0.0234\n",
            "  equal_opportunity_difference: -0.0015\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AHL-3AJmLOdv"
      },
      "execution_count": 84,
      "outputs": []
    }
  ]
}